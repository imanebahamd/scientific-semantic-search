<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/" xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns="http://www.w3.org/2005/Atom">
  <id>https://arxiv.org/api/kDEpPeFkDEdYRnVAOGodfwsOkPE</id>
  <title>arXiv Query: search_query=cat:cs.AI OR cat:cs.LG OR cat:cs.CL OR cat:cs.CV OR cat:cs.NE OR cat:cs.SE OR cat:cs.CR OR cat:cs.DC&amp;id_list=&amp;start=300&amp;max_results=100</title>
  <updated>2025-12-16T13:50:38Z</updated>
  <link href="https://arxiv.org/api/query?search_query=cat:cs.AI+OR+(cat:cs.LG+OR+(cat:cs.CL+OR+(cat:cs.CV+OR+(cat:cs.NE+OR+(cat:cs.SE+OR+(cat:cs.CR+OR+cat:cs.DC))))))&amp;start=300&amp;max_results=100&amp;id_list=" type="application/atom+xml"/>
  <opensearch:itemsPerPage>100</opensearch:itemsPerPage>
  <opensearch:totalResults>566020</opensearch:totalResults>
  <opensearch:startIndex>300</opensearch:startIndex>
  <entry>
    <id>http://arxiv.org/abs/2512.12801v1</id>
    <title>Fine-Grained Energy Prediction For Parallellized LLM Inference With PIE-P</title>
    <updated>2025-12-14T18:50:51Z</updated>
    <link href="https://arxiv.org/abs/2512.12801v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12801v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>With the widespread adoption of Large Language Models (LLMs), energy costs of running LLMs is quickly becoming a critical concern. However, precisely measuring the energy consumption of LLMs is often infeasible because hardware-based power monitors are not always accessible and software-based energy measurement tools are not accurate. While various prediction techniques have been developed to estimate LLM energy consumption, these approaches are limited to single-GPU environments and thus are not applicable to modern LLM inference which is typically parallelized across multiple GPUs. In this work, we remedy this gap and introduce PIE-P, a fine-grained energy prediction framework for multi-GPU inference, including tensor, pipeline, and data parallelism. Predicting the energy under parallelized inference is complicated by the non-determinism in inter-GPU communication, additional communication overheads, and difficulties in isolating energy during the communication/synchronization phase. We develop a scalable prediction framework that addresses these issues via precise sampling, fine-grained modeling of inter-GPU communication, and careful accounting of parallelization overhead. Our evaluation results show that PIE-P yields accurate and fine-grained energy predictions across parallelism strategies, significantly outperforming baselines.</summary>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T18:50:51Z</published>
    <arxiv:primary_category term="cs.DC"/>
    <author>
      <name>Anurag Dutt</name>
    </author>
    <author>
      <name>Young Won Choi</name>
    </author>
    <author>
      <name>Avirup Sil</name>
    </author>
    <author>
      <name>Anshul Gandhi</name>
    </author>
    <author>
      <name>Aruna Balasubramanian</name>
    </author>
    <author>
      <name>Niranjan Balasubramanian</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12800v1</id>
    <title>Learning Common and Salient Generative Factors Between Two Image Datasets</title>
    <updated>2025-12-14T18:47:41Z</updated>
    <link href="https://arxiv.org/abs/2512.12800v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12800v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Recent advancements in image synthesis have enabled high-quality image generation and manipulation. Most works focus on: 1) conditional manipulation, where an image is modified conditioned on a given attribute, or 2) disentangled representation learning, where each latent direction should represent a distinct semantic attribute. In this paper, we focus on a different and less studied research problem, called Contrastive Analysis (CA). Given two image datasets, we want to separate the common generative factors, shared across the two datasets, from the salient ones, specific to only one dataset. Compared to existing methods, which use attributes as supervised signals for editing (e.g., glasses, gender), the proposed method is weaker, since it only uses the dataset signal. We propose a novel framework for CA, that can be adapted to both GAN and Diffusion models, to learn both common and salient factors. By defining new and well-adapted learning strategies and losses, we ensure a relevant separation between common and salient factors, preserving a high-quality generation. We evaluate our approach on diverse datasets, covering human faces, animal images and medical scans. Our framework demonstrates superior separation ability and image quality synthesis compared to prior methods.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T18:47:41Z</published>
    <arxiv:comment>This is the author's version of a work submitted to IEEE for possible publication. The final version may differ from this version</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Yunlong He</name>
    </author>
    <author>
      <name>Gwilherm Lesné</name>
    </author>
    <author>
      <name>Ziqian Liu</name>
    </author>
    <author>
      <name>Michaël Soumm</name>
    </author>
    <author>
      <name>Pietro Gori</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12799v1</id>
    <title>DrivePI: Spatial-aware 4D MLLM for Unified Autonomous Driving Understanding, Perception, Prediction and Planning</title>
    <updated>2025-12-14T18:45:54Z</updated>
    <link href="https://arxiv.org/abs/2512.12799v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12799v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Although multi-modal large language models (MLLMs) have shown strong capabilities across diverse domains, their application in generating fine-grained 3D perception and prediction outputs in autonomous driving remains underexplored. In this paper, we propose DrivePI, a novel spatial-aware 4D MLLM that serves as a unified Vision-Language-Action (VLA) framework that is also compatible with vision-action (VA) models. Our method jointly performs spatial understanding, 3D perception (i.e., 3D occupancy), prediction (i.e., occupancy flow), and planning (i.e., action outputs) in parallel through end-to-end optimization. To obtain both precise geometric information and rich visual appearance, our approach integrates point clouds, multi-view images, and language instructions within a unified MLLM architecture. We further develop a data engine to generate text-occupancy and text-flow QA pairs for 4D spatial understanding. Remarkably, with only a 0.5B Qwen2.5 model as MLLM backbone, DrivePI as a single unified model matches or exceeds both existing VLA models and specialized VA models. Specifically, compared to VLA models, DrivePI outperforms OpenDriveVLA-7B by 2.5% mean accuracy on nuScenes-QA and reduces collision rate by 70% over ORION (from 0.37% to 0.11%) on nuScenes. Against specialized VA models, DrivePI surpasses FB-OCC by 10.3 RayIoU for 3D occupancy on OpenOcc, reduces the mAVE from 0.591 to 0.509 for occupancy flow on OpenOcc, and achieves 32% lower L2 error than VAD (from 0.72m to 0.49m) for planning on nuScenes. Code will be available at https://github.com/happinesslz/DrivePI</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T18:45:54Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Zhe Liu</name>
    </author>
    <author>
      <name>Runhui Huang</name>
    </author>
    <author>
      <name>Rui Yang</name>
    </author>
    <author>
      <name>Siming Yan</name>
    </author>
    <author>
      <name>Zining Wang</name>
    </author>
    <author>
      <name>Lu Hou</name>
    </author>
    <author>
      <name>Di Lin</name>
    </author>
    <author>
      <name>Xiang Bai</name>
    </author>
    <author>
      <name>Hengshuang Zhao</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12795v1</id>
    <title>TRACER: Transfer Learning based Real-time Adaptation for Clinical Evolving Risk</title>
    <updated>2025-12-14T18:23:22Z</updated>
    <link href="https://arxiv.org/abs/2512.12795v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12795v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Clinical decision support tools built on electronic health records often experience performance drift due to temporal population shifts, particularly when changes in the clinical environment initially affect only a subset of patients, resulting in a transition to mixed populations. Such case-mix changes commonly arise following system-level operational updates or the emergence of new diseases, such as COVID-19. We propose TRACER (Transfer Learning-based Real-time Adaptation for Clinical Evolving Risk), a framework that identifies encounter-level transition membership and adapts predictive models using transfer learning without full retraining. In simulation studies, TRACER outperformed static models trained on historical or contemporary data. In a real-world application predicting hospital admission following emergency department visits across the COVID-19 transition, TRACER improved both discrimination and calibration. TRACER provides a scalable approach for maintaining robust predictive performance under evolving and heterogeneous clinical conditions.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T18:23:22Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Mengying Yan</name>
    </author>
    <author>
      <name>Ziye Tian</name>
    </author>
    <author>
      <name>Siqi Li</name>
    </author>
    <author>
      <name>Nan Liu</name>
    </author>
    <author>
      <name>Benjamin A. Goldstein</name>
    </author>
    <author>
      <name>Molei Liu</name>
    </author>
    <author>
      <name>Chuan Hong</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12792v1</id>
    <title>Liquid Reasoning Transformers: A Sudoku-Based Prototype for Chess-Scale Algorithmic Tasks</title>
    <updated>2025-12-14T18:20:05Z</updated>
    <link href="https://arxiv.org/abs/2512.12792v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12792v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The Liquid Reasoning Transformer (LRT) is a transformer architecture designed for inference with adaptive depths using iterative changes, discard-based correction, and a learned stopping mechanism. Instead of relying on a single feedforward pass, the model updates a recurrent reasoning token across multiple internal steps, allowing it to correct early errors and allocate computation based on input difficulty. We evaluate the LRT on Sudoku as a controlled testbed for structured reasoning and show that it achieves strong performance, reaching 98.68% digit accuracy and 36.30% full-puzzle accuracy without using symbolic rules or search. Analyzing internal patterns shows that the discard and stop gates play different, important roles in stabilizing inferences and adjusting computational depth. We discuss how these mechanisms extend naturally to chess-scale reasoning tasks and outline extensions for multi-token reasoning and larger domains.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T18:20:05Z</published>
    <arxiv:comment>11 pages, 0 figures</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Shivansh Sahni</name>
    </author>
    <author>
      <name>Wenzhi Zhang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12791v1</id>
    <title>Beyond Task Completion: An Assessment Framework for Evaluating Agentic AI Systems</title>
    <updated>2025-12-14T18:17:40Z</updated>
    <link href="https://arxiv.org/abs/2512.12791v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12791v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Recent advances in agentic AI have shifted the focus from standalone Large Language Models (LLMs) to integrated systems that combine LLMs with tools, memory, and other agents to perform complex tasks. These multi-agent architectures enable coordinated reasoning, planning, and execution across diverse domains, allowing agents to collaboratively automate complex workflows. Despite these advances, evaluation and assessment of LLM agents and the multi-agent systems they constitute remain a fundamental challenge. Although various approaches have been proposed in the software engineering literature for evaluating conventional software components, existing methods for AI-based systems often overlook the non-deterministic nature of models. This non-determinism introduces behavioral uncertainty during execution, yet existing evaluations rely on binary task completion metrics that fail to capture it. Evaluating agentic systems therefore requires examining additional dimensions, including the agent ability to invoke tools, ingest and retrieve memory, collaborate with other agents, and interact effectively with its environment. We propose an end-to-end Agent Assessment Framework with four evaluation pillars encompassing LLMs, Memory, Tools, and Environment. We validate the framework on a representative Autonomous CloudOps use case, where experiments reveal behavioral deviations overlooked by conventional metrics, demonstrating its effectiveness in capturing runtime uncertainties.</summary>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T18:17:40Z</published>
    <arxiv:primary_category term="cs.MA"/>
    <author>
      <name>Sreemaee Akshathala</name>
    </author>
    <author>
      <name>Bassam Adnan</name>
    </author>
    <author>
      <name>Mahisha Ramesh</name>
    </author>
    <author>
      <name>Karthik Vaidhyanathan</name>
    </author>
    <author>
      <name>Basil Muhammed</name>
    </author>
    <author>
      <name>Kannan Parthasarathy</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12790v1</id>
    <title>L-STEC: Learned Video Compression with Long-term Spatio-Temporal Enhanced Context</title>
    <updated>2025-12-14T18:11:16Z</updated>
    <link href="https://arxiv.org/abs/2512.12790v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12790v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Neural Video Compression has emerged in recent years, with condition-based frameworks outperforming traditional codecs. However, most existing methods rely solely on the previous frame's features to predict temporal context, leading to two critical issues. First, the short reference window misses long-term dependencies and fine texture details. Second, propagating only feature-level information accumulates errors over frames, causing prediction inaccuracies and loss of subtle textures. To address these, we propose the Long-term Spatio-Temporal Enhanced Context (L-STEC) method. We first extend the reference chain with LSTM to capture long-term dependencies. We then incorporate warped spatial context from the pixel domain, fusing spatio-temporal information through a multi-receptive field network to better preserve reference details. Experimental results show that L-STEC significantly improves compression by enriching contextual information, achieving 37.01% bitrate savings in PSNR and 31.65% in MS-SSIM compared to DCVC-TCM, outperforming both VTM-17.0 and DCVC-FM and establishing new state-of-the-art performance.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T18:11:16Z</published>
    <arxiv:comment>Accepted to Data Compression Conference (DCC) 2026 as an oral paper</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Tiange Zhang</name>
    </author>
    <author>
      <name>Zhimeng Huang</name>
    </author>
    <author>
      <name>Xiandong Meng</name>
    </author>
    <author>
      <name>Kai Zhang</name>
    </author>
    <author>
      <name>Zhipin Deng</name>
    </author>
    <author>
      <name>Siwei Ma</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12788v1</id>
    <title>Temporal HAL-API Dependencies as a Gateway to Formal Embedded Software Development</title>
    <updated>2025-12-14T18:08:09Z</updated>
    <link href="https://arxiv.org/abs/2512.12788v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12788v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Temporal HAL-API Dependencies (THADs) can be useful to capture an interesting class of correctness properties in embedded software development. They demand a moderate effort for specification (which can be done via program annotations) and verification (which can be done automatically via software model checking). In this sense, they have the potential to form an interesting sweet spot between generic properties (that demand virtually no specification effort, and that are typically addressed by static analysis) and application-specific properties as addressed by full-fledged formal methods. Thus, they may form a gateway to wider and more economic use of formal methods in industrial embedded software development.</summary>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T18:08:09Z</published>
    <arxiv:primary_category term="cs.SE"/>
    <author>
      <name>Manuel Bentele</name>
    </author>
    <author>
      <name>Andreas Podelski</name>
    </author>
    <author>
      <name>Axel Sikora</name>
    </author>
    <author>
      <name>Bernd Westphal</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12787v1</id>
    <title>Unveiling Statistical Significance of Online Regression over Multiple Datasets</title>
    <updated>2025-12-14T18:04:11Z</updated>
    <link href="https://arxiv.org/abs/2512.12787v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12787v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Despite extensive focus on techniques for evaluating the performance of two learning algorithms on a single dataset, the critical challenge of developing statistical tests to compare multiple algorithms across various datasets has been largely overlooked in most machine learning research. Additionally, in the realm of Online Learning, ensuring statistical significance is essential to validate continuous learning processes, particularly for achieving rapid convergence and effectively managing concept drifts in a timely manner. Robust statistical methods are needed to assess the significance of performance differences as data evolves over time. This article examines the state-of-the-art online regression models and empirically evaluates several suitable tests. To compare multiple online regression models across various datasets, we employed the Friedman test along with corresponding post-hoc tests. For thorough evaluations, utilizing both real and synthetic datasets with 5-fold cross-validation and seed averaging ensures comprehensive assessment across various data subsets. Our tests generally confirmed the performance of competitive baselines as consistent with their individual reports. However, some statistical test results also indicate that there is still room for improvement in certain aspects of state-of-the-art methods.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T18:04:11Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <arxiv:journal_ref>2024 IEEE 7th International Conference on Multimedia Information Processing (MIPR 2024)</arxiv:journal_ref>
    <author>
      <name>Mohammad Abu-Shaira</name>
    </author>
    <author>
      <name>Weishi Shi</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12785v1</id>
    <title>OLC-WA: Drift Aware Tuning-Free Online Classification with Weighted Average</title>
    <updated>2025-12-14T17:52:39Z</updated>
    <link href="https://arxiv.org/abs/2512.12785v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12785v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Real-world data sets often exhibit temporal dynamics characterized by evolving data distributions. Disregarding this phenomenon, commonly referred to as concept drift, can significantly diminish a model's predictive accuracy. Furthermore, the presence of hyperparameters in online models exacerbates this issue. These parameters are typically fixed and cannot be dynamically adjusted by the user in response to the evolving data distribution. This paper introduces Online Classification with Weighted Average (OLC-WA), an adaptive, hyperparameter-free online classification model equipped with an automated optimization mechanism. OLC-WA operates by blending incoming data streams with an existing base model. This blending is facilitated by an exponentially weighted moving average. Furthermore, an integrated optimization mechanism dynamically detects concept drift, quantifies its magnitude, and adjusts the model based on the observed data stream characteristics. This approach empowers the model to effectively adapt to evolving data distributions within streaming environments. Rigorous empirical evaluation across diverse benchmark datasets shows that OLC-WA achieves performance comparable to batch models in stationary environments, maintaining accuracy within 1-3%, and surpasses leading online baselines by 10-25% under drift, demonstrating its effectiveness in adapting to dynamic data streams.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T17:52:39Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <arxiv:journal_ref>Expert Systems with Applications (Elsevier), 2025</arxiv:journal_ref>
    <author>
      <name>Mohammad Abu Shaira</name>
    </author>
    <author>
      <name>Yunhe Feng</name>
    </author>
    <author>
      <name>Heng Fan</name>
    </author>
    <author>
      <name>Weishi Shi</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12783v1</id>
    <title>Credit Risk Estimation with Non-Financial Features: Evidence from a Synthetic Istanbul Dataset</title>
    <updated>2025-12-14T17:48:13Z</updated>
    <link href="https://arxiv.org/abs/2512.12783v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12783v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Financial exclusion constrains entrepreneurship, increases income volatility, and widens wealth gaps. Underbanked consumers in Istanbul often have no bureau file because their earnings and payments flow through informal channels. To study how such borrowers can be evaluated we create a synthetic dataset of one hundred thousand Istanbul residents that reproduces first quarter 2025 TÜİK census marginals and telecom usage patterns. Retrieval augmented generation feeds these public statistics into the OpenAI o3 model, which synthesises realistic yet private records. Each profile contains seven socio demographic variables and nine alternative attributes that describe phone specifications, online shopping rhythm, subscription spend, car ownership, monthly rent, and a credit card flag. To test the impact of the alternative financial data CatBoost, LightGBM, and XGBoost are each trained in two versions. Demo models use only the socio demographic variables; Full models include both socio demographic and alternative attributes. Across five fold stratified validation the alternative block raises area under the curve by about one point three percentage and lifts balanced \(F_{1}\) from roughly 0.84 to 0.95, a fourteen percent gain. We contribute an open Istanbul 2025 Q1 synthetic dataset, a fully reproducible modeling pipeline, and empirical evidence that a concise set of behavioural attributes can approach bureau level discrimination power while serving borrowers who lack formal credit records. These findings give lenders and regulators a transparent blueprint for extending fair and safe credit access to the underbanked.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T17:48:13Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Atalay Denknalbant</name>
    </author>
    <author>
      <name>Emre Sezdi</name>
    </author>
    <author>
      <name>Zeki Furkan Kutlu</name>
    </author>
    <author>
      <name>Polat Goktas</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12779v1</id>
    <title>OLR-WAA: Adaptive and Drift-Resilient Online Regression with Dynamic Weighted Averaging</title>
    <updated>2025-12-14T17:39:51Z</updated>
    <link href="https://arxiv.org/abs/2512.12779v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12779v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Real-world datasets frequently exhibit evolving data distributions, reflecting temporal variations and underlying shifts. Overlooking this phenomenon, known as concept drift, can substantially degrade the predictive performance of the model. Furthermore, the presence of hyperparameters in online models exacerbates this issue, as these parameters are typically fixed and lack the flexibility to dynamically adjust to evolving data. This paper introduces "OLR-WAA: An Adaptive and Drift-Resilient Online Regression with Dynamic Weighted Average", a hyperparameter-free model designed to tackle the challenges of non-stationary data streams and enable effective, continuous adaptation. The objective is to strike a balance between model stability and adaptability. OLR-WAA incrementally updates its base model by integrating incoming data streams, utilizing an exponentially weighted moving average. It further introduces a unique optimization mechanism that dynamically detects concept drift, quantifies its magnitude, and adjusts the model based on real-time data characteristics. Rigorous evaluations show that it matches batch regression performance in static settings and consistently outperforms or rivals state-of-the-art online models, confirming its effectiveness. Concept drift datasets reveal a performance gap that OLR-WAA effectively bridges, setting it apart from other online models. In addition, the model effectively handles confidence-based scenarios through a conservative update strategy that prioritizes stable, high-confidence data points. Notably, OLR-WAA converges rapidly, consistently yielding higher R2 values compared to other online models.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T17:39:51Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <arxiv:journal_ref>Data Science and Engineering, 2025</arxiv:journal_ref>
    <author>
      <name>Mohammad Abu-Shaira</name>
    </author>
    <author>
      <name>Weishi Shi</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12777v1</id>
    <title>State over Tokens: Characterizing the Role of Reasoning Tokens</title>
    <updated>2025-12-14T17:30:34Z</updated>
    <link href="https://arxiv.org/abs/2512.12777v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12777v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Large Language Models (LLMs) can generate reasoning tokens before their final answer to boost performance on complex tasks. While these sequences seem like human thought processes, empirical evidence reveals that they are not a faithful explanation of the model's actual reasoning process. To address this gap between appearance and function, we introduce the State over Tokens (SoT) conceptual framework. SoT reframes reasoning tokens not as a linguistic narrative, but as an externalized computational state -- the sole persistent information carrier across the model's stateless generation cycles. This explains how the tokens can drive correct reasoning without being a faithful explanation when read as text and surfaces previously overlooked research questions on these tokens. We argue that to truly understand the process that LLMs do, research must move beyond reading the reasoning tokens as text and focus on decoding them as state.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T17:30:34Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Mosh Levy</name>
    </author>
    <author>
      <name>Zohar Elyoseph</name>
    </author>
    <author>
      <name>Shauli Ravfogel</name>
    </author>
    <author>
      <name>Yoav Goldberg</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12775v1</id>
    <title>Persistent Personas? Role-Playing, Instruction Following, and Safety in Extended Interactions</title>
    <updated>2025-12-14T17:27:02Z</updated>
    <link href="https://arxiv.org/abs/2512.12775v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12775v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Persona-assigned large language models (LLMs) are used in domains such as education, healthcare, and sociodemographic simulation. Yet, they are typically evaluated only in short, single-round settings that do not reflect real-world usage. We introduce an evaluation protocol that combines long persona dialogues (over 100 rounds) and evaluation datasets to create dialogue-conditioned benchmarks that can robustly measure long-context effects. We then investigate the effects of dialogue length on persona fidelity, instruction-following, and safety of seven state-of-the-art open- and closed-weight LLMs. We find that persona fidelity degrades over the course of dialogues, especially in goal-oriented conversations, where models must sustain both persona fidelity and instruction following. We identify a trade-off between fidelity and instruction following, with non-persona baselines initially outperforming persona-assigned models; as dialogues progress and fidelity fades, persona responses become increasingly similar to baseline responses. Our findings highlight the fragility of persona applications in extended interactions and our work provides a protocol to systematically measure such failures.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T17:27:02Z</published>
    <arxiv:comment>31 pages, 35 figures</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Pedro Henrique Luz de Araujo</name>
    </author>
    <author>
      <name>Michael A. Hedderich</name>
    </author>
    <author>
      <name>Ali Modarressi</name>
    </author>
    <author>
      <name>Hinrich Schuetze</name>
    </author>
    <author>
      <name>Benjamin Roth</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12774v1</id>
    <title>Fast 2DGS: Efficient Image Representation with Deep Gaussian Prior</title>
    <updated>2025-12-14T17:23:28Z</updated>
    <link href="https://arxiv.org/abs/2512.12774v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12774v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>As generative models become increasingly capable of producing high-fidelity visual content, the demand for efficient, interpretable, and editable image representations has grown substantially. Recent advances in 2D Gaussian Splatting (2DGS) have emerged as a promising solution, offering explicit control, high interpretability, and real-time rendering capabilities (&gt;1000 FPS). However, high-quality 2DGS typically requires post-optimization. Existing methods adopt random or heuristics (e.g., gradient maps), which are often insensitive to image complexity and lead to slow convergence (&gt;10s). More recent approaches introduce learnable networks to predict initial Gaussian configurations, but at the cost of increased computational and architectural complexity. To bridge this gap, we present Fast-2DGS, a lightweight framework for efficient Gaussian image representation. Specifically, we introduce Deep Gaussian Prior, implemented as a conditional network to capture the spatial distribution of Gaussian primitives under different complexities. In addition, we propose an attribute regression network to predict dense Gaussian properties. Experiments demonstrate that this disentangled architecture achieves high-quality reconstruction in a single forward pass, followed by minimal fine-tuning. More importantly, our approach significantly reduces computational cost without compromising visual quality, bringing 2DGS closer to industry-ready deployment.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T17:23:28Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Hao Wang</name>
    </author>
    <author>
      <name>Ashish Bastola</name>
    </author>
    <author>
      <name>Chaoyi Zhou</name>
    </author>
    <author>
      <name>Wenhui Zhu</name>
    </author>
    <author>
      <name>Xiwen Chen</name>
    </author>
    <author>
      <name>Xuanzhao Dong</name>
    </author>
    <author>
      <name>Siyu Huang</name>
    </author>
    <author>
      <name>Abolfazl Razi</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12773v1</id>
    <title>Designing The Drive: Enhancing User Experience through Adaptive Interfaces in Autonomous Vehicles</title>
    <updated>2025-12-14T17:23:28Z</updated>
    <link href="https://arxiv.org/abs/2512.12773v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12773v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>With the recent development and integration of autonomous vehicles (AVs) in transportation systems of the modern world, the emphasis on customizing user interfaces to optimize the overall user experience has been growing expediently. Therefore, understanding user needs and preferences is essential to the acceptance and trust of these technologies as they continue to grow in prevalence. This paper addresses the implementation of HCI principles in the personalization of interfaces to improve safety, security, and usability for the users. This paper explores the way that personalized interfaces can be devised to increase user engagement and satisfaction through various HCI strategies such as adaptive design, multi-modal interaction, and user feedback mechanisms. Moreover, this paper puts emphasis on factors of transparency and user control in the design of an interface; hence, allowing users to design or modify their experience could foster an increase in trust in autonomous systems. In so doing, this research touches on the quite influential role HCI will play in this future scenario of autonomous vehicles while designing to ensure relevance to the diverse needs of users while maintaining high standards of safety and security. Discussing various HCI strategies such as adaptive design, multi-modal interaction, and feedback mechanisms to the user, this paper demonstrates how personalized interfaces can enhance significantly both user engagement and satisfaction. Transparency and user control also in designing an interface are further discussed, pointing out the need for a prerequisite condition of enabling the user to take control of their experience as a state of trust in autonomous systems. In summary, this paper points out the role of HCI in the development of autonomous vehicles and addresses numerous needs with respect to those enforced safety and security standards.</summary>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T17:23:28Z</published>
    <arxiv:comment>8 pages, 4 figures</arxiv:comment>
    <arxiv:primary_category term="cs.HC"/>
    <author>
      <name>Reeteesha Roy</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12772v1</id>
    <title>JointAVBench: A Benchmark for Joint Audio-Visual Reasoning Evaluation</title>
    <updated>2025-12-14T17:23:21Z</updated>
    <link href="https://arxiv.org/abs/2512.12772v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12772v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Understanding videos inherently requires reasoning over both visual and auditory information. To properly evaluate Omni-Large Language Models (Omni-LLMs), which are capable of processing multi-modal information including vision and audio, an effective benchmark must comprehensively cover three key aspects: (1) multi-modal dependency (i.e., questions that cannot be answered using vision or audio alone), (2) diverse audio information types (e.g., speech, sound events), and (3) varying scene spans. However, existing datasets fall short in one or more of these dimensions, limiting strict and comprehensive evaluation. To address this gap, we introduce JointAVBench, a novel benchmark with strict audio-video correlation, spanning five cognitive dimensions, four audio information types (speech, sound events, music, vocal traits), and three scene spans (single-, cross-, and full-scene). Given the high cost of manual annotation, we propose an automated pipeline that leverages state-of-the-art vision-LLMs, audio-LLMs, and general-purpose LLMs to synthesize questions and answers that strictly require joint audio-visual understanding. We evaluate leading vision-only, audio-only, and Omni-LLMs on our dataset. Results show that even the best-performing Omni-LLM achieves an average accuracy of only 62.6\%, outperforming uni-modal baselines but revealing substantial room for improvement, especially in cross-scene reasoning.</summary>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T17:23:21Z</published>
    <arxiv:primary_category term="cs.MM"/>
    <author>
      <name>Jianghan Chao</name>
    </author>
    <author>
      <name>Jianzhang Gao</name>
    </author>
    <author>
      <name>Wenhui Tan</name>
    </author>
    <author>
      <name>Yuchong Sun</name>
    </author>
    <author>
      <name>Ruihua Song</name>
    </author>
    <author>
      <name>Liyun Ru</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12770v1</id>
    <title>Curió-Edu 7B: Examining Data Selection Impacts in LLM Continued Pretraining</title>
    <updated>2025-12-14T17:19:32Z</updated>
    <link href="https://arxiv.org/abs/2512.12770v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12770v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Continued pretraining extends a language model's capabilities by further exposing it to additional data, often tailored to a specific linguistic or domain context. This strategy has emerged as an efficient alternative to full retraining when adapting general-purpose models to new settings. In this work, we investigate this paradigm through Curió 7B, a 7-billion-parameter model derived from LLaMA-2 and trained on 100 billion Portuguese tokens from the ClassiCC-PT corpus - the most extensive Portuguese-specific continued-pretraining effort above the three-billion-parameter scale to date. Beyond scale, we investigate whether quantity alone suffices or whether data quality plays a decisive role in linguistic adaptation. To this end, we introduce Curió-Edu 7B, a variant trained exclusively on the educational and STEM-filtered subset of the same corpus, totaling just 10 billion tokens. Despite using only 10% of the data and 20% of the computation, Curió-Edu 7B surpasses the full-corpus model in our evaluations, demonstrating that data selection can be fundamental even when adapting models with limited prior exposure to the target language. The developed models are available at https://huggingface.co/collections/ClassiCC-Corpus/curio-edu</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T17:19:32Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Thales Sales Almeida</name>
    </author>
    <author>
      <name>Rodrigo Nogueira</name>
    </author>
    <author>
      <name>Hélio Pedrini</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12769v1</id>
    <title>Adaptive Edge-Cloud Inference for Speech-to-Action Systems Using ASR and Large Language Models (ASTA)</title>
    <updated>2025-12-14T17:07:23Z</updated>
    <link href="https://arxiv.org/abs/2512.12769v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12769v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Voice-based interaction has emerged as a natural and intuitive modality for controlling IoT devices. However, speech-driven edge devices face a fundamental trade-off between cloud-based solutions, which offer stronger language understanding capabilities at the cost of latency, connectivity dependence, and privacy concerns, and edge-based solutions, which provide low latency and improved privacy but are limited by computational constraints. This paper presents ASTA, an adaptive speech-to-action solution that dynamically routes voice commands between edge and cloud inference to balance performance and system resource utilization. ASTA integrates on-device automatic speech recognition and lightweight offline language-model inference with cloud-based LLM processing, guided by real-time system metrics such as CPU workload, device temperature, and network latency. A metric-aware routing mechanism selects the inference path at runtime, while a rule-based command validation and repair component ensures successful end-to-end command execution. We implemented our solution on an NVIDIA Jetson-based edge platform and evaluated it using a diverse dataset of 80 spoken commands. Experimental results show that ASTA successfully routes all input commands for execution, achieving a balanced distribution between online and offline inference. The system attains an ASR accuracy of 62.5% and generates executable commands without repair for only 47.5% of inputs, highlighting the importance of the repair mechanism in improving robustness. These results suggest that adaptive edge-cloud orchestration is a viable approach for resilient and resource-aware voice-controlled IoT systems.</summary>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T17:07:23Z</published>
    <arxiv:comment>preprint, 6 pages, 7 figures, 1 table</arxiv:comment>
    <arxiv:primary_category term="cs.SD"/>
    <author>
      <name>Mohammad Jalili Torkamani</name>
    </author>
    <author>
      <name>Israt Zarin</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12768v1</id>
    <title>CoRe3D: Collaborative Reasoning as a Foundation for 3D Intelligence</title>
    <updated>2025-12-14T17:05:11Z</updated>
    <link href="https://arxiv.org/abs/2512.12768v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12768v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Recent advances in large multimodal models suggest that explicit reasoning mechanisms play a critical role in improving model reliability, interpretability, and cross-modal alignment. While such reasoning-centric approaches have been proven effective in language and vision tasks, their extension to 3D remains underdeveloped. CoRe3D introduces a unified 3D understanding and generation reasoning framework that jointly operates over semantic and spatial abstractions, enabling high-level intent inferred from language to directly guide low-level 3D content formation. Central to this design is a spatially grounded reasoning representation that decomposes 3D latent space into localized regions, allowing the model to reason over geometry in a compositional and procedural manner. By tightly coupling semantic chain-of-thought inference with structured spatial reasoning, CoRe3D produces 3D outputs that exhibit strong local consistency and faithful alignment with linguistic descriptions.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T17:05:11Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Tianjiao Yu</name>
    </author>
    <author>
      <name>Xinzhuo Li</name>
    </author>
    <author>
      <name>Yifan Shen</name>
    </author>
    <author>
      <name>Yuanzhe Liu</name>
    </author>
    <author>
      <name>Ismini Lourentzou</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12762v1</id>
    <title>Federated Learning with Feedback Alignment</title>
    <updated>2025-12-14T16:59:55Z</updated>
    <link href="https://arxiv.org/abs/2512.12762v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12762v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Federated Learning (FL) enables collaborative training across multiple clients while preserving data privacy, yet it struggles with data heterogeneity, where clients' data are not distributed independently and identically (non-IID). This causes local drift, hindering global model convergence. To address this, we introduce Federated Learning with Feedback Alignment (FLFA), a novel framework that integrates feedback alignment into FL. FLFA uses the global model's weights as a shared feedback matrix during local training's backward pass, aligning local updates with the global model efficiently. This approach mitigates local drift with minimal additional computational cost and no extra communication overhead.
  Our theoretical analysis supports FLFA's design by showing how it alleviates local drift and demonstrates robust convergence for both local and global models. Empirical evaluations, including accuracy comparisons and measurements of local drift, further illustrate that FLFA can enhance other FL methods demonstrating its effectiveness.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T16:59:55Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Incheol Baek</name>
    </author>
    <author>
      <name>Hyungbin Kim</name>
    </author>
    <author>
      <name>Minseo Kim</name>
    </author>
    <author>
      <name>Yon Dohn Chung</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12760v1</id>
    <title>Intelligent Scientific Literature Explorer using Machine Learning (ISLE)</title>
    <updated>2025-12-14T16:54:24Z</updated>
    <link href="https://arxiv.org/abs/2512.12760v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12760v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The rapid acceleration of scientific publishing has created substantial challenges for researchers attempting to discover, contextualize, and interpret relevant literature. Traditional keyword-based search systems provide limited semantic understanding, while existing AI-driven tools typically focus on isolated tasks such as retrieval, clustering, or bibliometric visualization. This paper presents an integrated system for scientific literature exploration that combines large-scale data acquisition, hybrid retrieval, semantic topic modeling, and heterogeneous knowledge graph construction. The system builds a comprehensive corpus by merging full-text data from arXiv with structured metadata from OpenAlex. A hybrid retrieval architecture fuses BM25 lexical search with embedding-based semantic search using Reciprocal Rank Fusion. Topic modeling is performed on retrieved results using BERTopic or non-negative matrix factorization depending on computational resources. A knowledge graph unifies papers, authors, institutions, countries, and extracted topics into an interpretable structure. The system provides a multi-layered exploration environment that reveals not only relevant publications but also the conceptual and relational landscape surrounding a query. Evaluation across multiple queries demonstrates improvements in retrieval relevance, topic coherence, and interpretability. The proposed framework contributes an extensible foundation for AI-assisted scientific discovery.</summary>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T16:54:24Z</published>
    <arxiv:comment>18 pages, 7 figures, 3 tables</arxiv:comment>
    <arxiv:primary_category term="cs.IR"/>
    <author>
      <name>Sina Jani</name>
    </author>
    <author>
      <name>Arman Heidari</name>
    </author>
    <author>
      <name>Amirmohammad Anvari</name>
    </author>
    <author>
      <name>Zahra Rahimi</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12756v1</id>
    <title>FysicsWorld: A Unified Full-Modality Benchmark for Any-to-Any Understanding, Generation, and Reasoning</title>
    <updated>2025-12-14T16:41:29Z</updated>
    <link href="https://arxiv.org/abs/2512.12756v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12756v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Despite rapid progress in multimodal large language models (MLLMs) and emerging omni-modal architectures, current benchmarks remain limited in scope and integration, suffering from incomplete modality coverage, restricted interaction to text-centric outputs, and weak interdependence and complementarity among modalities. To bridge these gaps, we introduce FysicsWorld, the first unified full-modality benchmark that supports bidirectional input-output across image, video, audio, and text, enabling comprehensive any-to-any evaluation across understanding, generation, and reasoning. FysicsWorld encompasses 16 primary tasks and 3,268 curated samples, aggregated from over 40 high-quality sources and covering a rich set of open-domain categories with diverse question types. We also propose the Cross-Modal Complementarity Screening (CMCS) strategy integrated in a systematic data construction framework that produces omni-modal data for spoken interaction and fusion-dependent cross-modal reasoning. Through a comprehensive evaluation of over 30 state-of-the-art baselines, spanning MLLMs, modality-specific models, unified understanding-generation models, and omni-modal language models, FysicsWorld exposes the performance disparities and limitations across models in understanding, generation, and reasoning. Our benchmark establishes a unified foundation and strong baselines for evaluating and advancing next-generation full-modality architectures.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T16:41:29Z</published>
    <arxiv:comment>The omni-modal benchmark report from Fysics AI</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Yue Jiang</name>
    </author>
    <author>
      <name>Dingkang Yang</name>
    </author>
    <author>
      <name>Minghao Han</name>
    </author>
    <author>
      <name>Jinghang Han</name>
    </author>
    <author>
      <name>Zizhi Chen</name>
    </author>
    <author>
      <name>Yizhou Liu</name>
    </author>
    <author>
      <name>Mingcheng Li</name>
    </author>
    <author>
      <name>Peng Zhai</name>
    </author>
    <author>
      <name>Lihua Zhang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12755v1</id>
    <title>An End-to-End Approach for Microgrid Probabilistic Forecasting and Robust Operation via Decision-focused Learning</title>
    <updated>2025-12-14T16:36:04Z</updated>
    <link href="https://arxiv.org/abs/2512.12755v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12755v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>High penetration of renewable energy sources (RES) introduces significant uncertainty and intermittency into microgrid operations, posing challenges to economic and reliable scheduling. To address this, this paper proposes an end-to-end decision-focused framework that jointly optimizes probabilistic forecasting and robust operation for microgrids. A multilayer encoder-decoder (MED) probabilistic forecasting model is integrated with a two-stage robust optimization (TSRO) model involving direct load control (DLC) through a differentiable decision pathway, enabling gradient-based feedback from operational outcomes to improve forecasting performance. Unlike conventional sequential approaches, the proposed method aligns forecasting accuracy with operational objectives by directly minimizing decision regret via a surrogate smart predict-then-optimize (SPO) loss function. This integration ensures that probabilistic forecasts are optimized for downstream decisions, enhancing both economic efficiency and robustness. Case studies on modified IEEE 33-bus and 69-bus systems demonstrate that the proposed framework achieves superior forecasting accuracy and operational performance, reducing total and net operation costs by up to 18% compared with conventional forecasting and optimization combinations. The results verify the effectiveness and scalability of the end-to-end decision-focused approach for resilient and cost-efficient microgrid management under uncertainty.</summary>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T16:36:04Z</published>
    <arxiv:comment>10 pages</arxiv:comment>
    <arxiv:primary_category term="eess.SY"/>
    <author>
      <name>Tingwei Cao</name>
    </author>
    <author>
      <name>Yan Xu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12751v1</id>
    <title>GenieDrive: Towards Physics-Aware Driving World Model with 4D Occupancy Guided Video Generation</title>
    <updated>2025-12-14T16:23:51Z</updated>
    <link href="https://arxiv.org/abs/2512.12751v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12751v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Physics-aware driving world model is essential for drive planning, out-of-distribution data synthesis, and closed-loop evaluation. However, existing methods often rely on a single diffusion model to directly map driving actions to videos, which makes learning difficult and leads to physically inconsistent outputs. To overcome these challenges, we propose GenieDrive, a novel framework designed for physics-aware driving video generation. Our approach starts by generating 4D occupancy, which serves as a physics-informed foundation for subsequent video generation. 4D occupancy contains rich physical information, including high-resolution 3D structures and dynamics. To facilitate effective compression of such high-resolution occupancy, we propose a VAE that encodes occupancy into a latent tri-plane representation, reducing the latent size to only 58% of that used in previous methods. We further introduce Mutual Control Attention (MCA) to accurately model the influence of control on occupancy evolution, and we jointly train the VAE and the subsequent prediction module in an end-to-end manner to maximize forecasting accuracy. Together, these designs yield a 7.2% improvement in forecasting mIoU at an inference speed of 41 FPS, while using only 3.47 M parameters. Additionally, a Normalized Multi-View Attention is introduced in the video generation model to generate multi-view driving videos with guidance from our 4D occupancy, significantly improving video quality with a 20.7% reduction in FVD. Experiments demonstrate that GenieDrive enables highly controllable, multi-view consistent, and physics-aware driving video generation.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T16:23:51Z</published>
    <arxiv:comment>The project page is available at https://huster-yzy.github.io/geniedrive_project_page/</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Zhenya Yang</name>
    </author>
    <author>
      <name>Zhe Liu</name>
    </author>
    <author>
      <name>Yuxiang Lu</name>
    </author>
    <author>
      <name>Liping Hou</name>
    </author>
    <author>
      <name>Chenxuan Miao</name>
    </author>
    <author>
      <name>Siyi Peng</name>
    </author>
    <author>
      <name>Bailan Feng</name>
    </author>
    <author>
      <name>Xiang Bai</name>
    </author>
    <author>
      <name>Hengshuang Zhao</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12749v1</id>
    <title>Flow-matching Operators for Residual-Augmented Probabilistic Learning of Partial Differential Equations</title>
    <updated>2025-12-14T16:06:10Z</updated>
    <link href="https://arxiv.org/abs/2512.12749v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12749v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Learning probabilistic surrogates for PDEs remains challenging in data-scarce regimes: neural operators require large amounts of high-fidelity data, while generative approaches typically sacrifice resolution invariance. We formulate flow matching in an infinite-dimensional function space to learn a probabilistic transport that maps low-fidelity approximations to the manifold of high-fidelity PDE solutions via learned residual corrections. We develop a conditional neural operator architecture based on feature-wise linear modulation for flow-matching vector fields directly in function space, enabling inference at arbitrary spatial resolutions without retraining. To improve stability and representational control of the induced neural ODE, we parameterize the flow vector field as a sum of a linear operator and a nonlinear operator, combining lightweight linear components with a conditioned Fourier neural operator for expressive, input-dependent dynamics. We then formulate a residual-augmented learning strategy where the flow model learns probabilistic corrections from inexpensive low-fidelity surrogates to high-fidelity solutions, rather than learning the full solution mapping from scratch. Finally, we derive tractable training objectives that extend conditional flow matching to the operator setting with input-function-dependent couplings. To demonstrate the effectiveness of our approach, we present numerical experiments on a range of PDEs, including the 1D advection and Burgers' equation, and a 2D Darcy flow problem for flow through a porous medium.
  We show that the proposed method can accurately learn solution operators across different resolutions and fidelities and produces uncertainty estimates that appropriately reflect model confidence, even when trained on limited high-fidelity data.</summary>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T16:06:10Z</published>
    <arxiv:primary_category term="stat.CO"/>
    <author>
      <name>Sahil Bhola</name>
    </author>
    <author>
      <name>Karthik Duraisamy</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12744v1</id>
    <title>Resting Neurons, Active Insights: Improving Input Sparsification for Large Language Models</title>
    <updated>2025-12-14T15:47:40Z</updated>
    <link href="https://arxiv.org/abs/2512.12744v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12744v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Large Language Models (LLMs) achieve state-of-the-art performance across a wide range of applications, but their massive scale poses significant challenges for both efficiency and interpretability. Structural pruning, which reduces model size by removing redundant computational units such as neurons, has been widely explored as a solution, and this study devotes to input sparsification, an increasingly popular technique that improves efficiency by selectively activating only a subset of entry values for each input. However, existing approaches focus primarily on computational savings, often overlooking the representational consequences of sparsification and leaving a noticeable performance gap compared to full models. In this work, we first reinterpret input sparsification as a form of dynamic structural pruning. Motivated by the spontaneous baseline firing rates observed in biological neurons, we introduce a small set of trainable spontaneous neurons that act as compensatory units to stabilize activations in sparsified LLMs. Experiments demonstrate that these auxiliary neurons substantially reduce the sparsification-induced performance gap while generalizing effectively across tasks.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T15:47:40Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Haotian Xu</name>
    </author>
    <author>
      <name>Tian Gao</name>
    </author>
    <author>
      <name>Tsui-Wei Weng</name>
    </author>
    <author>
      <name>Tengfei Ma</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12742v1</id>
    <title>Transport Reversible Jump Markov Chain Monte Carlo with proposals generated by Variational Inference with Normalizing Flows</title>
    <updated>2025-12-14T15:38:47Z</updated>
    <link href="https://arxiv.org/abs/2512.12742v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12742v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We present a framework using variational inference with normalizing flows (VI-NFs) to generate proposals of reversible jump Markov chain Monte Carlo (RJMCMC) for efficient trans-dimensional Bayesian inference. Unlike transport reversible jump methods relying on forward KL minimization with pilot MCMC samples, our approach minimizes the reverse KL divergence which requires only samples from a base distribution, eliminating costly target sampling. The method employs RealNVP-based flows to learn model-specific transport maps, enabling construction of both between-model and within-model proposals. Our framework provides accurate marginal likelihood estimates from the variational approximation. This facilitates efficient model comparison and proposal adaptation in RJMCMC. Experiments on illustrative example, factor analysis and variable selection tasks in linear regression show that TRJ designed by VI-NFs achieves faster mixing and more efficient model space exploration compared to existing baselines. The proposed algorithm can be extended to conditional flows for amortized vairiational inference across models. Code is available at https://github.com/YinPingping111/TRJ_VINFs.</summary>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T15:38:47Z</published>
    <arxiv:primary_category term="stat.ML"/>
    <author>
      <name>Pingping Yin</name>
    </author>
    <author>
      <name>Xiyun Jiao</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12737v1</id>
    <title>SPARK: Igniting Communication-Efficient Decentralized Learning via Stage-wise Projected NTK and Accelerated Regularization</title>
    <updated>2025-12-14T15:21:31Z</updated>
    <link href="https://arxiv.org/abs/2512.12737v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12737v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Decentralized federated learning (DFL) faces critical challenges from statistical heterogeneity and communication overhead. While NTK-based methods achieve faster convergence, transmitting full Jacobian matrices is impractical for bandwidth-constrained edge networks. We propose SPARK, synergistically integrating random projection-based Jacobian compression, stage-wise annealed distillation, and Nesterov momentum acceleration. Random projections compress Jacobians while preserving spectral properties essential for convergence. Stage-wise annealed distillation transitions from pure NTK evolution to neighbor-regularized learning, counteracting compression noise. Nesterov momentum accelerates convergence through stable accumulation enabled by distillation smoothing. SPARK achieves 98.7% communication reduction compared to NTK-DFL while maintaining convergence speed and superior accuracy. With momentum, SPARK reaches target performance 3 times faster, establishing state-of-the-art results for communication-efficient decentralized learning and enabling practical deployment in bandwidth-limited edge environments.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T15:21:31Z</published>
    <arxiv:comment>11 pages, 8 figures</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Li Xia</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12736v1</id>
    <title>Personalized QoE Prediction: A Demographic-Augmented Machine Learning Framework for 5G Video Streaming Networks</title>
    <updated>2025-12-14T15:19:16Z</updated>
    <link href="https://arxiv.org/abs/2512.12736v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12736v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Quality of Experience (QoE) prediction is a critical component of modern multimedia systems, particularly for adaptive video streaming in 5G networks. Accurate QoE estimation enables intelligent resource management and supports user centric service delivery. Existing QoE prediction approaches primarily rely on limited datasets and assume uniform user perception, which restricts their applicability in heterogeneous real world environments.
  This paper proposes a demographic aware machine learning framework for personalized QoE prediction. We introduce a behaviorally realistic demographic based data augmentation strategy that expands a small QoE dataset six fold by modeling varying user sensitivities to streaming impairments such as rebuffering, bitrate variation, and quality degradation. Using the augmented dataset, we evaluate a comprehensive set of classical machine learning models alongside advanced deep learning architectures, including an attention-based MLP and TabNet.
  Experimental results demonstrate significant improvements in prediction accuracy across RMSE, MAE, and R metrics compared to baseline models. Among all evaluated approaches, TabNet achieves the strongest performance, benefiting from its inherent feature selection and attention mechanisms. The results confirm that demographic-aware augmentation substantially enhances QoE prediction robustness and provides a scalable direction for personalized QoE-aware intelligence in 5G video streaming networks.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T15:19:16Z</published>
    <arxiv:comment>11 pages, 5 figures</arxiv:comment>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Syeda Zunaira Ahmed</name>
    </author>
    <author>
      <name>Hejab Tahira Beg</name>
    </author>
    <author>
      <name>Maryam Khalid</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12735v1</id>
    <title>Limits To (Machine) Learning</title>
    <updated>2025-12-14T15:18:45Z</updated>
    <link href="https://arxiv.org/abs/2512.12735v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12735v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Machine learning (ML) methods are highly flexible, but their ability to approximate the true data-generating process is fundamentally constrained by finite samples. We characterize a universal lower bound, the Limits-to-Learning Gap (LLG), quantifying the unavoidable discrepancy between a model's empirical fit and the population benchmark. Recovering the true population $R^2$, therefore, requires correcting observed predictive performance by this bound. Using a broad set of variables, including excess returns, yields, credit spreads, and valuation ratios, we find that the implied LLGs are large. This indicates that standard ML approaches can substantially understate true predictability in financial data. We also derive LLG-based refinements to the classic Hansen and Jagannathan (1991) bounds, analyze implications for parameter learning in general-equilibrium settings, and show that the LLG provides a natural mechanism for generating excess volatility.</summary>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T15:18:45Z</published>
    <arxiv:primary_category term="stat.ML"/>
    <author>
      <name>Zhimin Chen</name>
    </author>
    <author>
      <name>Bryan Kelly</name>
    </author>
    <author>
      <name>Semyon Malamud</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12732v1</id>
    <title>Ethical Risk Analysis of L2 Rollups</title>
    <updated>2025-12-14T15:17:23Z</updated>
    <link href="https://arxiv.org/abs/2512.12732v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12732v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Layer 2 rollups improve throughput and fees, but can reintroduce risk through operator discretion and information asymmetry. We ask which operator and governance designs produce ethically problematic user risk. We adapt Ethical Risk Analysis to rollup architectures, build a role-based taxonomy of decision authority and exposure, and pair the framework with two empirical signals, a cross sectional snapshot of 129 projects from L2BEAT and a hand curated incident set covering 2022 to 2025. We analyze mechanisms that affect risks to users funds, including upgrade timing and exit windows, proposer liveness and whitelisting, forced inclusion usability, and data availability choices. We find that ethical hazards rooted in L2 components control arrangements are widespread: instant upgrades without exit windows appear in about 86 percent of projects, and proposer controls that can freeze withdrawals in about 50 percent. Reported incidents concentrate in sequencer liveness and inclusion, consistent with these dependencies. We translate these findings into ethically grounded suggestions on mitigation strategies including technical components and governance mechanisms.</summary>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T15:17:23Z</published>
    <arxiv:primary_category term="cs.DC"/>
    <author>
      <name>Georgy Ishmaev</name>
    </author>
    <author>
      <name>Emmanuelle Anceaume</name>
    </author>
    <author>
      <name>Davide Frey</name>
    </author>
    <author>
      <name>François Taïani</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12731v1</id>
    <title>Solving a Machine Learning Regression Problem Based on the Theory of Random Functions</title>
    <updated>2025-12-14T15:12:18Z</updated>
    <link href="https://arxiv.org/abs/2512.12731v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12731v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>This paper studies a machine learning regression problem as a multivariate approximation problem using the framework of the theory of random functions. An ab initio derivation of a regression method is proposed, starting from postulates of indifference. It is shown that if a probability measure on an infinite-dimensional function space possesses natural symmetries (invariance under translation, rotation, scaling, and Gaussianity), then the entire solution scheme, including the kernel form, the type of regularization, and the noise parameterization, follows analytically from these postulates. The resulting kernel coincides with a generalized polyharmonic spline; however, unlike existing approaches, it is not chosen empirically but arises as a consequence of the indifference principle. This result provides a theoretical foundation for a broad class of smoothing and interpolation methods, demonstrating their optimality in the absence of a priori information.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T15:12:18Z</published>
    <arxiv:comment>Part 1 of 4 in the "Polyharmonic Cascade" cycle. 25 pages, 2 figures. Source code is available at: https://github.com/xolod7/polyharmonic-cascade</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Yuriy N. Bakhvalov</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12730v1</id>
    <title>NL2Repo-Bench: Towards Long-Horizon Repository Generation Evaluation of Coding Agents</title>
    <updated>2025-12-14T15:12:13Z</updated>
    <link href="https://arxiv.org/abs/2512.12730v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12730v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Recent advances in coding agents suggest rapid progress toward autonomous software development, yet existing benchmarks fail to rigorously evaluate the long-horizon capabilities required to build complete software systems. Most prior evaluations focus on localized code generation, scaffolded completion, or short-term repair tasks, leaving open the question of whether agents can sustain coherent reasoning, planning, and execution over the extended horizons demanded by real-world repository construction. To address this gap, we present NL2Repo Bench, a benchmark explicitly designed to evaluate the long-horizon repository generation ability of coding agents. Given only a single natural-language requirements document and an empty workspace, agents must autonomously design the architecture, manage dependencies, implement multi-module logic, and produce a fully installable Python library. Our experiments across state-of-the-art open- and closed-source models reveal that long-horizon repository generation remains largely unsolved: even the strongest agents achieve below 40% average test pass rates and rarely complete an entire repository correctly. Detailed analysis uncovers fundamental long-horizon failure modes, including premature termination, loss of global coherence, fragile cross-file dependencies, and inadequate planning over hundreds of interaction steps. NL2Repo Bench establishes a rigorous, verifiable testbed for measuring sustained agentic competence and highlights long-horizon reasoning as a central bottleneck for the next generation of autonomous coding agents.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T15:12:13Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Jingzhe Ding</name>
    </author>
    <author>
      <name>Shengda Long</name>
    </author>
    <author>
      <name>Changxin Pu</name>
    </author>
    <author>
      <name>Huan Zhou</name>
    </author>
    <author>
      <name>Hongwan Gao</name>
    </author>
    <author>
      <name>Xiang Gao</name>
    </author>
    <author>
      <name>Chao He</name>
    </author>
    <author>
      <name>Yue Hou</name>
    </author>
    <author>
      <name>Fei Hu</name>
    </author>
    <author>
      <name>Zhaojian Li</name>
    </author>
    <author>
      <name>Weiran Shi</name>
    </author>
    <author>
      <name>Zaiyuan Wang</name>
    </author>
    <author>
      <name>Daoguang Zan</name>
    </author>
    <author>
      <name>Chenchen Zhang</name>
    </author>
    <author>
      <name>Xiaoxu Zhang</name>
    </author>
    <author>
      <name>Qizhi Chen</name>
    </author>
    <author>
      <name>Xianfu Cheng</name>
    </author>
    <author>
      <name>Bo Deng</name>
    </author>
    <author>
      <name>Qingshui Gu</name>
    </author>
    <author>
      <name>Kai Hua</name>
    </author>
    <author>
      <name>Juntao Lin</name>
    </author>
    <author>
      <name>Pai Liu</name>
    </author>
    <author>
      <name>Mingchen Li</name>
    </author>
    <author>
      <name>Xuanguang Pan</name>
    </author>
    <author>
      <name>Zifan Peng</name>
    </author>
    <author>
      <name>Yujia Qin</name>
    </author>
    <author>
      <name>Yong Shan</name>
    </author>
    <author>
      <name>Zhewen Tan</name>
    </author>
    <author>
      <name>Weihao Xie</name>
    </author>
    <author>
      <name>Zihan Wang</name>
    </author>
    <author>
      <name>Yishuo Yuan</name>
    </author>
    <author>
      <name>Jiayu Zhang</name>
    </author>
    <author>
      <name>Enduo Zhao</name>
    </author>
    <author>
      <name>Yunfei Zhao</name>
    </author>
    <author>
      <name>He Zhu</name>
    </author>
    <author>
      <name>Chenyang Zou</name>
    </author>
    <author>
      <name>Ming Ding</name>
    </author>
    <author>
      <name>Jianpeng Jiao</name>
    </author>
    <author>
      <name>Jiaheng Liu</name>
    </author>
    <author>
      <name>Minghao Liu</name>
    </author>
    <author>
      <name>Qian Liu</name>
    </author>
    <author>
      <name>Chongyao Tao</name>
    </author>
    <author>
      <name>Jian Yang</name>
    </author>
    <author>
      <name>Tong Yang</name>
    </author>
    <author>
      <name>Zhaoxiang Zhang</name>
    </author>
    <author>
      <name>Xinjie Chen</name>
    </author>
    <author>
      <name>Wenhao Huang</name>
    </author>
    <author>
      <name>Ge Zhang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12729v1</id>
    <title>RunPBA -- Runtime attestation for microcontrollers with PACBTI</title>
    <updated>2025-12-14T15:09:48Z</updated>
    <link href="https://arxiv.org/abs/2512.12729v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12729v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The widespread adoption of embedded systems has led to their deployment in critical real-world applications, making them attractive targets for malicious actors. These devices face unique challenges in mitigating vulnerabilities due to intrinsic constraints, such as low energy consumption requirements and limited computational resources. This paper presents RunPBA, a hardware-based runtime attestation system designed to defend against control flow attacks while maintaining minimal performance overhead and adhering to strict power consumption constraints. RunPBA leverages PACBTI, a new processor extension tailored for the Arm Cortex M processor family, allowing robust protection without requiring hardware modifications, a limitation present in similar solutions. We implemented a proof-of-concept and evaluated it using two benchmark suites. Experimental results indicate that RunPBA imposes a geometric mean performance overhead of only 1% and 4.7% across the benchmarks, underscoring its efficiency and suitability for real-world deployment.</summary>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T15:09:48Z</published>
    <arxiv:primary_category term="cs.CR"/>
    <author>
      <name>André Cirne</name>
    </author>
    <author>
      <name>Patrícia R. Sousa</name>
    </author>
    <author>
      <name>João S. Resende</name>
    </author>
    <author>
      <name>Luís Antunes</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12719v1</id>
    <title>Towards AI Agents Supported Research Problem Formulation</title>
    <updated>2025-12-14T14:44:27Z</updated>
    <link href="https://arxiv.org/abs/2512.12719v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12719v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Poorly formulated research problems can compromise the practical relevance of Software Engineering studies by not reflecting the complexities of industrial practice. This vision paper explores the use of artificial intelligence agents to support SE researchers during the early stage of a research project, the formulation of the research problem. Based on the Lean Research Inception framework and using a published study on code maintainability in machine learning as a reference, we developed a descriptive evaluation of a scenario illustrating how AI agents, integrated into LRI, can support SE researchers by pre filling problem attributes, aligning stakeholder perspectives, refining research questions, simulating multiperspective assessments, and supporting decision making. The descriptive evaluation of the scenario suggests that AI agent support can enrich collaborative discussions and enhance critical reflection on the value, feasibility, and applicability of the research problem. Although the vision of integrating AI agents into LRI was perceived as promising to support the context aware and practice oriented formulation of research problems, empirical validation is needed to confirm and refine the integration of AI agents into problem formulation.</summary>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T14:44:27Z</published>
    <arxiv:primary_category term="cs.SE"/>
    <author>
      <name>Anrafel Fernandes Pereira</name>
    </author>
    <author>
      <name>Maria Teresa Baldassarre</name>
    </author>
    <author>
      <name>Daniel Mendez</name>
    </author>
    <author>
      <name>Marcos Kalinowski</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12718v1</id>
    <title>Spinal Line Detection for Posture Evaluation through Train-ing-free 3D Human Body Reconstruction with 2D Depth Images</title>
    <updated>2025-12-14T14:43:42Z</updated>
    <link href="https://arxiv.org/abs/2512.12718v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12718v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The spinal angle is an important indicator of body balance. It is important to restore the 3D shape of the human body and estimate the spine center line. Existing mul-ti-image-based body restoration methods require expensive equipment and complex pro-cedures, and single image-based body restoration methods have limitations in that it is difficult to accurately estimate the internal structure such as the spine center line due to occlusion and viewpoint limitation. This study proposes a method to compensate for the shortcomings of the multi-image-based method and to solve the limitations of the sin-gle-image method. We propose a 3D body posture analysis system that integrates depth images from four directions to restore a 3D human model and automatically estimate the spine center line. Through hierarchical matching of global and fine registration, restora-tion to noise and occlusion is performed. Also, the Adaptive Vertex Reduction is applied to maintain the resolution and shape reliability of the mesh, and the accuracy and stabil-ity of spinal angle estimation are simultaneously secured by using the Level of Detail en-semble. The proposed method achieves high-precision 3D spine registration estimation without relying on training data or complex neural network models, and the verification confirms the improvement of matching quality.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T14:43:42Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Sehyun Kim</name>
    </author>
    <author>
      <name>Hye Jun Lee</name>
    </author>
    <author>
      <name>Jiwoo Lee</name>
    </author>
    <author>
      <name>Changgyun Kim</name>
    </author>
    <author>
      <name>Taemin Lee</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12716v1</id>
    <title>CoDA: A Context-Decoupled Hierarchical Agent with Reinforcement Learning</title>
    <updated>2025-12-14T14:41:29Z</updated>
    <link href="https://arxiv.org/abs/2512.12716v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12716v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Large Language Model (LLM) agents trained with reinforcement learning (RL) show great promise for solving complex, multi-step tasks. However, their performance is often crippled by "Context Explosion", where the accumulation of long text outputs overwhelms the model's context window and leads to reasoning failures. To address this, we introduce CoDA, a Context-Decoupled hierarchical Agent, a simple but effective reinforcement learning framework that decouples high-level planning from low-level execution. It employs a single, shared LLM backbone that learns to operate in two distinct, contextually isolated roles: a high-level Planner that decomposes tasks within a concise strategic context, and a low-level Executor that handles tool interactions in an ephemeral, isolated workspace. We train this unified agent end-to-end using PECO (Planner-Executor Co-Optimization), a reinforcement learning methodology that applies a trajectory-level reward to jointly optimize both roles, fostering seamless collaboration through context-dependent policy updates. Extensive experiments demonstrate that CoDA achieves significant performance improvements over state-of-the-art baselines on complex multi-hop question-answering benchmarks, and it exhibits strong robustness in long-context scenarios, maintaining stable performance while all other baselines suffer severe degradation, thus further validating the effectiveness of our hierarchical design in mitigating context overload.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T14:41:29Z</published>
    <arxiv:comment>Accepted to WSDM '26 Oral</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Xuanzhang Liu</name>
    </author>
    <author>
      <name>Jianglun Feng</name>
    </author>
    <author>
      <name>Zhuoran Zhuang</name>
    </author>
    <author>
      <name>Junzhe Zhao</name>
    </author>
    <author>
      <name>Maofei Que</name>
    </author>
    <author>
      <name>Jieting Li</name>
    </author>
    <author>
      <name>Dianlei Wang</name>
    </author>
    <author>
      <name>Hao Tong</name>
    </author>
    <author>
      <name>Ye Chen</name>
    </author>
    <author>
      <name>Pan Li</name>
    </author>
    <arxiv:doi>10.1145/3773966.3777986</arxiv:doi>
    <link rel="related" href="https://doi.org/10.1145/3773966.3777986" title="doi"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12713v1</id>
    <title>Self-Motivated Growing Neural Network for Adaptive Architecture via Local Structural Plasticity</title>
    <updated>2025-12-14T14:31:21Z</updated>
    <link href="https://arxiv.org/abs/2512.12713v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12713v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Control policies in deep reinforcement learning are often implemented with fixed-capacity multilayer perceptrons trained by backpropagation, which lack structural plasticity and depend on global error signals. This paper introduces the Self-Motivated Growing Neural Network (SMGrNN), a controller whose topology evolves online through a local Structural Plasticity Module (SPM). The SPM monitors neuron activations and edge-wise weight update statistics over short temporal windows and uses these signals to trigger neuron insertion and pruning, while synaptic weights are updated by a standard gradient-based optimizer. This allows network capacity to be regulated during learning without manual architectural tuning.
  SMGrNN is evaluated on control benchmarks via policy distillation. Compared with multilayer perceptron baselines, it achieves similar or higher returns, lower variance, and task-appropriate network sizes. Ablation studies with growth disabled and growth-only variants isolate the role of structural plasticity, showing that adaptive topology improves reward stability. The local and modular design of SPM enables future integration of a Hebbian plasticity module and spike-timing-dependent plasticity, so that SMGrNN can support both artificial and spiking neural implementations driven by local rules.</summary>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T14:31:21Z</published>
    <arxiv:primary_category term="cs.NE"/>
    <author>
      <name>Yiyang Jia</name>
    </author>
    <author>
      <name>Chengxu Zhou</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12710v1</id>
    <title>Practical Hybrid Quantum Language Models with Observable Readout on Real Hardware</title>
    <updated>2025-12-14T14:22:44Z</updated>
    <link href="https://arxiv.org/abs/2512.12710v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12710v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Hybrid quantum-classical models represent a crucial step toward leveraging near-term quantum devices for sequential data processing. We present Quantum Recurrent Neural Networks (QRNNs) and Quantum Convolutional Neural Networks (QCNNs) as hybrid quantum language models, reporting the first empirical demonstration of generative language modeling trained and evaluated end-to-end on real quantum hardware. Our architecture combines hardware-optimized parametric quantum circuits with a lightweight classical projection layer, utilizing a multi-sample SPSA strategy to efficiently train quantum parameters despite hardware noise. To characterize the capabilities of these models, we introduce a synthetic dataset designed to isolate syntactic dependencies in a controlled, low-resource environment. Experiments on IBM Quantum processors reveal the critical trade-offs between circuit depth and trainability, demonstrating that while noise remains a significant factor, observable-based readout enables the successful learning of sequential patterns on NISQ devices. These results establish a rigorous engineering baseline for generative quantum natural language processing, validating the feasibility of training complex sequence models on current quantum hardware.</summary>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T14:22:44Z</published>
    <arxiv:primary_category term="quant-ph"/>
    <author>
      <name>Stefan Balauca</name>
    </author>
    <author>
      <name>Ada-Astrid Balauca</name>
    </author>
    <author>
      <name>Adrian Iftene</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12708v1</id>
    <title>Multi-Trajectory Physics-Informed Neural Networks for HJB Equations with Hard-Zero Terminal Inventory: Optimal Execution on Synthetic &amp; SPY Data</title>
    <updated>2025-12-14T14:20:58Z</updated>
    <link href="https://arxiv.org/abs/2512.12708v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12708v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We study optimal trade execution with a hard-zero terminal inventory constraint, modeled via Hamilton-Jacobi-Bellman (HJB) equations. Vanilla PINNs often under-enforce this constraint and produce unstable controls. We propose a Multi-Trajectory PINN (MT-PINN) that adds a rollout-based trajectory loss and propagates a terminal penalty on terminal inventory via backpropagation-through-time, directly enforcing zero terminal inventory. A lightweight lambda-curriculum is adopted to stabilize training as the state expands from a risk-neutral reduced HJB to a risk-averse HJB. On the Gatheral-Schied single-asset model, MT-PINN aligns closely with their derived closed-form solutions and concentrates terminal inventory tightly around zero while reducing errors along optimal paths. We apply MT-PINNs on SPY intraday data, matching TWAP when risk-neutral, and achieving lower exposure and competitive costs, especially in falling windows, for higher risk-aversion.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T14:20:58Z</published>
    <arxiv:comment>24 pages, 19 figures. Accepted to the NeurIPS 2025 Workshop on Generative AI in Finance</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Anthime Valin</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12706v1</id>
    <title>Synergizing Code Coverage and Gameplay Intent: Coverage-Aware Game Playtesting with LLM-Guided Reinforcement Learning</title>
    <updated>2025-12-14T14:18:18Z</updated>
    <link href="https://arxiv.org/abs/2512.12706v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12706v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The widespread adoption of the "Games as a Service" model necessitates frequent content updates, placing immense pressure on quality assurance. In response, automated game testing has been viewed as a promising solution to cope with this demanding release cadence. However, existing automated testing approaches typically create a dichotomy: code-centric methods focus on structural coverage without understanding gameplay context, while player-centric agents validate high-level intent but often fail to cover specific underlying code changes. To bridge this gap, we propose SMART (Structural Mapping for Augmented Reinforcement Testing), a novel framework that synergizes structural verification and functional validation for game update testing. SMART leverages large language models (LLMs) to interpret abstract syntax tree (AST) differences and extract functional intent, constructing a context-aware hybrid reward mechanism. This mechanism guides reinforcement learning agents to sequentially fulfill gameplay goals while adaptively exploring modified code branches. We evaluate SMART on two environments, Overcooked and Minecraft. The results demonstrate that SMART significantly outperforms state-of-the-art baselines; it achieves over 94% branch coverage of modified code, nearly double that of traditional reinforcement learning methods, while maintaining a 98% task completion rate, effectively balancing structural comprehensiveness with functional correctness.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T14:18:18Z</published>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Enhong Mu</name>
    </author>
    <author>
      <name>Minami Yoda</name>
    </author>
    <author>
      <name>Yan Zhang</name>
    </author>
    <author>
      <name>Mingyue Zhang</name>
    </author>
    <author>
      <name>Yutaka Matsuno</name>
    </author>
    <author>
      <name>Jialong Li</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12703v1</id>
    <title>Robust Motion Generation using Part-level Reliable Data from Videos</title>
    <updated>2025-12-14T14:15:16Z</updated>
    <link href="https://arxiv.org/abs/2512.12703v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12703v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Extracting human motion from large-scale web videos offers a scalable solution to the data scarcity issue in character animation. However, some human parts in many video frames cannot be seen due to off-screen captures or occlusions. It brings a dilemma: discarding the data missing any part limits scale and diversity, while retaining it compromises data quality and model performance.
  To address this problem, we propose leveraging credible part-level data extracted from videos to enhance motion generation via a robust part-aware masked autoregression model. First, we decompose a human body into five parts and detect the parts clearly seen in a video frame as "credible". Second, the credible parts are encoded into latent tokens by our proposed part-aware variational autoencoder. Third, we propose a robust part-level masked generation model to predict masked credible parts, while ignoring those noisy parts.
  In addition, we contribute K700-M, a challenging new benchmark comprising approximately 200k real-world motion sequences, for evaluation. Experimental results indicate that our method successfully outperforms baselines on both clean and noisy datasets in terms of motion quality, semantic consistency and diversity. Project page: https://boyuaner.github.io/ropar-main/</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T14:15:16Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Boyuan Li</name>
    </author>
    <author>
      <name>Sipeng Zheng</name>
    </author>
    <author>
      <name>Bin Cao</name>
    </author>
    <author>
      <name>Ruihua Song</name>
    </author>
    <author>
      <name>Zongqing Lu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12701v1</id>
    <title>Efficient Vision-Language Reasoning via Adaptive Token Pruning</title>
    <updated>2025-12-14T14:11:32Z</updated>
    <link href="https://arxiv.org/abs/2512.12701v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12701v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Real-world deployment of Vision-Language Models (VLMs) is hindered by high computational demands, as existing architectures inefficiently process all tokens uniformly. We introduce Adaptive Token Pruning (ATP), a dynamic inference mechanism that retains only the most informative tokens based on contextual relevance. ATP operates at the vision-language interface, assigning a hybrid importance score combining ViT CLS attention (intra-modal saliency) and CLIP text-image similarity (inter-modal relevance) to keep top-K tokens for the LLM. Unlike static compression, ATP adapts to each input without modifying the backbone. Proposed as a lightweight gating module, ATP is compatible with popular backbones like BLIP-2, LLaVA, and Flamingo. Preliminary evaluations across VQAv2, GQA, and COCO indicate that ATP reduces inference FLOPs by around 40% and achieves roughly 1.5x speedups in end-to-end latency with negligible accuracy loss (less than 1%). Qualitative analyses suggest ATP preserves visual grounding and enhances interpretability. Beyond efficiency, we investigate robustness under corruptions; observations suggest adaptive pruning suppresses spurious correlations, improving stability. These findings imply that resource-constrained inference and model reliability are not competing objectives. Finally, we discuss ATP's role in efficient multimodal edge computing pipelines.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T14:11:32Z</published>
    <arxiv:comment>10 pages, 3 figures. Expanded version of an extended abstract accepted at NeurIPS 2025 Workshop on VLM4RWD. Presents methodology and preliminary experimental results</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Xue Li</name>
    </author>
    <author>
      <name>Xiaonan Song</name>
    </author>
    <author>
      <name>Henry Hu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12699v1</id>
    <title>Attributes to Support the Formulation of Practically Relevant Research Problems in Software Engineering</title>
    <updated>2025-12-14T14:06:25Z</updated>
    <link href="https://arxiv.org/abs/2512.12699v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12699v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>[Background] A well-formulated research problem is essential for achieving practical relevance in Software Engineering (SE), yet there is a lack of structured guidance in this early phase. [Aims] Our goal is to introduce and evaluate seven attributes identified in the SE literature as relevant for formulating research problems (practical problem, context, implications/impacts, practitioners, evidence, objective, and research questions) in terms of their perceived importance and completeness, and learn how they can be applied. [Method] We conducted a workshop with 42 senior SE researchers during the ISERN 2024 meeting. The seven attributes were presented using a Problem Vision board filled with a research example. Participants discussed attributes in groups, shared written feedback, and individually completed a survey assessing their importance, completeness, and suggestions for improvement. [Results] The findings confirm the importance of the seven attributes in the formulation of industry-oriented research problems. Qualitative feedback illustrated how they can be applied in practice and revealed suggestions to refine them, such as incorporating financial criteria (e.g., ROI) into implications/impacts and addressing feasibility and constraints under evidence. [Conclusion] The results reaffirm the importance of the seven attributes in supporting a reflective and context-aware problem formulation. Adapting their use to specific research contexts can help to improve the alignment between academic research and industry needs.</summary>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T14:06:25Z</published>
    <arxiv:primary_category term="cs.SE"/>
    <author>
      <name>Anrafel Fernandes Pereira</name>
    </author>
    <author>
      <name>Maria Teresa Baldassarre</name>
    </author>
    <author>
      <name>Daniel Mendez</name>
    </author>
    <author>
      <name>Jürgen Börstler</name>
    </author>
    <author>
      <name>Nauman bin Ali</name>
    </author>
    <author>
      <name>Rahul Mohanani</name>
    </author>
    <author>
      <name>Darja Smite</name>
    </author>
    <author>
      <name>Stefan Biffl</name>
    </author>
    <author>
      <name>Rogardt Heldal</name>
    </author>
    <author>
      <name>Davide Falessi</name>
    </author>
    <author>
      <name>Daniel Graziotin</name>
    </author>
    <author>
      <name>Marcos Kalinowski</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12694v1</id>
    <title>Hybrid Retrieval-Augmented Generation for Robust Multilingual Document Question Answering</title>
    <updated>2025-12-14T13:57:05Z</updated>
    <link href="https://arxiv.org/abs/2512.12694v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12694v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Large-scale digitization initiatives have unlocked massive collections of historical newspapers, yet effective computational access remains hindered by OCR corruption, multilingual orthographic variation, and temporal language drift. We develop and evaluate a multilingual Retrieval-Augmented Generation pipeline specifically designed for question answering on noisy historical documents. Our approach integrates: (i) semantic query expansion and multi-query fusion using Reciprocal Rank Fusion to improve retrieval robustness against vocabulary mismatch; (ii) a carefully engineered generation prompt that enforces strict grounding in retrieved evidence and explicit abstention when evidence is insufficient; and (iii) a modular architecture enabling systematic component evaluation. We conduct comprehensive ablation studies on Named Entity Recognition and embedding model selection, demonstrating the importance of syntactic coherence in entity extraction and balanced performance-efficiency trade-offs in dense retrieval. Our end-to-end evaluation framework shows that the pipeline generates faithful answers for well-supported queries while correctly abstaining from unanswerable questions. The hybrid retrieval strategy improves recall stability, particularly benefiting from RRF's ability to smooth performance variance across query formulations. We release our code and configurations at https://anonymous.4open.science/r/RAGs-C5AE/, providing a reproducible foundation for robust historical document question answering.</summary>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T13:57:05Z</published>
    <arxiv:comment>Preprint</arxiv:comment>
    <arxiv:primary_category term="cs.DL"/>
    <author>
      <name>Anthony Mudet</name>
    </author>
    <author>
      <name>Souhail Bakkali</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12693v1</id>
    <title>Co-Exploration and Co-Exploitation via Shared Structure in Multi-Task Bandits</title>
    <updated>2025-12-14T13:56:58Z</updated>
    <link href="https://arxiv.org/abs/2512.12693v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12693v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We propose a novel Bayesian framework for efficient exploration in contextual multi-task multi-armed bandit settings, where the context is only observed partially and dependencies between reward distributions are induced by latent context variables. In order to exploit these structural dependencies, our approach integrates observations across all tasks and learns a global joint distribution, while still allowing personalised inference for new tasks. In this regard, we identify two key sources of epistemic uncertainty, namely structural uncertainty in the latent reward dependencies across arms and tasks, and user-specific uncertainty due to incomplete context and limited interaction history. To put our method into practice, we represent the joint distribution over tasks and rewards using a particle-based approximation of a log-density Gaussian process. This representation enables flexible, data-driven discovery of both inter-arm and inter-task dependencies without prior assumptions on the latent variables. Empirically, we demonstrate that our method outperforms baselines such as hierarchical model bandits, especially in settings with model misspecification or complex latent heterogeneity.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T13:56:58Z</published>
    <arxiv:comment>18 pages, 9 figures, preprint</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Sumantrak Mukherjee</name>
    </author>
    <author>
      <name>Serafima Lebedeva</name>
    </author>
    <author>
      <name>Valentin Margraf</name>
    </author>
    <author>
      <name>Jonas Hanselle</name>
    </author>
    <author>
      <name>Kanta Yamaoka</name>
    </author>
    <author>
      <name>Viktor Bengs</name>
    </author>
    <author>
      <name>Stefan Konigorski</name>
    </author>
    <author>
      <name>Eyke Hüllermeier</name>
    </author>
    <author>
      <name>Sebastian Josef Vollmer</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12692v1</id>
    <title>WebOperator: Action-Aware Tree Search for Autonomous Agents in Web Environment</title>
    <updated>2025-12-14T13:56:54Z</updated>
    <link href="https://arxiv.org/abs/2512.12692v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12692v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>LLM-based agents often operate in a greedy, step-by-step manner, selecting actions solely based on the current observation without considering long-term consequences or alternative paths. This lack of foresight is particularly problematic in web environments, which are only partially observable-limited to browser-visible content (e.g., DOM and UI elements)-where a single misstep often requires complex and brittle navigation to undo. Without an explicit backtracking mechanism, agents struggle to correct errors or systematically explore alternative paths. Tree-search methods provide a principled framework for such structured exploration, but existing approaches lack mechanisms for safe backtracking, making them prone to unintended side effects. They also assume that all actions are reversible, ignoring the presence of irreversible actions-limitations that reduce their effectiveness in realistic web tasks. To address these challenges, we introduce WebOperator, a tree-search framework that enables reliable backtracking and strategic exploration. Our method incorporates a best-first search strategy that ranks actions by both reward estimates and safety considerations, along with a robust backtracking mechanism that verifies the feasibility of previously visited paths before replaying them, preventing unintended side effects. To further guide exploration, WebOperator generates action candidates from multiple, varied reasoning contexts to ensure diverse and robust exploration, and subsequently curates a high-quality action set by filtering out invalid actions pre-execution and merging semantically equivalent ones. Experimental results on WebArena and WebVoyager demonstrate the effectiveness of WebOperator. On WebArena, WebOperator achieves a state-of-the-art 54.6% success rate with gpt-4o, underscoring the critical advantage of integrating strategic foresight with safe execution.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T13:56:54Z</published>
    <arxiv:comment>Under review at ICLR 2026. Project page: https://kagnlp.github.io/WebOperator/</arxiv:comment>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Mahir Labib Dihan</name>
    </author>
    <author>
      <name>Tanzima Hashem</name>
    </author>
    <author>
      <name>Mohammed Eunus Ali</name>
    </author>
    <author>
      <name>Md Rizwan Parvez</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12690v1</id>
    <title>Reassessing the Role of Supervised Fine-Tuning: An Empirical Study in VLM Reasoning</title>
    <updated>2025-12-14T13:46:42Z</updated>
    <link href="https://arxiv.org/abs/2512.12690v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12690v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Recent advances in vision-language models (VLMs) reasoning have been largely attributed to the rise of reinforcement Learning (RL), which has shifted the community's focus away from the supervised fine-tuning (SFT) paradigm. Many studies suggest that introducing the SFT stage not only fails to improve reasoning ability but may also negatively impact model training. In this study, we revisit this RL-centric belief through a systematic and controlled comparison of SFT and RL on VLM Reasoning. Using identical data sources, we find that the relative effectiveness of SFT and RL is conditional and strongly influenced by model capacity, data scale, and data distribution. Contrary to common assumptions, our findings show that SFT plays a crucial role across several scenarios: (1) Effectiveness for weaker models. SFT more reliably elicits reasoning capabilities in smaller or weaker VLMs. (2) Data efficiency. SFT with only 2K achieves comparable or better reasoning performance to RL with 20K. (3) Cross-modal transferability. SFT demonstrates stronger generalization across modalities. Moreover, we identify a pervasive issue of deceptive rewards, where higher rewards fail to correlate with better reasoning accuracy in RL. These results challenge the prevailing "RL over SFT" narrative. They highlight that the role of SFT may have been underestimated and support a more balanced post-training pipeline in which SFT and RL function as complementary components.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T13:46:42Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Yongcan Yu</name>
    </author>
    <author>
      <name>Lingxiao He</name>
    </author>
    <author>
      <name>Shuo Lu</name>
    </author>
    <author>
      <name>Lijun Sheng</name>
    </author>
    <author>
      <name>Yinuo Xu</name>
    </author>
    <author>
      <name>Yanbo Wang</name>
    </author>
    <author>
      <name>Kuangpu Guo</name>
    </author>
    <author>
      <name>Jianjie Cheng</name>
    </author>
    <author>
      <name>Meng Wang</name>
    </author>
    <author>
      <name>Qianlong Xie</name>
    </author>
    <author>
      <name>Xingxing Wang</name>
    </author>
    <author>
      <name>Dapeng Hu</name>
    </author>
    <author>
      <name>Jian Liang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12688v1</id>
    <title>Theoretical Foundations of Prompt Engineering: From Heuristics to Expressivity</title>
    <updated>2025-12-14T13:42:20Z</updated>
    <link href="https://arxiv.org/abs/2512.12688v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12688v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Prompts can switch a model's behavior even when the weights are fixed, yet this phenomenon is rarely treated as a clean theoretical object rather than a heuristic. We study the family of functions obtainable by holding a Transformer backbone fixed as an executor and varying only the prompt. Our core idea is to view the prompt as an externally injected program and to construct a simplified Transformer that interprets it to implement different computations. The construction exposes a mechanism-level decomposition: attention performs selective routing from prompt memory, the FFN performs local arithmetic conditioned on retrieved fragments, and depth-wise stacking composes these local updates into a multi-step computation. Under this viewpoint, we prove a constructive existential result showing that a single fixed backbone can approximate a broad class of target behaviors via prompts alone. The framework provides a unified starting point for formalizing trade-offs under prompt length/precision constraints and for studying structural limits of prompt-based switching, while remaining distinct from empirical claims about pretrained LLMs.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T13:42:20Z</published>
    <arxiv:comment>24 pages</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Dongseok Kim</name>
    </author>
    <author>
      <name>Hyoungsun Choi</name>
    </author>
    <author>
      <name>Mohamed Jismy Aashik Rasool</name>
    </author>
    <author>
      <name>Gisung Oh</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12686v1</id>
    <title>Memoria: A Scalable Agentic Memory Framework for Personalized Conversational AI</title>
    <updated>2025-12-14T13:38:06Z</updated>
    <link href="https://arxiv.org/abs/2512.12686v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12686v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Agentic memory is emerging as a key enabler for large language models (LLM) to maintain continuity, personalization, and long-term context in extended user interactions, critical capabilities for deploying LLMs as truly interactive and adaptive agents. Agentic memory refers to the memory that provides an LLM with agent-like persistence: the ability to retain and act upon information across conversations, similar to how a human would. We present Memoria, a modular memory framework that augments LLM-based conversational systems with persistent, interpretable, and context-rich memory. Memoria integrates two complementary components: dynamic session-level summarization and a weighted knowledge graph (KG)-based user modelling engine that incrementally captures user traits, preferences, and behavioral patterns as structured entities and relationships. This hybrid architecture enables both short-term dialogue coherence and long-term personalization while operating within the token constraints of modern LLMs. We demonstrate how Memoria enables scalable, personalized conversational artificial intelligence (AI) by bridging the gap between stateless LLM interfaces and agentic memory systems, offering a practical solution for industry applications requiring adaptive and evolving user experiences.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T13:38:06Z</published>
    <arxiv:comment>Paper accepted at 5th International Conference of AIML Systems 2025, Bangalore, India</arxiv:comment>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Samarth Sarin</name>
    </author>
    <author>
      <name>Lovepreet Singh</name>
    </author>
    <author>
      <name>Bhaskarjit Sarmah</name>
    </author>
    <author>
      <name>Dhagash Mehta</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12683v1</id>
    <title>Quantum Implicit Neural Representations for 3D Scene Reconstruction and Novel View Synthesis</title>
    <updated>2025-12-14T13:24:11Z</updated>
    <link href="https://arxiv.org/abs/2512.12683v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12683v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Implicit neural representations (INRs) have become a powerful paradigm for continuous signal modeling and 3D scene reconstruction, yet classical networks suffer from a well-known spectral bias that limits their ability to capture high-frequency details. Quantum Implicit Representation Networks (QIREN) mitigate this limitation by employing parameterized quantum circuits with inherent Fourier structures, enabling compact and expressive frequency modeling beyond classical MLPs. In this paper, we present Quantum Neural Radiance Fields (Q-NeRF), the first hybrid quantum-classical framework for neural radiance field rendering. Q-NeRF integrates QIREN modules into the Nerfacto backbone, preserving its efficient sampling, pose refinement, and volumetric rendering strategies while replacing selected density and radiance prediction components with quantum-enhanced counterparts. We systematically evaluate three hybrid configurations on standard multi-view indoor datasets, comparing them to classical baselines using PSNR, SSIM, and LPIPS metrics. Results show that hybrid quantum-classical models achieve competitive reconstruction quality under limited computational resources, with quantum modules particularly effective in representing fine-scale, view-dependent appearance. Although current implementations rely on quantum circuit simulators constrained to few-qubit regimes, the results highlight the potential of quantum encodings to alleviate spectral bias in implicit representations. Q-NeRF provides a foundational step toward scalable quantum-enabled 3D scene reconstruction and a baseline for future quantum neural rendering research.</summary>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T13:24:11Z</published>
    <arxiv:primary_category term="quant-ph"/>
    <author>
      <name>Yeray Cordero</name>
    </author>
    <author>
      <name>Paula García-Molina</name>
    </author>
    <author>
      <name>Fernando Vilariño</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12678v1</id>
    <title>$β$-CLIP: Text-Conditioned Contrastive Learning for Multi-Granular Vision-Language Alignment</title>
    <updated>2025-12-14T13:03:20Z</updated>
    <link href="https://arxiv.org/abs/2512.12678v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12678v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>CLIP achieves strong zero-shot image-text retrieval by aligning global vision and text representations, yet it falls behind on fine-grained tasks even when fine-tuned on long, detailed captions. In this work, we propose $β$-CLIP, a multi-granular text-conditioned contrastive learning framework designed to achieve hierarchical alignment between multiple textual granularities-from full captions to sentences and phrases-and their corresponding visual regions. For each level of granularity, $β$-CLIP utilizes cross-attention to dynamically pool image patches, producing contextualized visual embeddings. To address the semantic overlap inherent in this hierarchy, we introduce the $β$-Contextualized Contrastive Alignment Loss ($β$-CAL). This objective parameterizes the trade-off between strict query-specific matching and relaxed intra-image contextualization, supporting both soft Cross-Entropy and hard Binary Cross-Entropy formulations. Through extensive experiments, we demonstrate that $β$-CLIP significantly improves dense alignment: achieving 91.8% T2I 92.3% I2T at R@1 on Urban1K and 30.9% on FG-OVD (Hard), setting state-of-the-art among methods trained without hard negatives. $β$-CLIP establishes a robust, adaptive baseline for dense vision-language correspondence. The code and models are released at https://github.com/fzohra/B-CLIP.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T13:03:20Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Fatimah Zohra</name>
    </author>
    <author>
      <name>Chen Zhao</name>
    </author>
    <author>
      <name>Hani Itani</name>
    </author>
    <author>
      <name>Bernard Ghanem</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12677v1</id>
    <title>Fine-Tuning Causal LLMs for Text Classification: Embedding-Based vs. Instruction-Based Approaches</title>
    <updated>2025-12-14T13:02:06Z</updated>
    <link href="https://arxiv.org/abs/2512.12677v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12677v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We explore efficient strategies to fine-tune decoder-only Large Language Models (LLMs) for downstream text classification under resource constraints. Two approaches are investigated: (1) attaching a classification head to a pre-trained causal LLM and fine-tuning on the task (using the LLM's final token embedding as a sequence representation), and (2) instruction-tuning the LLM in a prompt-&gt;response format for classification. To enable single-GPU fine-tuning of models up to 8B parameters, we combine 4-bit model quantization with Low-Rank Adaptation (LoRA) for parameter-efficient training. Experiments on two datasets - a proprietary single-label dataset and the public WIPO-Alpha patent dataset (extreme multi-label classification) - show that the embedding-based method significantly outperforms the instruction-tuned method in F1-score, and is very competitive with - even surpassing - fine-tuned domain-specific models (e.g. BERT) on the same tasks. These results demonstrate that directly leveraging the internal representations of causal LLMs, along with efficient fine-tuning techniques, yields impressive classification performance under limited computational resources. We discuss the advantages of each approach while outlining practical guidelines and future directions for optimizing LLM fine-tuning in classification scenarios.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T13:02:06Z</published>
    <arxiv:comment>18 pages, 6 figures</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Amirhossein Yousefiramandi</name>
    </author>
    <author>
      <name>Ciaran Cooney</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12676v1</id>
    <title>Robust Variational Bayes by Min-Max Median Aggregation</title>
    <updated>2025-12-14T13:02:00Z</updated>
    <link href="https://arxiv.org/abs/2512.12676v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12676v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We propose a robust and scalable variational Bayes (VB) framework designed to effectively handle contamination and outliers in dataset. Our approach partitions the data into $m$ disjoint subsets and formulates a joint optimization problem based on robust aggregation principles. A key insight is that the full posterior distribution is equivalent to the minimizer of the mean Kullback-Leibler (KL) divergence from the $m$-powered local posterior distributions. To enhance robustness, we replace the mean KL divergence with a min-max median formulation. The min-max formulation not only ensures consistency between the KL minimizer and the Evidence Lower Bound (ELBO) maximizer but also facilitates the establishment of improved statistical rates for the mean of variational posterior. We observe a notable discrepancy in the $m$-powered marginal log likelihood function contingent on the presence of local latent variables. To address this, we treat these two scenarios separately to guarantee the consistency of the aggregated variational posterior. Specifically, when local latent variables are present, we introduce an aggregate-and-rescale strategy. Theoretically, we provide a non-asymptotic analysis of our proposed posterior, incorporating a refined analysis of Bernstein-von Mises (BvM) theorem to accommodate a diverging number of subsets $m$. Our findings indicate that the two-stage approach yields a smaller approximation error compared to directly aggregating the $m$-powered local posteriors. Furthermore, we establish a nearly optimal statistical rate for the mean of the proposed posterior, advancing existing theories related to min-max median estimators. The efficacy of our method is demonstrated through extensive simulation studies.</summary>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T13:02:00Z</published>
    <arxiv:comment>34 pages, 11 figures</arxiv:comment>
    <arxiv:primary_category term="stat.ME"/>
    <author>
      <name>Jiawei Yan</name>
    </author>
    <author>
      <name>Ju Liu</name>
    </author>
    <author>
      <name>Weidong Liu</name>
    </author>
    <author>
      <name>Jiyuan Tu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12675v1</id>
    <title>Scone: Bridging Composition and Distinction in Subject-Driven Image Generation via Unified Understanding-Generation Modeling</title>
    <updated>2025-12-14T12:58:19Z</updated>
    <link href="https://arxiv.org/abs/2512.12675v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12675v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Subject-driven image generation has advanced from single- to multi-subject composition, while neglecting distinction, the ability to identify and generate the correct subject when inputs contain multiple candidates. This limitation restricts effectiveness in complex, realistic visual settings. We propose Scone, a unified understanding-generation method that integrates composition and distinction. Scone enables the understanding expert to act as a semantic bridge, conveying semantic information and guiding the generation expert to preserve subject identity while minimizing interference. A two-stage training scheme first learns composition, then enhances distinction through semantic alignment and attention-based masking. We also introduce SconeEval, a benchmark for evaluating both composition and distinction across diverse scenarios. Experiments demonstrate that Scone outperforms existing open-source models in composition and distinction tasks on two benchmarks. Our model, benchmark, and training data are available at: https://github.com/Ryann-Ran/Scone.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T12:58:19Z</published>
    <arxiv:comment>Code: https://github.com/Ryann-Ran/Scone</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Yuran Wang</name>
    </author>
    <author>
      <name>Bohan Zeng</name>
    </author>
    <author>
      <name>Chengzhuo Tong</name>
    </author>
    <author>
      <name>Wenxuan Liu</name>
    </author>
    <author>
      <name>Yang Shi</name>
    </author>
    <author>
      <name>Xiaochen Ma</name>
    </author>
    <author>
      <name>Hao Liang</name>
    </author>
    <author>
      <name>Yuanxing Zhang</name>
    </author>
    <author>
      <name>Wentao Zhang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12673v1</id>
    <title>Progressive Conditioned Scale-Shift Recalibration of Self-Attention for Online Test-time Adaptation</title>
    <updated>2025-12-14T12:56:02Z</updated>
    <link href="https://arxiv.org/abs/2512.12673v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12673v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Online test-time adaptation aims to dynamically adjust a network model in real-time based on sequential input samples during the inference stage. In this work, we find that, when applying a transformer network model to a new target domain, the Query, Key, and Value features of its self-attention module often change significantly from those in the source domain, leading to substantial performance degradation of the transformer model. To address this important issue, we propose to develop a new approach to progressively recalibrate the self-attention at each layer using a local linear transform parameterized by conditioned scale and shift factors. We consider the online model adaptation from the source domain to the target domain as a progressive domain shift separation process. At each transformer network layer, we learn a Domain Separation Network to extract the domain shift feature, which is used to predict the scale and shift parameters for self-attention recalibration using a Factor Generator Network. These two lightweight networks are adapted online during inference. Experimental results on benchmark datasets demonstrate that the proposed progressive conditioned scale-shift recalibration (PCSR) method is able to significantly improve the online test-time domain adaptation performance by a large margin of up to 3.9\% in classification accuracy on the ImageNet-C dataset.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T12:56:02Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Yushun Tang</name>
    </author>
    <author>
      <name>Ziqiong Liu</name>
    </author>
    <author>
      <name>Jiyuan Jia</name>
    </author>
    <author>
      <name>Yi Zhang</name>
    </author>
    <author>
      <name>Zhihai He</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12671v1</id>
    <title>On Approaches to Building Surrogate ODE Models for Diffusion Bridges</title>
    <updated>2025-12-14T12:49:38Z</updated>
    <link href="https://arxiv.org/abs/2512.12671v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12671v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Diffusion and Schrödinger Bridge models have established state-of-the-art performance in generative modeling but are often hampered by significant computational costs and complex training procedures. While continuous-time bridges promise faster sampling, overparameterized neural networks describe their optimal dynamics, and the underlying stochastic differential equations can be difficult to integrate efficiently. This work introduces a novel paradigm that uses surrogate models to create simpler, faster, and more flexible approximations of these dynamics. We propose two specific algorithms: SINDy Flow Matching (SINDy-FM), which leverages sparse regression to identify interpretable, symbolic differential equations from data, and a Neural-ODE reformulation of the Schrödinger Bridge (DSBM-NeuralODE) for flexible continuous-time parameterization. Our experiments on Gaussian transport tasks and MNIST latent translation demonstrate that these surrogates achieve competitive performance while offering dramatic improvements in efficiency and interpretability. The symbolic SINDy-FM models, in particular, reduce parameter counts by several orders of magnitude and enable near-instantaneous inference, paving the way for a new class of tractable and high-performing bridge models for practical deployment.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T12:49:38Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Maria Khilchuk</name>
    </author>
    <author>
      <name>Vladimir Latypov</name>
    </author>
    <author>
      <name>Pavel Kleshchev</name>
    </author>
    <author>
      <name>Alexander Hvatov</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12669v1</id>
    <title>DynaGen: Unifying Temporal Knowledge Graph Reasoning with Dynamic Subgraphs and Generative Regularization</title>
    <updated>2025-12-14T12:46:07Z</updated>
    <link href="https://arxiv.org/abs/2512.12669v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12669v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Temporal Knowledge Graph Reasoning (TKGR) aims to complete missing factual elements along the timeline. Depending on the temporal position of the query, the task is categorized into interpolation and extrapolation. Existing interpolation methods typically embed temporal information into individual facts to complete missing historical knowledge, while extrapolation techniques often leverage sequence models over graph snapshots to identify recurring patterns for future event prediction. These methods face two critical challenges: limited contextual modeling in interpolation and cognitive generalization bias in extrapolation. To address these, we propose a unified method for TKGR, dubbed DynaGen. For interpolation, DynaGen dynamically constructs entity-centric subgraphs and processes them with a synergistic dual-branch GNN encoder to capture evolving structural context. For extrapolation, it applies a conditional diffusion process, which forces the model to learn underlying evolutionary principles rather than just superficial patterns, enhancing its ability to predict unseen future events. Extensive experiments on six benchmark datasets show DynaGen achieves state-of-the-art performance. On average, compared to the second-best models, DynaGen improves the Mean Reciprocal Rank (MRR) score by 2.61 points for interpolation and 1.45 points for extrapolation.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T12:46:07Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Jiawei Shen</name>
    </author>
    <author>
      <name>Jia Zhu</name>
    </author>
    <author>
      <name>Hanghui Guo</name>
    </author>
    <author>
      <name>Weijie Shi</name>
    </author>
    <author>
      <name>Guoqing Ma</name>
    </author>
    <author>
      <name>Yidan Liang</name>
    </author>
    <author>
      <name>Jingjiang Liu</name>
    </author>
    <author>
      <name>Hao Chen</name>
    </author>
    <author>
      <name>Shimin Di</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12667v1</id>
    <title>Open-World Deepfake Attribution via Confidence-Aware Asymmetric Learning</title>
    <updated>2025-12-14T12:31:28Z</updated>
    <link href="https://arxiv.org/abs/2512.12667v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12667v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The proliferation of synthetic facial imagery has intensified the need for robust Open-World DeepFake Attribution (OW-DFA), which aims to attribute both known and unknown forgeries using labeled data for known types and unlabeled data containing a mixture of known and novel types. However, existing OW-DFA methods face two critical limitations: 1) A confidence skew that leads to unreliable pseudo-labels for novel forgeries, resulting in biased training. 2) An unrealistic assumption that the number of unknown forgery types is known *a priori*. To address these challenges, we propose a Confidence-Aware Asymmetric Learning (CAL) framework, which adaptively balances model confidence across known and novel forgery types. CAL mainly consists of two components: Confidence-Aware Consistency Regularization (CCR) and Asymmetric Confidence Reinforcement (ACR). CCR mitigates pseudo-label bias by dynamically scaling sample losses based on normalized confidence, gradually shifting the training focus from high- to low-confidence samples. ACR complements this by separately calibrating confidence for known and novel classes through selective learning on high-confidence samples, guided by their confidence gap. Together, CCR and ACR form a mutually reinforcing loop that significantly improves the model's OW-DFA performance. Moreover, we introduce a Dynamic Prototype Pruning (DPP) strategy that automatically estimates the number of novel forgery types in a coarse-to-fine manner, removing the need for unrealistic prior assumptions and enhancing the scalability of our methods to real-world OW-DFA scenarios. Extensive experiments on the standard OW-DFA benchmark and a newly extended benchmark incorporating advanced manipulations demonstrate that CAL consistently outperforms previous methods, achieving new state-of-the-art performance on both known and novel forgery attribution.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T12:31:28Z</published>
    <arxiv:comment>Accepted by AAAI2026</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Haiyang Zheng</name>
    </author>
    <author>
      <name>Nan Pu</name>
    </author>
    <author>
      <name>Wenjing Li</name>
    </author>
    <author>
      <name>Teng Long</name>
    </author>
    <author>
      <name>Nicu Sebe</name>
    </author>
    <author>
      <name>Zhun Zhong</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12664v1</id>
    <title>InteracTalker: Prompt-Based Human-Object Interaction with Co-Speech Gesture Generation</title>
    <updated>2025-12-14T12:29:49Z</updated>
    <link href="https://arxiv.org/abs/2512.12664v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12664v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Generating realistic human motions that naturally respond to both spoken language and physical objects is crucial for interactive digital experiences. Current methods, however, address speech-driven gestures or object interactions independently, limiting real-world applicability due to a lack of integrated, comprehensive datasets. To overcome this, we introduce InteracTalker, a novel framework that seamlessly integrates prompt-based object-aware interactions with co-speech gesture generation. We achieve this by employing a multi-stage training process to learn a unified motion, speech, and prompt embedding space. To support this, we curate a rich human-object interaction dataset, formed by augmenting an existing text-to-motion dataset with detailed object interaction annotations. Our framework utilizes a Generalized Motion Adaptation Module that enables independent training, adapting to the corresponding motion condition, which is then dynamically combined during inference. To address the imbalance between heterogeneous conditioning signals, we propose an adaptive fusion strategy, which dynamically reweights the conditioning signals during diffusion sampling. InteracTalker successfully unifies these previously separate tasks, outperforming prior methods in both co-speech gesture generation and object-interaction synthesis, outperforming gesture-focused diffusion methods, yielding highly realistic, object-aware full-body motions with enhanced realism, flexibility, and control.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T12:29:49Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Sreehari Rajan</name>
    </author>
    <author>
      <name>Kunal Bhosikar</name>
    </author>
    <author>
      <name>Charu Sharma</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12663v1</id>
    <title>PerNodeDrop: A Method Balancing Specialized Subnets and Regularization in Deep Neural Networks</title>
    <updated>2025-12-14T12:26:56Z</updated>
    <link href="https://arxiv.org/abs/2512.12663v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12663v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Deep neural networks possess strong representational capacity yet remain vulnerable to overfitting, primarily because neurons tend to co-adapt in ways that, while capturing complex and fine-grained feature interactions, also reinforce spurious and non-generalizable patterns that inflate training performance but reduce reliability on unseen data. Noise-based regularizers such as Dropout and DropConnect address this issue by injecting stochastic perturbations during training, but the noise they apply is typically uniform across a layer or across a batch of samples, which can suppress both harmful and beneficial co-adaptation.
  This work introduces PerNodeDrop, a lightweight stochastic regularization method. It applies per-sample, per-node perturbations to break the uniformity of the noise injected by existing techniques, thereby allowing each node to experience input-specific variability. Hence, PerNodeDrop preserves useful co-adaptation while applying regularization. This narrows the gap between training and validation performance and improves reliability on unseen data, as evident from the experiments.
  Although superficially similar to DropConnect, PerNodeDrop operates at the sample level. It drops weights at the sample level, not the batch level. An expected-loss analysis formalizes how its perturbations attenuate excessive co-adaptation while retaining predictive interactions. Empirical evaluations on vision, text, and audio benchmarks indicate improved generalization relative to the standard noise-based regularizer.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T12:26:56Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Gelesh G Omathil</name>
    </author>
    <author>
      <name>Sreeja CS</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12662v1</id>
    <title>Anatomy-Guided Representation Learning Using a Transformer-Based Network for Thyroid Nodule Segmentation in Ultrasound Images</title>
    <updated>2025-12-14T12:20:20Z</updated>
    <link href="https://arxiv.org/abs/2512.12662v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12662v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Accurate thyroid nodule segmentation in ultrasound images is critical for diagnosis and treatment planning. However, ambiguous boundaries between nodules and surrounding tissues, size variations, and the scarcity of annotated ultrasound data pose significant challenges for automated segmentation. Existing deep learning models struggle to incorporate contextual information from the thyroid gland and generalize effectively across diverse cases. To address these challenges, we propose SSMT-Net, a Semi-Supervised Multi-Task Transformer-based Network that leverages unlabeled data to enhance Transformer-centric encoder feature extraction capability in an initial unsupervised phase. In the supervised phase, the model jointly optimizes nodule segmentation, gland segmentation, and nodule size estimation, integrating both local and global contextual features. Extensive evaluations on the TN3K and DDTI datasets demonstrate that SSMT-Net outperforms state-of-the-art methods, with higher accuracy and robustness, indicating its potential for real-world clinical applications.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T12:20:20Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Muhammad Umar Farooq</name>
    </author>
    <author>
      <name>Abd Ur Rehman</name>
    </author>
    <author>
      <name>Azka Rehman</name>
    </author>
    <author>
      <name>Muhammad Usman</name>
    </author>
    <author>
      <name>Dong-Kyu Chae</name>
    </author>
    <author>
      <name>Junaid Qadir</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12658v1</id>
    <title>CogDoc: Towards Unified thinking in Documents</title>
    <updated>2025-12-14T12:14:17Z</updated>
    <link href="https://arxiv.org/abs/2512.12658v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12658v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Current document reasoning paradigms are constrained by a fundamental trade-off between scalability (processing long-context documents) and fidelity (capturing fine-grained, multimodal details). To bridge this gap, we propose CogDoc, a unified coarse-to-fine thinking framework that mimics human cognitive processes: a low-resolution "Fast Reading" phase for scalable information localization,followed by a high-resolution "Focused Thinking" phase for deep reasoning. We conduct a rigorous investigation into post-training strategies for the unified thinking framework, demonstrating that a Direct Reinforcement Learning (RL) approach outperforms RL with Supervised Fine-Tuning (SFT) initialization. Specifically, we find that direct RL avoids the "policy conflict" observed in SFT. Empirically, our 7B model achieves state-of-the-art performance within its parameter class, notably surpassing significantly larger proprietary models (e.g., GPT-4o) on challenging, visually rich document benchmarks.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T12:14:17Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Qixin Xu</name>
    </author>
    <author>
      <name>Haozhe Wang</name>
    </author>
    <author>
      <name>Che Liu</name>
    </author>
    <author>
      <name>Fangzhen Lin</name>
    </author>
    <author>
      <name>Wenhu Chen</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12657v1</id>
    <title>Cross-modal Fundus Image Registration under Large FoV Disparity</title>
    <updated>2025-12-14T12:10:37Z</updated>
    <link href="https://arxiv.org/abs/2512.12657v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12657v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Previous work on cross-modal fundus image registration (CMFIR) assumes small cross-modal Field-of-View (FoV) disparity. By contrast, this paper is targeted at a more challenging scenario with large FoV disparity, to which directly applying current methods fails. We propose Crop and Alignment for cross-modal fundus image Registration(CARe), a very simple yet effective method. Specifically, given an OCTA with smaller FoV as a source image and a wide-field color fundus photograph (wfCFP) as a target image, our Crop operation exploits the physiological structure of the retina to crop from the target image a sub-image with its FoV roughly aligned with that of the source. This operation allows us to re-purpose the previous small-FoV-disparity oriented methods for subsequent image registration. Moreover, we improve spatial transformation by a double-fitting based Alignment module that utilizes the classical RANSAC algorithm and polynomial-based coordinate fitting in a sequential manner. Extensive experiments on a newly developed test set of 60 OCTA-wfCFP pairs verify the viability of CARe for CMFIR.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T12:10:37Z</published>
    <arxiv:comment>Accepted as a regular paper at MMM 2026</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Hongyang Li</name>
    </author>
    <author>
      <name>Junyi Tao</name>
    </author>
    <author>
      <name>Qijie Wei</name>
    </author>
    <author>
      <name>Ningzhi Yang</name>
    </author>
    <author>
      <name>Meng Wang</name>
    </author>
    <author>
      <name>Weihong Yu</name>
    </author>
    <author>
      <name>Xirong Li</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12654v1</id>
    <title>Modeling Authorial Style in Urdu Novels Using Character Interaction Graphs and Graph Neural Networks</title>
    <updated>2025-12-14T11:59:16Z</updated>
    <link href="https://arxiv.org/abs/2512.12654v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12654v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Authorship analysis has traditionally focused on lexical and stylistic cues within text, while higher-level narrative structure remains underexplored, particularly for low-resource languages such as Urdu. This work proposes a graph-based framework that models Urdu novels as character interaction networks to examine whether authorial style can be inferred from narrative structure alone. Each novel is represented as a graph where nodes correspond to characters and edges denote their co-occurrence within narrative proximity. We systematically compare multiple graph representations, including global structural features, node-level semantic summaries, unsupervised graph embeddings, and supervised graph neural networks. Experiments on a dataset of 52 Urdu novels written by seven authors show that learned graph representations substantially outperform hand-crafted and unsupervised baselines, achieving up to 0.857 accuracy under a strict author-aware evaluation protocol.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T11:59:16Z</published>
    <arxiv:comment>6 pages</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Hassan Mujtaba</name>
    </author>
    <author>
      <name>Hamza Naveed</name>
    </author>
    <author>
      <name>Hanzlah Munir</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12652v1</id>
    <title>Value-Aware Multiagent Systems</title>
    <updated>2025-12-14T11:53:36Z</updated>
    <link href="https://arxiv.org/abs/2512.12652v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12652v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>This paper introduces the concept of value awareness in AI, which goes beyond the traditional value-alignment problem. Our definition of value awareness presents us with a concise and simplified roadmap for engineering value-aware AI. The roadmap is structured around three core pillars: (1) learning and representing human values using formal semantics, (2) ensuring the value alignment of both individual agents and multiagent systems, and (3) providing value-based explainability on behaviour. The paper presents a selection of our ongoing work on some of these topics, along with applications to real-life domains.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T11:53:36Z</published>
    <arxiv:primary_category term="cs.AI"/>
    <arxiv:journal_ref>In Coordination, Organizations, Institutions, Norms, and Ethics for Governance of Multi-Agent Systems XVII. COINE 2024. LNCS, vol 15398. Springer, Cham (2025)</arxiv:journal_ref>
    <author>
      <name>Nardine Osman</name>
    </author>
    <arxiv:doi>10.1007/978-3-031-82039-7_3</arxiv:doi>
    <link rel="related" href="https://doi.org/10.1007/978-3-031-82039-7_3" title="doi"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12650v1</id>
    <title>A Systematic Analysis of Higher Education on Software Engineering in the Netherlands</title>
    <updated>2025-12-14T11:37:16Z</updated>
    <link href="https://arxiv.org/abs/2512.12650v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12650v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Software engineering educators strive to continuously improve their courses and programs. Understanding the current state of practice of software engineering higher education can empower educators to critically assess their courses, fine-tune them by benchmarking against observed practices, and ultimately enhance their curricula. In this study, we aim to provide an encompassing analysis of higher education on software engineering by considering the higher educational offering of an entire European country, namely the Netherlands. We leverage a crowd-sourced analysis process by considering 10 Dutch universities and 207 university courses. The courses are analysed via knowledge areas adopted from the SWEBOK. The mapping process is refined via homogenisation and internal consistency improvement phases, and is followed by a data analysis phase. Given its fundamental nature, Construction and Programming is the most covered knowledge area at Bachelor level. Other knowledge areas are equally covered at Bachelor and Master level (e.g., software engineering models), while more advanced ones are almost exclusively covered at Master level. We identify three clusters of tightly coupled knowledge areas: (i) requirements, architecture, and design, (ii) testing, verification, and security, and (iii) process-oriented and DevOps topics. Dutch universities generally cover all knowledge areas uniformly, with minor deviations reflecting institutional research strengths. Our results highlight correlations among key knowledge areas and their potential for enhancing integrated learning. We also identify underrepresented areas, such as software engineering economics, which educators may consider including in curricula. We invite researchers to use our research method in their own geographical region, in order to contrast software engineering education programs across the globe.</summary>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T11:37:16Z</published>
    <arxiv:primary_category term="cs.SE"/>
    <author>
      <name>Bastiaan Heeren</name>
    </author>
    <author>
      <name>Fabiano Dalpiaz</name>
    </author>
    <author>
      <name>Mazyar Seraj</name>
    </author>
    <author>
      <name>Roberto Verdecchia</name>
    </author>
    <author>
      <name>Vadim Zaytsev</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12643v1</id>
    <title>LexRel: Benchmarking Legal Relation Extraction for Chinese Civil Cases</title>
    <updated>2025-12-14T11:16:39Z</updated>
    <link href="https://arxiv.org/abs/2512.12643v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12643v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Legal relations form a highly consequential analytical framework of civil law system, serving as a crucial foundation for resolving disputes and realizing values of the rule of law in judicial practice. However, legal relations in Chinese civil cases remain underexplored in the field of legal artificial intelligence (legal AI), largely due to the absence of comprehensive schemas. In this work, we firstly introduce a comprehensive schema, which contains a hierarchical taxonomy and definitions of arguments, for AI systems to capture legal relations in Chinese civil cases. Based on this schema, we then formulate legal relation extraction task and present LexRel, an expert-annotated benchmark for legal relation extraction in Chinese civil law. We use LexRel to evaluate state-of-the-art large language models (LLMs) on legal relation extractions, showing that current LLMs exhibit significant limitations in accurately identifying civil legal relations. Furthermore, we demonstrate that incorporating legal relations information leads to consistent performance gains on other downstream legal AI tasks.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T11:16:39Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Yida Cai</name>
    </author>
    <author>
      <name>Ranjuexiao Hu</name>
    </author>
    <author>
      <name>Huiyuan Xie</name>
    </author>
    <author>
      <name>Chenyang Li</name>
    </author>
    <author>
      <name>Yun Liu</name>
    </author>
    <author>
      <name>Yuxiao Ye</name>
    </author>
    <author>
      <name>Zhenghao Liu</name>
    </author>
    <author>
      <name>Weixing Shen</name>
    </author>
    <author>
      <name>Zhiyuan Liu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12642v1</id>
    <title>Torch Geometric Pool: the Pytorch library for pooling in Graph Neural Networks</title>
    <updated>2025-12-14T11:15:09Z</updated>
    <link href="https://arxiv.org/abs/2512.12642v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12642v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We introduce Torch Geometric Pool (tgp), a library for hierarchical pooling in Graph Neural Networks. Built upon Pytorch Geometric, Torch Geometric Pool (tgp) provides a wide variety of pooling operators, unified under a consistent API and a modular design. The library emphasizes usability and extensibility, and includes features like precomputed pooling, which significantly accelerate training for a class of operators. In this paper, we present tgp's structure and present an extensive benchmark. The latter showcases the library's features and systematically compares the performance of the implemented graph-pooling methods in different downstream tasks. The results, showing that the choice of the optimal pooling operator depends on tasks and data at hand, support the need for a library that enables fast prototyping.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T11:15:09Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Filippo Maria Bianchi</name>
    </author>
    <author>
      <name>Carlo Abate</name>
    </author>
    <author>
      <name>Ivan Marisca</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12641v1</id>
    <title>Which Pieces Does Unigram Tokenization Really Need?</title>
    <updated>2025-12-14T11:13:49Z</updated>
    <link href="https://arxiv.org/abs/2512.12641v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12641v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The Unigram tokenization algorithm offers a probabilistic alternative to the greedy heuristics of Byte-Pair Encoding. Despite its theoretical elegance, its implementation in practice is complex, limiting its adoption to the SentencePiece package and adapters thereof. We bridge this gap between theory and practice by providing a clear guide to implementation and parameter choices. We also identify a simpler algorithm that accepts slightly higher training loss in exchange for improved compression.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T11:13:49Z</published>
    <arxiv:comment>10 pages, 1 figure. For associated code, see https://github.com/sanderland/script_tok</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Sander Land</name>
    </author>
    <author>
      <name>Yuval Pinter</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12634v1</id>
    <title>Modular and Multi-Path-Aware Offline Benchmarking for Mobile GUI Agents</title>
    <updated>2025-12-14T10:41:39Z</updated>
    <link href="https://arxiv.org/abs/2512.12634v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12634v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Mobile GUI Agents, AI agents capable of interacting with mobile applications on behalf of users, have the potential to transform human computer interaction. However, current evaluation practices for GUI agents face two fundamental limitations. First, they either rely on single path offline benchmarks or online live benchmarks. Offline benchmarks using static, single path annotated datasets unfairly penalize valid alternative actions, while online benchmarks suffer from poor scalability and reproducibility due to the dynamic and unpredictable nature of live evaluation. Second, existing benchmarks treat agents as monolithic black boxes, overlooking the contributions of individual components, which often leads to unfair comparisons or obscures key performance bottlenecks. To address these limitations, we present MobiBench, the first modular and multi path aware offline benchmarking framework for mobile GUI agents that enables high fidelity, scalable, and reproducible evaluation entirely in offline settings. Our experiments demonstrate that MobiBench achieves 94.72 percent agreement with human evaluators, on par with carefully engineered online benchmarks, while preserving the scalability and reproducibility of static offline benchmarks. Furthermore, our comprehensive module level analysis uncovers several key insights, including a systematic evaluation of diverse techniques used in mobile GUI agents, optimal module configurations across model scales, the inherent limitations of current LFMs, and actionable guidelines for designing more capable and cost efficient mobile agents.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T10:41:39Z</published>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Youngmin Im</name>
    </author>
    <author>
      <name>Byeongung Jo</name>
    </author>
    <author>
      <name>Jaeyoung Wi</name>
    </author>
    <author>
      <name>Seungwoo Baek</name>
    </author>
    <author>
      <name>Tae Hoon Min</name>
    </author>
    <author>
      <name>Joo Hyung Lee</name>
    </author>
    <author>
      <name>Sangeun Oh</name>
    </author>
    <author>
      <name>Insik Shin</name>
    </author>
    <author>
      <name>Sunjae Lee</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12633v1</id>
    <title>DiG: Differential Grounding for Enhancing Fine-Grained Perception in Multimodal Large Language Model</title>
    <updated>2025-12-14T10:40:27Z</updated>
    <link href="https://arxiv.org/abs/2512.12633v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12633v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Multimodal Large Language Models have achieved impressive performance on a variety of vision-language tasks, yet their fine-grained visual perception and precise spatial reasoning remain limited. In this work, we introduce DiG (Differential Grounding), a novel proxy task framework where MLLMs learn fine-grained perception by identifying and localizing all differences between similar image pairs without prior knowledge of their number. To support scalable training, we develop an automated 3D rendering-based data generation pipeline that produces high-quality paired images with fully controllable discrepancies. To address the sparsity of difference signals, we further employ curriculum learning that progressively increases complexity from single to multiple differences, enabling stable optimization. Extensive experiments demonstrate that DiG significantly improves model performance across a variety of visual perception benchmarks and that the learned fine-grained perception skills transfer effectively to standard downstream tasks, including RefCOCO, RefCOCO+, RefCOCOg, and general multimodal perception benchmarks. Our results highlight differential grounding as a scalable and robust approach for advancing fine-grained visual reasoning in MLLMs.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T10:40:27Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Zhou Tao</name>
    </author>
    <author>
      <name>Shida Wang</name>
    </author>
    <author>
      <name>Yongxiang Hua</name>
    </author>
    <author>
      <name>Haoyu Cao</name>
    </author>
    <author>
      <name>Linli Xu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12630v1</id>
    <title>ORIBA: Exploring LLM-Driven Role-Play Chatbot as a Creativity Support Tool for Original Character Artists</title>
    <updated>2025-12-14T10:29:35Z</updated>
    <link href="https://arxiv.org/abs/2512.12630v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12630v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Recent advances in Generative AI (GAI) have led to new opportunities for creativity support. However, this technology has raised ethical concerns in the visual artists community. This paper explores how GAI can assist visual artists in developing original characters (OCs) while respecting their creative agency. We present ORIBA, an AI chatbot leveraging large language models (LLMs) to enable artists to role-play with their OCs, focusing on conceptualization (e.g., backstories) while leaving exposition (visual creation) to creators. Through a study with 14 artists, we found ORIBA motivated artists' imaginative engagement, developing multidimensional attributes and stronger bonds with OCs that inspire their creative process. Our contributions include design insights for AI systems that develop from artists' perspectives, demonstrating how LLMs can support cross-modal creativity while preserving creative agency in OC art. This paper highlights the potential of GAI as a neutral, non-visual support that strengthens existing creative practice, without infringing artistic exposition.</summary>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T10:29:35Z</published>
    <arxiv:primary_category term="cs.HC"/>
    <author>
      <name>Yuqian Sun</name>
    </author>
    <author>
      <name>Xingyu Li</name>
    </author>
    <author>
      <name>Shunyu Yao</name>
    </author>
    <author>
      <name>Noura Howell</name>
    </author>
    <author>
      <name>Tristan Braud</name>
    </author>
    <author>
      <name>Chang Hee Lee</name>
    </author>
    <author>
      <name>Ali Asadipour</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12624v1</id>
    <title>CoLSE: A Lightweight and Robust Hybrid Learned Model for Single-Table Cardinality Estimation using Joint CDF</title>
    <updated>2025-12-14T10:08:20Z</updated>
    <link href="https://arxiv.org/abs/2512.12624v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12624v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Cardinality estimation (CE), the task of predicting the result size of queries is a critical component of query optimization. Accurate estimates are essential for generating efficient query execution plans. Recently, machine learning techniques have been applied to CE, broadly categorized into query-driven and data-driven approaches. Data-driven methods learn the joint distribution of data, while query-driven methods construct regression models that map query features to cardinalities. Ideally, a CE technique should strike a balance among three key factors: accuracy, efficiency, and memory footprint. However, existing state-of-the-art models often fail to achieve this balance.
  To address this, we propose CoLSE, a hybrid learned approach for single-table cardinality estimation. CoLSE directly models the joint probability over queried intervals using a novel algorithm based on copula theory and integrates a lightweight neural network to correct residual estimation errors. Experimental results show that CoLSE achieves a favorable trade-off among accuracy, training time, inference latency, and model size, outperforming existing state-of-the-art methods.</summary>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T10:08:20Z</published>
    <arxiv:primary_category term="cs.DB"/>
    <author>
      <name>Lankadinee Rathuwadu</name>
    </author>
    <author>
      <name>Guanli Liu</name>
    </author>
    <author>
      <name>Christopher Leckie</name>
    </author>
    <author>
      <name>Renata Borovica-Gajic</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12623v1</id>
    <title>Reasoning Within the Mind: Dynamic Multimodal Interleaving in Latent Space</title>
    <updated>2025-12-14T10:07:45Z</updated>
    <link href="https://arxiv.org/abs/2512.12623v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12623v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced cross-modal understanding and reasoning by incorporating Chain-of-Thought (CoT) reasoning in the semantic space. Building upon this, recent studies extend the CoT mechanism to the visual modality, enabling models to integrate visual information during reasoning through external tools or explicit image generation. However, these methods remain dependent on explicit step-by-step reasoning, unstable perception-reasoning interaction and notable computational overhead. Inspired by human cognition, we posit that thinking unfolds not linearly but through the dynamic interleaving of reasoning and perception within the mind. Motivated by this perspective, we propose DMLR, a test-time Dynamic Multimodal Latent Reasoning framework that employs confidence-guided latent policy gradient optimization to refine latent think tokens for in-depth reasoning. Furthermore, a Dynamic Visual Injection Strategy is introduced, which retrieves the most relevant visual features at each latent think token and updates the set of best visual patches. The updated patches are then injected into latent think token to achieve dynamic visual-textual interleaving. Experiments across seven multimodal reasoning benchmarks and various model architectures demonstrate that DMLR significantly improves reasoning and perception performance while maintaining high inference efficiency.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T10:07:45Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Chengzhi Liu</name>
    </author>
    <author>
      <name>Yuzhe Yang</name>
    </author>
    <author>
      <name>Yue Fan</name>
    </author>
    <author>
      <name>Qingyue Wei</name>
    </author>
    <author>
      <name>Sheng Liu</name>
    </author>
    <author>
      <name>Xin Eric Wang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12622v1</id>
    <title>D3D-VLP: Dynamic 3D Vision-Language-Planning Model for Embodied Grounding and Navigation</title>
    <updated>2025-12-14T09:53:15Z</updated>
    <link href="https://arxiv.org/abs/2512.12622v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12622v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Embodied agents face a critical dilemma that end-to-end models lack interpretability and explicit 3D reasoning, while modular systems ignore cross-component interdependencies and synergies. To bridge this gap, we propose the Dynamic 3D Vision-Language-Planning Model (D3D-VLP). Our model introduces two key innovations: 1) A Dynamic 3D Chain-of-Thought (3D CoT) that unifies planning, grounding, navigation, and question answering within a single 3D-VLM and CoT pipeline; 2) A Synergistic Learning from Fragmented Supervision (SLFS) strategy, which uses a masked autoregressive loss to learn from massive and partially-annotated hybrid data. This allows different CoT components to mutually reinforce and implicitly supervise each other. To this end, we construct a large-scale dataset with 10M hybrid samples from 5K real scans and 20K synthetic scenes that are compatible with online learning methods such as RL and DAgger. Our D3D-VLP achieves state-of-the-art results on multiple benchmarks, including Vision-and-Language Navigation (R2R-CE, REVERIE-CE, NavRAG-CE), Object-goal Navigation (HM3D-OVON), and Task-oriented Sequential Grounding and Navigation (SG3D). Real-world mobile manipulation experiments further validate the effectiveness.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T09:53:15Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Zihan Wang</name>
    </author>
    <author>
      <name>Seungjun Lee</name>
    </author>
    <author>
      <name>Guangzhao Dai</name>
    </author>
    <author>
      <name>Gim Hee Lee</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12620v1</id>
    <title>Understanding Syllogistic Reasoning in LLMs from Formal and Natural Language Perspectives</title>
    <updated>2025-12-14T09:50:10Z</updated>
    <link href="https://arxiv.org/abs/2512.12620v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12620v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We study syllogistic reasoning in LLMs from the logical and natural language perspectives. In process, we explore fundamental reasoning capabilities of the LLMs and the direction this research is moving forward. To aid in our studies, we use 14 large language models and investigate their syllogistic reasoning capabilities in terms of symbolic inferences as well as natural language understanding. Even though this reasoning mechanism is not a uniform emergent property across LLMs, the perfect symbolic performances in certain models make us wonder whether LLMs are becoming more and more formal reasoning mechanisms, rather than making explicit the nuances of human reasoning.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T09:50:10Z</published>
    <arxiv:comment>9 pages, 4 figures, 5 tables. Submitted to AAAI 2026 Bridge Program on Logic &amp; AI. Code available at https://github.com/XAheli/Logic-in-LLMs</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Aheli Poddar</name>
      <arxiv:affiliation>Institute of Engineering &amp; Management, Kolkata</arxiv:affiliation>
    </author>
    <author>
      <name>Saptarshi Sahoo</name>
      <arxiv:affiliation>Indian Statistical Institute, Chennai</arxiv:affiliation>
    </author>
    <author>
      <name>Sujata Ghosh</name>
      <arxiv:affiliation>Indian Statistical Institute, Chennai</arxiv:affiliation>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12617v1</id>
    <title>Spectral Sentinel: Scalable Byzantine-Robust Decentralized Federated Learning via Sketched Random Matrix Theory on Blockchain</title>
    <updated>2025-12-14T09:43:03Z</updated>
    <link href="https://arxiv.org/abs/2512.12617v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12617v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Decentralized federated learning (DFL) enables collaborative model training without centralized trust, but it remains vulnerable to Byzantine clients that poison gradients under heterogeneous (Non-IID) data. Existing defenses face a scalability trilemma: distance-based filtering (e.g., Krum) can reject legitimate Non-IID updates, geometric-median methods incur prohibitive $O(n^2 d)$ cost, and many certified defenses are evaluated only on models below 100M parameters. We propose Spectral Sentinel, a Byzantine detection and aggregation framework that leverages a random-matrix-theoretic signature: honest Non-IID gradients produce covariance eigenspectra whose bulk follows the Marchenko-Pastur law, while Byzantine perturbations induce detectable tail anomalies. Our algorithm combines Frequent Directions sketching with data-dependent MP tracking, enabling detection on models up to 1.5B parameters using $O(k^2)$ memory with $k \ll d$. Under a $(σ,f)$ threat model with coordinate-wise honest variance bounded by $σ^2$ and $f &lt; 1/2$ adversaries, we prove $(ε,δ)$-Byzantine resilience with convergence rate $O(σf / \sqrt{T} + f^2 / T)$, and we provide a matching information-theoretic lower bound $Ω(σf / \sqrt{T})$, establishing minimax optimality. We implement the full system with blockchain integration on Polygon networks and validate it across 144 attack-aggregator configurations, achieving 78.4 percent average accuracy versus 48-63 percent for baseline methods.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T09:43:03Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Animesh Mishra</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12613v1</id>
    <title>StruProKGR: A Structural and Probabilistic Framework for Sparse Knowledge Graph Reasoning</title>
    <updated>2025-12-14T09:36:58Z</updated>
    <link href="https://arxiv.org/abs/2512.12613v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12613v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Sparse Knowledge Graphs (KGs) are commonly encountered in real-world applications, where knowledge is often incomplete or limited. Sparse KG reasoning, the task of inferring missing knowledge over sparse KGs, is inherently challenging due to the scarcity of knowledge and the difficulty of capturing relational patterns in sparse scenarios. Among all sparse KG reasoning methods, path-based ones have attracted plenty of attention due to their interpretability. Existing path-based methods typically rely on computationally intensive random walks to collect paths, producing paths of variable quality. Additionally, these methods fail to leverage the structured nature of graphs by treating paths independently. To address these shortcomings, we propose a Structural and Probabilistic framework named StruProKGR, tailored for efficient and interpretable reasoning on sparse KGs. StruProKGR utilizes a distance-guided path collection mechanism to significantly reduce computational costs while exploring more relevant paths. It further enhances the reasoning process by incorporating structural information through probabilistic path aggregation, which prioritizes paths that reinforce each other. Extensive experiments on five sparse KG reasoning benchmarks reveal that StruProKGR surpasses existing path-based methods in both effectiveness and efficiency, providing an effective, efficient, and interpretable solution for sparse KG reasoning.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T09:36:58Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Yucan Guo</name>
    </author>
    <author>
      <name>Saiping Guan</name>
    </author>
    <author>
      <name>Miao Su</name>
    </author>
    <author>
      <name>Zeya Zhao</name>
    </author>
    <author>
      <name>Xiaolong Jin</name>
    </author>
    <author>
      <name>Jiafeng Guo</name>
    </author>
    <author>
      <name>Xueqi Cheng</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12610v1</id>
    <title>Patch-wise Retrieval: A Bag of Practical Techniques for Instance-level Matching</title>
    <updated>2025-12-14T09:24:51Z</updated>
    <link href="https://arxiv.org/abs/2512.12610v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12610v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Instance-level image retrieval aims to find images containing the same object as a given query, despite variations in size, position, or appearance. To address this challenging task, we propose Patchify, a simple yet effective patch-wise retrieval framework that offers high performance, scalability, and interpretability without requiring fine-tuning. Patchify divides each database image into a small number of structured patches and performs retrieval by comparing these local features with a global query descriptor, enabling accurate and spatially grounded matching. To assess not just retrieval accuracy but also spatial correctness, we introduce LocScore, a localization-aware metric that quantifies whether the retrieved region aligns with the target object. This makes LocScore a valuable diagnostic tool for understanding and improving retrieval behavior. We conduct extensive experiments across multiple benchmarks, backbones, and region selection strategies, showing that Patchify outperforms global methods and complements state-of-the-art reranking pipelines. Furthermore, we apply Product Quantization for efficient large-scale retrieval and highlight the importance of using informative features during compression, which significantly boosts performance. Project website: https://wons20k.github.io/PatchwiseRetrieval/</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T09:24:51Z</published>
    <arxiv:comment>WACV 2026</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Wonseok Choi</name>
    </author>
    <author>
      <name>Sohwi Lim</name>
    </author>
    <author>
      <name>Nam Hyeon-Woo</name>
    </author>
    <author>
      <name>Moon Ye-Bin</name>
    </author>
    <author>
      <name>Dong-Ju Jeong</name>
    </author>
    <author>
      <name>Jinyoung Hwang</name>
    </author>
    <author>
      <name>Tae-Hyun Oh</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12608v1</id>
    <title>Human-Inspired Learning for Large Language Models via Obvious Record and Maximum-Entropy Method Discovery</title>
    <updated>2025-12-14T09:12:09Z</updated>
    <link href="https://arxiv.org/abs/2512.12608v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12608v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Large Language Models (LLMs) excel at extracting common patterns from large-scale corpora, yet they struggle with rare, low-resource, or previously unseen scenarios-such as niche hardware deployment issues or irregular IoT device behaviors-because such cases are sparsely represented in training data. Moreover, LLMs rely primarily on implicit parametric memory, which limits their ability to explicitly acquire, recall, and refine methods, causing them to behave predominantly as intuition-driven predictors rather than deliberate, method-oriented learners. Inspired by how humans learn from rare experiences, this paper proposes a human-inspired learning framework that integrates two complementary mechanisms. The first, Obvious Record, explicitly stores cause--result (or question--solution) relationships as symbolic memory, enabling persistent learning even from single or infrequent encounters. The second, Maximum-Entropy Method Discovery, prioritizes and preserves methods with high semantic dissimilarity, allowing the system to capture diverse and underrepresented strategies that are typically overlooked by next-token prediction. Verification on a benchmark of 60 semantically diverse question--solution pairs demonstrates that the proposed entropy-guided approach achieves stronger coverage of unseen questions and significantly greater internal diversity than a random baseline, confirming its effectiveness in discovering more generalizable and human-inspired methods.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T09:12:09Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Hong Su</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12605v1</id>
    <title>Causal inference and model explainability tools for retail</title>
    <updated>2025-12-14T09:02:44Z</updated>
    <link href="https://arxiv.org/abs/2512.12605v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12605v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Most major retailers today have multiple divisions focused on various aspects, such as marketing, supply chain, online customer experience, store customer experience, employee productivity, and vendor fulfillment. They also regularly collect data corresponding to all these aspects as dashboards and weekly/monthly/quarterly reports. Although several machine learning and statistical techniques have been in place to analyze and predict key metrics, such models typically lack interpretability. Moreover, such techniques also do not allow the validation or discovery of causal links. In this paper, we aim to provide a recipe for applying model interpretability and causal inference for deriving sales insights. In this paper, we review the existing literature on causal inference and interpretability in the context of problems in e-commerce and retail, and apply them to a real-world dataset. We find that an inherently explainable model has a lower variance of SHAP values, and show that including multiple confounders through a double machine learning approach allows us to get the correct sign of causal effect.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T09:02:44Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Pranav Gupta</name>
    </author>
    <author>
      <name>Nithin Surendran</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12604v1</id>
    <title>No Cache Left Idle: Accelerating diffusion model via Extreme-slimming Caching</title>
    <updated>2025-12-14T09:02:18Z</updated>
    <link href="https://arxiv.org/abs/2512.12604v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12604v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Diffusion models achieve remarkable generative quality, but computational overhead scales with step count, model depth, and sequence length. Feature caching is effective since adjacent timesteps yield highly similar features. However, an inherent trade-off remains: aggressive timestep reuse offers large speedups but can easily cross the critical line, hurting fidelity, while block- or token-level reuse is safer but yields limited computational savings. We present X-Slim (eXtreme-Slimming Caching), a training-free, cache-based accelerator that, to our knowledge, is the first unified framework to exploit cacheable redundancy across timesteps, structure (blocks), and space (tokens). Rather than simply mixing levels, X-Slim introduces a dual-threshold controller that turns caching into a push-then-polish process: it first pushes reuse at the timestep level up to an early-warning line, then switches to lightweight block- and token-level refresh to polish the remaining redundancy, and triggers full inference once the critical line is crossed to reset accumulated error. At each level, context-aware indicators decide when and where to cache. Across diverse tasks, X-Slim advances the speed-quality frontier. On FLUX.1-dev and HunyuanVideo, it reduces latency by up to 4.97x and 3.52x with minimal perceptual loss. On DiT-XL/2, it reaches 3.13x acceleration and improves FID by 2.42 over prior methods.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T09:02:18Z</published>
    <arxiv:comment>Project page: https://thu-accdiff.github.io/xslim-page/ Code: https://github.com/THU-AccDiff/xslim</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Tingyan Wen</name>
    </author>
    <author>
      <name>Haoyu Li</name>
    </author>
    <author>
      <name>Yihuang Chen</name>
    </author>
    <author>
      <name>Xing Zhou</name>
    </author>
    <author>
      <name>Lifei Zhu</name>
    </author>
    <author>
      <name>Xueqian Wang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12602v1</id>
    <title>Error-Free Linear Attention is a Free Lunch: Exact Solution from Continuous-Time Dynamics</title>
    <updated>2025-12-14T08:51:02Z</updated>
    <link href="https://arxiv.org/abs/2512.12602v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12602v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Linear-time attention and State Space Models (SSMs) promise to solve the quadratic cost bottleneck in long-context language models employing softmax attention. We introduce Error-Free Linear Attention (EFLA), a numerically stable, fully parallelism and generalized formulation of the delta rule. Specifically, we formulate the online learning update as a continuous-time dynamical system and prove that its exact solution is not only attainable but also computable in linear time with full parallelism. By leveraging the rank-1 structure of the dynamics matrix, we directly derive the exact closed-form solution effectively corresponding to the infinite-order Runge-Kutta method. This attention mechanism is theoretically free from error accumulation, perfectly capturing the continuous dynamics while preserving the linear-time complexity. Through an extensive suite of experiments, we show that EFLA enables robust performance in noisy environments, achieving lower language modeling perplexity and superior downstream benchmark performance than DeltaNet without introducing additional parameters. Our work provides a new theoretical foundation for building high-fidelity, scalable linear-time attention models.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T08:51:02Z</published>
    <arxiv:comment>17 pages, 2 figures</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Jingdi Lei</name>
    </author>
    <author>
      <name>Di Zhang</name>
    </author>
    <author>
      <name>Soujanya Poria</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12598v1</id>
    <title>Geometry-Aware Scene-Consistent Image Generation</title>
    <updated>2025-12-14T08:35:04Z</updated>
    <link href="https://arxiv.org/abs/2512.12598v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12598v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We study geometry-aware scene-consistent image generation: given a reference scene image and a text condition specifying an entity to be generated in the scene and its spatial relation to the scene, the goal is to synthesize an output image that preserves the same physical environment as the reference scene while correctly generating the entity according to the spatial relation described in the text. Existing methods struggle to balance scene preservation with prompt adherence: they either replicate the scene with high fidelity but poor responsiveness to the prompt, or prioritize prompt compliance at the expense of scene consistency. To resolve this trade-off, we introduce two key contributions: (i) a scene-consistent data construction pipeline that generates diverse, geometrically-grounded training pairs, and (ii) a novel geometry-guided attention loss that leverages cross-view cues to regularize the model's spatial reasoning. Experiments on our scene-consistent benchmark show that our approach achieves better scene alignment and text-image consistency than state-of-the-art baselines, according to both automatic metrics and human preference studies. Our method produces geometrically coherent images with diverse compositions that remain faithful to the textual instructions and the underlying scene structure.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T08:35:04Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Cong Xie</name>
    </author>
    <author>
      <name>Che Wang</name>
    </author>
    <author>
      <name>Yan Zhang</name>
    </author>
    <author>
      <name>Zheng Pan</name>
    </author>
    <author>
      <name>Han Zou</name>
    </author>
    <author>
      <name>Zhenpeng Zhan</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12597v1</id>
    <title>AgentSHAP: Interpreting LLM Agent Tool Importance with Monte Carlo Shapley Value Estimation</title>
    <updated>2025-12-14T08:31:43Z</updated>
    <link href="https://arxiv.org/abs/2512.12597v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12597v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>LLM agents that use external tools can solve complex tasks, but understanding which tools actually contributed to a response remains a blind spot. No existing XAI methods address tool-level explanations. We introduce AgentSHAP, the first framework for explaining tool importance in LLM agents. AgentSHAP is model-agnostic: it treats the agent as a black box and works with any LLM (GPT, Claude, Llama, etc.) without needing access to internal weights or gradients. Using Monte Carlo Shapley values, AgentSHAP tests how an agent responds with different tool subsets and computes fair importance scores based on game theory. Our contributions are: (1) the first explainability method for agent tool attribution, grounded in Shapley values from game theory; (2) Monte Carlo sampling that reduces cost from O(2n) to practical levels; and (3) comprehensive experiments on API-Bank showing that AgentSHAP produces consistent scores across runs, correctly identifies which tools matter, and distinguishes relevant from irrelevant tools. AgentSHAP joins TokenSHAP (for tokens) and PixelSHAP (for image regions) to complete a family of Shapley-based XAI tools for modern generative AI. Code: https://github.com/GenAISHAP/TokenSHAP.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T08:31:43Z</published>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Miriam Horovicz</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12596v1</id>
    <title>Content-Aware Ad Banner Layout Generation with Two-Stage Chain-of-Thought in Vision Language Models</title>
    <updated>2025-12-14T08:30:15Z</updated>
    <link href="https://arxiv.org/abs/2512.12596v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12596v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>In this paper, we propose a method for generating layouts for image-based advertisements by leveraging a Vision-Language Model (VLM). Conventional advertisement layout techniques have predominantly relied on saliency mapping to detect salient regions within a background image, but such approaches often fail to fully account for the image's detailed composition and semantic content. To overcome this limitation, our method harnesses a VLM to recognize the products and other elements depicted in the background and to inform the placement of text and logos. The proposed layout-generation pipeline consists of two steps. In the first step, the VLM analyzes the image to identify object types and their spatial relationships, then produces a text-based "placement plan" based on this analysis. In the second step, that plan is rendered into the final layout by generating HTML-format code. We validated the effectiveness of our approach through evaluation experiments, conducting both quantitative and qualitative comparisons against existing methods. The results demonstrate that by explicitly considering the background image's content, our method produces noticeably higher-quality advertisement layouts.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T08:30:15Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Kei Yoshitake</name>
    </author>
    <author>
      <name>Kento Hosono</name>
    </author>
    <author>
      <name>Ken Kobayashi</name>
    </author>
    <author>
      <name>Kazuhide Nakata</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12595v1</id>
    <title>Vision-Enhanced Large Language Models for High-Resolution Image Synthesis and Multimodal Data Interpretation</title>
    <updated>2025-12-14T08:28:50Z</updated>
    <link href="https://arxiv.org/abs/2512.12595v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12595v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>This research introduces a transformative framework for integrating Vision-Enhanced Large Language Models (LLMs) with advanced transformer-based architectures to tackle challenges in high-resolution image synthesis and multimodal data interpretation. The proposed model incorporates a rectified flow mechanism that connects noise and data with linear paths, enabling efficient and high-quality generation. A bidirectional tokenization strategy is employed to seamlessly merge inputs from text, image, and video modalities, fostering a unified understanding across diverse data types. By embedding spatial-temporal features and leveraging a hybrid text-image sequence modeling approach, the framework achieves unparalleled fidelity in synthesized images and coherent multimodal representations. The architecture is optimized with a noise-aware learning algorithm, addressing discrepancies in noisy data distributions and improving generative performance under varying input conditions. Rigorous evaluations on benchmark datasets demonstrate a 25% increase in image resolution clarity and a 20% reduction in computational requirements compared to diffusion-based methods. Furthermore, the model exhibits robust scalability and adaptability, showcasing its potential in applications like autonomous systems, creative content generation, and advanced video analysis. This work underscores the role of vision-centric LLMs in redefining capabilities in computer vision and multimodal artificial intelligence.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T08:28:50Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Karthikeya KV</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12594v1</id>
    <title>ceLLMate: Sandboxing Browser AI Agents</title>
    <updated>2025-12-14T08:25:31Z</updated>
    <link href="https://arxiv.org/abs/2512.12594v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12594v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Browser-using agents (BUAs) are an emerging class of autonomous agents that interact with web browsers in human-like ways, including clicking, scrolling, filling forms, and navigating across pages. While these agents help automate repetitive online tasks, they are vulnerable to prompt injection attacks that can trick an agent into performing undesired actions, such as leaking private information or issuing state-changing requests. We propose ceLLMate, a browser-level sandboxing framework that restricts the agent's ambient authority and reduces the blast radius of prompt injections. We address two fundamental challenges: (1) The semantic gap challenge in policy enforcement arises because the agent operates through low-level UI observations and manipulations; however, writing and enforcing policies directly over UI-level events is brittle and error-prone. To address this challenge, we introduce an agent sitemap that maps low-level browser behaviors to high-level semantic actions. (2) Policy prediction in BUAs is the norm rather than the exception. BUAs have no app developer to pre-declare sandboxing policies, and thus, ceLLMate pairs website-authored mandatory policies with an automated policy-prediction layer that adapts and instantiates these policies from the user's natural-language task. We implement ceLLMate as an agent-agnostic browser extension and demonstrate how it enables sandboxing policies that effectively block various types of prompt injection attacks with negligible overhead.</summary>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T08:25:31Z</published>
    <arxiv:comment>Homepage: https://cellmate-sandbox.github.io</arxiv:comment>
    <arxiv:primary_category term="cs.CR"/>
    <author>
      <name>Luoxi Meng</name>
    </author>
    <author>
      <name>Henry Feng</name>
    </author>
    <author>
      <name>Ilia Shumailov</name>
    </author>
    <author>
      <name>Earlence Fernandes</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12593v1</id>
    <title>SHERLOCK: A Deep Learning Approach To Detect Software Vulnerabilities</title>
    <updated>2025-12-14T08:24:06Z</updated>
    <link href="https://arxiv.org/abs/2512.12593v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12593v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The increasing reliance on software in various applications has made the problem of software vulnerability detection more critical. Software vulnerabilities can lead to security breaches, data theft, and other negative outcomes. Traditional software vulnerability detection techniques, such as static and dynamic analysis, have been shown to be ineffective at detecting multiple vulnerabilities.
  To address this issue, this study employed a deep learning approach, specifically Convolutional Neural Networks (CNN), to solve the software vulnerability detection problem. A 5-split cross-validation approach was used to train and evaluate the CNN model, which takes tokenized source code as input.
  The findings indicated that Sherlock successfully detected multiple vulnerabilities at the function level, and its performance was particularly strong for CWE-199, CWE-120, and CWE-Other, with an overall high accuracy rate and significant true positive and true negative values. However, the performance was less reliable for some vulnerabilities due to the lack of a standardized dataset which will be a future research direction. The results suggest that compared to current techniques, the proposed deep learning approach has the potential to substantially enhance the accuracy of software vulnerability detection.</summary>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T08:24:06Z</published>
    <arxiv:primary_category term="cs.SE"/>
    <author>
      <name>Saadh Jawwadh</name>
    </author>
    <author>
      <name>Guhanathan Poravi</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12590v1</id>
    <title>Automatic Wire-Harness Color Sequence Detector</title>
    <updated>2025-12-14T08:12:03Z</updated>
    <link href="https://arxiv.org/abs/2512.12590v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12590v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Wire harness inspection process remains a labor-intensive process prone to errors in the modern Electronics Manufacturing Services (EMS) industry. This paper introduces a semiautomated machine vision system capable of verifying correct wire positioning, correctness of the connector polarity and correctness of color sequences for both linear and circular wire harness configurations. Five industrial standard CMOS cameras are integrated into a modularized mechanical framework in the physical structure of the solution and a HSV and RGB color domain value comparison based color sequence classifier is used in the operation. For each harness batch, a user can train the system using at least five reference samples; the trained file is stored and reused for similar harness types. The Solution is deployed at GPV Lanka Pvt. Ltd. (Fig. 2) and the system achieved 100% detection accuracy and reduced inspection time by 44% compared to manual methods. Additional features include user management, adjustable lighting, session data storage, and secure login. Results of this product usage in the real world situation demonstrate that this approach delivers reliable and efficient inspection capabilities.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T08:12:03Z</published>
    <arxiv:comment>6 pages, 20 figures, IEEE ICIIS 2025 Conference - Accepted</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Indiwara Nanayakkara</name>
    </author>
    <author>
      <name>Dehan Jayawickrama</name>
    </author>
    <author>
      <name>Mervyn Parakrama B. Ekanayake</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12586v1</id>
    <title>StegaVAR: Privacy-Preserving Video Action Recognition via Steganographic Domain Analysis</title>
    <updated>2025-12-14T07:44:07Z</updated>
    <link href="https://arxiv.org/abs/2512.12586v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12586v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Despite the rapid progress of deep learning in video action recognition (VAR) in recent years, privacy leakage in videos remains a critical concern. Current state-of-the-art privacy-preserving methods often rely on anonymization. These methods suffer from (1) low concealment, where producing visually distorted videos that attract attackers' attention during transmission, and (2) spatiotemporal disruption, where degrading essential spatiotemporal features for accurate VAR. To address these issues, we propose StegaVAR, a novel framework that embeds action videos into ordinary cover videos and directly performs VAR in the steganographic domain for the first time. Throughout both data transmission and action analysis, the spatiotemporal information of hidden secret video remains complete, while the natural appearance of cover videos ensures the concealment of transmission. Considering the difficulty of steganographic domain analysis, we propose Secret Spatio-Temporal Promotion (STeP) and Cross-Band Difference Attention (CroDA) for analysis within the steganographic domain. STeP uses the secret video to guide spatiotemporal feature extraction in the steganographic domain during training. CroDA suppresses cover interference by capturing cross-band semantic differences. Experiments demonstrate that StegaVAR achieves superior VAR and privacy-preserving performance on widely used datasets. Moreover, our framework is effective for multiple steganographic models.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T07:44:07Z</published>
    <arxiv:comment>13 pages, 10 figures. This is the extended version of the paper accepted at AAAI 2026, including related works and appendix</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Lixin Chen</name>
    </author>
    <author>
      <name>Chaomeng Chen</name>
    </author>
    <author>
      <name>Jiale Zhou</name>
    </author>
    <author>
      <name>Zhijian Wu</name>
    </author>
    <author>
      <name>Xun Lin</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12583v1</id>
    <title>Detecting Prompt Injection Attacks Against Application Using Classifiers</title>
    <updated>2025-12-14T07:35:32Z</updated>
    <link href="https://arxiv.org/abs/2512.12583v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12583v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Prompt injection attacks can compromise the security and stability of critical systems, from infrastructure to large web applications. This work curates and augments a prompt injection dataset based on the HackAPrompt Playground Submissions corpus and trains several classifiers, including LSTM, feed forward neural networks, Random Forest, and Naive Bayes, to detect malicious prompts in LLM integrated web applications. The proposed approach improves prompt injection detection and mitigation, helping protect targeted applications and systems.</summary>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T07:35:32Z</published>
    <arxiv:comment>9 pages, X figures; undergraduate research project on detecting prompt injection attacks against LLM integrated web applications using classical machine learning and neural classifiers</arxiv:comment>
    <arxiv:primary_category term="cs.CR"/>
    <author>
      <name>Safwan Shaheer</name>
    </author>
    <author>
      <name>G. M. Refatul Islam</name>
    </author>
    <author>
      <name>Mohammad Rafid Hamid</name>
    </author>
    <author>
      <name>Md. Abrar Faiaz Khan</name>
    </author>
    <author>
      <name>Md. Omar Faruk</name>
    </author>
    <author>
      <name>Yaseen Nur</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12581v1</id>
    <title>Differentiable Energy-Based Regularization in GANs: A Simulator-Based Exploration of VQE-Inspired Auxiliary Losses</title>
    <updated>2025-12-14T07:23:57Z</updated>
    <link href="https://arxiv.org/abs/2512.12581v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12581v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>This paper presents an exploratory, simulator-based proof of concept investigating whether differentiable energy terms derived from parameterized quantum circuits can serve as auxiliary regularization signals in Generative Adversarial Networks (GANs). We augment the Auxiliary Classifier GAN (ACGAN) generator objective with a Variational Quantum Eigensolver (VQE)-inspired energy term computed from class-specific Ising Hamiltonians using Qiskit's EstimatorQNN and TorchConnector.
  Important limitations: All experiments run on a noiseless statevector simulator with only 4 qubits, use a deliberately simple Hamiltonian parameterization, and lack ablation studies comparing against equivalent classical biases. The computational overhead (approximately 200x slower than classical ACGAN) reflects simulator artifacts rather than inherent quantum costs.
  On MNIST, we observe that the energy-regularized model (termed QACGAN) achieves high classification accuracy (99 to 100 percent) within 5 epochs compared to 87.8 percent for ACGAN, suggesting the auxiliary term influences class conditioning. However, sample quality metrics (FID) show high variance across runs (coefficient of variation approximately 25 percent at epoch 5), with values ranging from 19.92 to 35.96. Extended runs stabilize around FID 23 to 24, comparable to the ACGAN baseline.
  We explicitly do not claim quantum advantage, improved stability in any general sense, or scalability beyond this toy setting. The contribution is methodological: demonstrating that VQE-style energy computations can be integrated into GAN training loops via differentiable pathways. Whether such auxiliary signals provide benefits beyond equivalent classical regularizers remains an open question requiring systematic ablation studies, which we leave for future work.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T07:23:57Z</published>
    <arxiv:comment>Exploratory, simulator-based proof of concept. No claims of quantum advantage</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>David Strnadel</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12580v1</id>
    <title>Cryptographic transformations over polyadic rings</title>
    <updated>2025-12-14T07:15:55Z</updated>
    <link href="https://arxiv.org/abs/2512.12580v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12580v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>This article introduces a novel cryptographic paradigm based on nonderived polyadic algebraic structures. Traditional cryptosystems rely on binary operations within groups, rings, or fields, whose well-understood properties can be exploited in cryptanalysis. To overcome these vulnerabilities, we propose a shift to polyadic rings, which generalize classical rings by allowing operations of higher arity: an $m$-ary addition and an $n$-ary multiplication. The foundation of our approach is the construction of polyadic integers -- congruence classes of ordinary integers endowed with such $m$-ary and $n$-ary operations. A key innovation is the parameter-to-arity mapping $Φ(a,b)=(m,n)$, which links the parameters $(a,b)$ defining a congruence class to the specific arities required for algebraic closure. This mapping is mathematically intricate: it is non-injective, non-surjective, and multivalued. This complex, non-unique relationship forms the core of the proposed cryptosystem's security. We present two concrete encryption procedures that leverage this structure by encoding plaintext within the parameters of polyadic rings and transmitting information via polyadically quantized analog signals. In one method, plaintext is linked to the additive arity $m_{i}$ and secured using the summation of such signals; in the other, it is linked to a ring parameter $a_{i}$ and secured using their multiplication. In both cases, the "quantized" nature of polyadic operations generates systems of equations that are straightforward for a legitimate recipient with the correct key but exceptionally difficult for an attacker without it. The resulting framework promises a substantial increase in cryptographic security. This work establishes the theoretical foundation for this new class of encryption schemes and highlights their potential for constructing robust, next-generation cryptographic protocols.</summary>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T07:15:55Z</published>
    <arxiv:comment>21 pages, revtex 4.2</arxiv:comment>
    <arxiv:primary_category term="cs.CR"/>
    <author>
      <name>Steven Duplij</name>
    </author>
    <author>
      <name>Na Fu</name>
    </author>
    <author>
      <name>Qiang Guo</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12578v1</id>
    <title>Scalable Quantum Error Mitigation with Neighbor-Informed Learning</title>
    <updated>2025-12-14T07:07:48Z</updated>
    <link href="https://arxiv.org/abs/2512.12578v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12578v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Noise in quantum hardware is the primary obstacle to realizing the transformative potential of quantum computing. Quantum error mitigation (QEM) offers a promising pathway to enhance computational accuracy on near-term devices, yet existing methods face a difficult trade-off between performance, resource overhead, and theoretical guarantees. In this work, we introduce neighbor-informed learning (NIL), a versatile and scalable QEM framework that unifies and strengthens existing methods such as zero-noise extrapolation (ZNE) and probabilistic error cancellation (PEC), while offering improved flexibility, accuracy, efficiency, and robustness.
  NIL learns to predict the ideal output of a target quantum circuit from the noisy outputs of its structurally related ``neighbor'' circuits. A key innovation is our 2-design training method, which generates training data for our machine learning model. In contrast to conventional learning-based QEM protocols that create training circuits by replacing non-Clifford gates with uniformly random Clifford gates, our approach achieves higher accuracy and efficiency, as demonstrated by both theoretical analysis and numerical simulation. Furthermore, we prove that the required size of the training set scales only \emph{logarithmically} with the total number of neighbor circuits, enabling NIL to be applied to problems involving large-scale quantum circuits. Our work establishes a theoretically grounded and practically efficient framework for QEM, paving a viable path toward achieving quantum advantage on noisy hardware.</summary>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T07:07:48Z</published>
    <arxiv:primary_category term="quant-ph"/>
    <author>
      <name>Zhenyu Chen</name>
    </author>
    <author>
      <name>Bin Cheng</name>
    </author>
    <author>
      <name>Minbo Gao</name>
    </author>
    <author>
      <name>Xiaodie Lin</name>
    </author>
    <author>
      <name>Ruiqi Zhang</name>
    </author>
    <author>
      <name>Zhaohui Wei</name>
    </author>
    <author>
      <name>Zhengfeng Ji</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12576v1</id>
    <title>Coupled Variational Reinforcement Learning for Language Model General Reasoning</title>
    <updated>2025-12-14T07:03:51Z</updated>
    <link href="https://arxiv.org/abs/2512.12576v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12576v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>While reinforcement learning have achieved impressive progress in language model reasoning, they are constrained by the requirement for verifiable rewards. Recent verifier-free RL methods address this limitation by utilizing the intrinsic probabilities of LLMs generating reference answers as reward signals. However, these approaches typically sample reasoning traces conditioned only on the question. This design decouples reasoning-trace sampling from answer information, leading to inefficient exploration and incoherence between traces and final answers. In this paper, we propose \textit{\b{Co}upled \b{V}ariational \b{R}einforcement \b{L}earning} (CoVRL), which bridges variational inference and reinforcement learning by coupling prior and posterior distributions through a hybrid sampling strategy. By constructing and optimizing a composite distribution that integrates these two distributions, CoVRL enables efficient exploration while preserving strong thought-answer coherence. Extensive experiments on mathematical and general reasoning benchmarks show that CoVRL improves performance by 12.4\% over the base model and achieves an additional 2.3\% improvement over strong state-of-the-art verifier-free RL baselines, providing a principled framework for enhancing the general reasoning capabilities of language models.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T07:03:51Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Xueru Wen</name>
    </author>
    <author>
      <name>Jie Lou</name>
    </author>
    <author>
      <name>Yanjiang Liu</name>
    </author>
    <author>
      <name>Hongyu Lin</name>
    </author>
    <author>
      <name>Ben He</name>
    </author>
    <author>
      <name>Xianpei Han</name>
    </author>
    <author>
      <name>Le Sun</name>
    </author>
    <author>
      <name>Yaojie Lu</name>
    </author>
    <author>
      <name>Debing Zhang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12574v1</id>
    <title>Mind the Jumps: A Scalable Robust Local Gaussian Process for Multidimensional Response Surfaces with Discontinuities</title>
    <updated>2025-12-14T06:52:17Z</updated>
    <link href="https://arxiv.org/abs/2512.12574v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12574v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Modeling response surfaces with abrupt jumps and discontinuities remains a major challenge across scientific and engineering domains. Although Gaussian process models excel at capturing smooth nonlinear relationships, their stationarity assumptions limit their ability to adapt to sudden input-output variations. Existing nonstationary extensions, particularly those based on domain partitioning, often struggle with boundary inconsistencies, sensitivity to outliers, and scalability issues in higher-dimensional settings, leading to reduced predictive accuracy and unreliable parameter estimation.
  To address these challenges, this paper proposes the Robust Local Gaussian Process (RLGP) model, a framework that integrates adaptive nearest-neighbor selection with a sparsity-driven robustification mechanism. Unlike existing methods, RLGP leverages an optimization-based mean-shift adjustment after a multivariate perspective transformation combined with local neighborhood modeling to mitigate the influence of outliers. This approach improves predictive accuracy near discontinuities while enhancing robustness to data heterogeneity.
  Comprehensive evaluations on real-world datasets show that RLGP consistently delivers high predictive accuracy and maintains competitive computational efficiency, especially in scenarios with sharp transitions and complex response structures. Scalability tests further confirm RLGP's stability and reliability in higher-dimensional settings, where other methods struggle. These results establish RLGP as an effective and practical solution for modeling nonstationary and discontinuous response surfaces across a wide range of applications.</summary>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T06:52:17Z</published>
    <arxiv:comment>25 pages, 4 figures</arxiv:comment>
    <arxiv:primary_category term="stat.ML"/>
    <arxiv:journal_ref>Neurocomputing 132317 (2025)</arxiv:journal_ref>
    <author>
      <name>Isaac Adjetey</name>
    </author>
    <author>
      <name>Yiyuan She</name>
    </author>
    <arxiv:doi>10.1016/j.neucom.2025.132317</arxiv:doi>
    <link rel="related" href="https://doi.org/10.1016/j.neucom.2025.132317" title="doi"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12572v1</id>
    <title>On the Accuracy of Newton Step and Influence Function Data Attributions</title>
    <updated>2025-12-14T06:33:52Z</updated>
    <link href="https://arxiv.org/abs/2512.12572v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12572v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Data attribution aims to explain model predictions by estimating how they would change if certain training points were removed, and is used in a wide range of applications, from interpretability and credit assignment to unlearning and privacy.
  Even in the relatively simple case of linear regressions, existing mathematical analyses of leading data attribution methods such as Influence Functions (IF) and single Newton Step (NS) remain limited in two key ways. First, they rely on global strong convexity assumptions which are often not satisfied in practice. Second, the resulting bounds scale very poorly with the number of parameters ($d$) and the number of samples removed ($k$). As a result, these analyses are not tight enough to answer fundamental questions such as "what is the asymptotic scaling of the errors of each method?" or "which of these methods is more accurate for a given dataset?"
  In this paper, we introduce a new analysis of the NS and IF data attribution methods for convex learning problems. To the best of our knowledge, this is the first analysis of these questions that does not assume global strong convexity and also the first explanation of [KATL19] and [RH25a]'s observation that NS data attribution is often more accurate than IF. We prove that for sufficiently well-behaved logistic regression, our bounds are asymptotically tight up to poly-logarithmic factors, yielding scaling laws for the errors in the average-case sample removals.
  \[ \mathbb{E}_{T \subseteq [n],\, |T| = k} \bigl[ \|\hatθ_T - \hatθ_T^{\mathrm{NS}}\|_2 \bigr] = \widetildeΘ\!\left(\frac{k d}{n^2}\right), \qquad \mathbb{E}_{T \subseteq [n],\, |T| = k} \bigl[ \|\hatθ_T^{\mathrm{NS}} - \hatθ_T^{\mathrm{IF}}\|_2 \bigr] = \widetildeΘ\!\left( \frac{(k + d)\sqrt{k d}}{n^2} \right). \]</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T06:33:52Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Ittai Rubinstein</name>
    </author>
    <author>
      <name>Samuel B. Hopkins</name>
    </author>
  </entry>
</feed>
