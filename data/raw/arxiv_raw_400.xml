<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/" xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns="http://www.w3.org/2005/Atom">
  <id>https://arxiv.org/api/nbd6bMtOXlgiQx8cmdcTZkw7dT8</id>
  <title>arXiv Query: search_query=cat:cs.AI OR cat:cs.LG OR cat:cs.CL OR cat:cs.CV OR cat:cs.NE OR cat:cs.SE OR cat:cs.CR OR cat:cs.DC&amp;id_list=&amp;start=400&amp;max_results=100</title>
  <updated>2025-12-16T13:50:49Z</updated>
  <link href="https://arxiv.org/api/query?search_query=cat:cs.AI+OR+(cat:cs.LG+OR+(cat:cs.CL+OR+(cat:cs.CV+OR+(cat:cs.NE+OR+(cat:cs.SE+OR+(cat:cs.CR+OR+cat:cs.DC))))))&amp;start=400&amp;max_results=100&amp;id_list=" type="application/atom+xml"/>
  <opensearch:itemsPerPage>100</opensearch:itemsPerPage>
  <opensearch:totalResults>566020</opensearch:totalResults>
  <opensearch:startIndex>400</opensearch:startIndex>
  <entry>
    <id>http://arxiv.org/abs/2512.12571v1</id>
    <title>From Tokens to Photons: Test-Time Physical Prompting for Vison-Language Models</title>
    <updated>2025-12-14T06:30:32Z</updated>
    <link href="https://arxiv.org/abs/2512.12571v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12571v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>To extend the application of vision-language models (VLMs) from web images to sensor-mediated physical environments, we propose Multi-View Physical-prompt for Test-Time Adaptation (MVP), a forward-only framework that moves test-time adaptation (TTA) from tokens to photons by treating the camera exposure triangle--ISO, shutter speed, and aperture--as physical prompts. At inference, MVP acquires a library of physical views per scene, selects the top-k sensor settings using a source-affinity score, evaluates each retained view under lightweight digital augmentations, filters the lowest-entropy subset of augmented views, and aggregates predictions with Zero-temperature softmax (i.e., hard voting). This selection-then-vote design is simple, calibration-friendly, and requires no gradients or model modifications. On ImageNet-ES and ImageNet-ES-Diverse, MVP consistently outperforms digital-only TTA on single Auto-Exposure captures, by up to 25.6 percentage points (pp), and delivers up to 3.4 pp additional gains over pipelines that combine conventional sensor control with TTA. MVP remains effective under reduced parameter candidate sets that lower capture latency, demonstrating practicality. These results support the main claim that, beyond post-capture prompting, measurement-time control--selecting and combining real physical views--substantially improves robustness for VLMs.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T06:30:32Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Boyeong Im</name>
    </author>
    <author>
      <name>Wooseok Lee</name>
    </author>
    <author>
      <name>Yoojin Kwon</name>
    </author>
    <author>
      <name>Hyung-Sin Kim</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12568v1</id>
    <title>Intelligent Adaptive Federated Byzantine Agreement for Robust Blockchain Consensus</title>
    <updated>2025-12-14T06:25:52Z</updated>
    <link href="https://arxiv.org/abs/2512.12568v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12568v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The Federated Byzantine Agreement (FBA) achieves rapid consensus by relying on overlapping quorum slices. But this architecture leads to a high dependence on the availability of validators when about one fourth of validators go down, the classical FBA can lose liveness or fail to reach agreement. We thus come up with an Adaptive FBA architecture that can reconfigure quorum slices intelligently based on real time validator reputation to overcome this drawback. Our model includes trust scores computed from EigenTrust and a sliding window behavioral assessment to determine the reliability of validators.
  We have built the intelligent adaptive FBA model and conducted tests in a Stellar based setting. Results of real life experiments reveal that the system is stable enough to keep consensus when more than half of the validators (up to 62 percent) are disconnected, which is a great extension of the failure threshold of a classical FBA. A fallback mode allows the network to be functional with as few as three validators, thus showing a significant robustness enhancement. Besides, a comparative study with the existing consensus protocols shows that Adaptive FBA can be an excellent choice for the next generation of blockchain systems, especially for constructing a resilient blockchain infrastructure.</summary>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T06:25:52Z</published>
    <arxiv:primary_category term="cs.CR"/>
    <author>
      <name>Erdhi Widyarto Nugroho</name>
    </author>
    <author>
      <name>R. Rizal Isnanto</name>
    </author>
    <author>
      <name>Luhur Bayuaji</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12567v1</id>
    <title>Optimal Mistake Bounds for Transductive Online Learning</title>
    <updated>2025-12-14T06:16:11Z</updated>
    <link href="https://arxiv.org/abs/2512.12567v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12567v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We resolve a 30-year-old open problem concerning the power of unlabeled data in online learning by tightly quantifying the gap between transductive and standard online learning. In the standard setting, the optimal mistake bound is characterized by the Littlestone dimension $d$ of the concept class $H$ (Littlestone 1987). We prove that in the transductive setting, the mistake bound is at least $Ω(\sqrt{d})$. This constitutes an exponential improvement over previous lower bounds of $Ω(\log\log d)$, $Ω(\sqrt{\log d})$, and $Ω(\log d)$, due respectively to Ben-David, Kushilevitz, and Mansour (1995, 1997) and Hanneke, Moran, and Shafer (2023). We also show that this lower bound is tight: for every $d$, there exists a class of Littlestone dimension $d$ with transductive mistake bound $O(\sqrt{d})$. Our upper bound also improves upon the best known upper bound of $(2/3)d$ from Ben-David, Kushilevitz, and Mansour (1997). These results establish a quadratic gap between transductive and standard online learning, thereby highlighting the benefit of advance access to the unlabeled instance sequence. This contrasts with the PAC setting, where transductive and standard learning exhibit similar sample complexities.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T06:16:11Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Zachary Chase</name>
    </author>
    <author>
      <name>Steve Hanneke</name>
    </author>
    <author>
      <name>Shay Moran</name>
    </author>
    <author>
      <name>Jonathan Shafer</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12560v1</id>
    <title>StreamingAssistant: Efficient Visual Token Pruning for Accelerating Online Video Understanding</title>
    <updated>2025-12-14T05:35:11Z</updated>
    <link href="https://arxiv.org/abs/2512.12560v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12560v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Online video understanding is essential for applications like public surveillance and AI glasses. However, applying Multimodal Large Language Models (MLLMs) to this domain is challenging due to the large number of video frames, resulting in high GPU memory usage and computational latency. To address these challenges, we propose token pruning as a means to reduce context length while retaining critical information. Specifically, we introduce a novel redundancy metric, Maximum Similarity to Spatially Adjacent Video Tokens (MSSAVT), which accounts for both token similarity and spatial position. To mitigate the bidirectional dependency between pruning and redundancy, we further design a masked pruning strategy that ensures only mutually unadjacent tokens are pruned. We also integrate an existing temporal redundancy-based pruning method to eliminate temporal redundancy of the video modality. Experimental results on multiple online and offline video understanding benchmarks demonstrate that our method significantly improves the accuracy (i.e., by 4\% at most) while incurring a negligible pruning latency (i.e., less than 1ms). Our full implementation will be made publicly available.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T05:35:11Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Xinqi Jin</name>
    </author>
    <author>
      <name>Hanxun Yu</name>
    </author>
    <author>
      <name>Bohan Yu</name>
    </author>
    <author>
      <name>Kebin Liu</name>
    </author>
    <author>
      <name>Jian Liu</name>
    </author>
    <author>
      <name>Keda Tao</name>
    </author>
    <author>
      <name>Yixuan Pei</name>
    </author>
    <author>
      <name>Huan Wang</name>
    </author>
    <author>
      <name>Fan Dang</name>
    </author>
    <author>
      <name>Jiangchuan Liu</name>
    </author>
    <author>
      <name>Weiqiang Wang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12559v1</id>
    <title>Unveiling Malicious Logic: Towards a Statement-Level Taxonomy and Dataset for Securing Python Packages</title>
    <updated>2025-12-14T05:28:30Z</updated>
    <link href="https://arxiv.org/abs/2512.12559v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12559v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The widespread adoption of open-source ecosystems enables developers to integrate third-party packages, but also exposes them to malicious packages crafted to execute harmful behavior via public repositories such as PyPI. Existing datasets (e.g., pypi-malregistry, DataDog, OpenSSF, MalwareBench) label packages as malicious or benign at the package level, but do not specify which statements implement malicious behavior. This coarse granularity limits research and practice: models cannot be trained to localize malicious code, detectors cannot justify alerts with code-level evidence, and analysts cannot systematically study recurring malicious indicators or attack chains. To address this gap, we construct a statement-level dataset of 370 malicious Python packages (833 files, 90,527 lines) with 2,962 labeled occurrences of malicious indicators. From these annotations, we derive a fine-grained taxonomy of 47 malicious indicators across 7 types that capture how adversarial behavior is implemented in code, and we apply sequential pattern mining to uncover recurring indicator sequences that characterize common attack workflows. Our contribution enables explainable, behavior-centric detection and supports both semantic-aware model training and practical heuristics for strengthening software supply-chain defenses.</summary>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T05:28:30Z</published>
    <arxiv:primary_category term="cs.CR"/>
    <author>
      <name>Ahmed Ryan</name>
    </author>
    <author>
      <name>Junaid Mansur Ifti</name>
    </author>
    <author>
      <name>Md Erfan</name>
    </author>
    <author>
      <name>Akond Ashfaque Ur Rahman</name>
    </author>
    <author>
      <name>Md Rayhanur Rahman</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12553v1</id>
    <title>Cargo Sherlock: An SMT-Based Checker for Software Trust Costs</title>
    <updated>2025-12-14T04:59:10Z</updated>
    <link href="https://arxiv.org/abs/2512.12553v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12553v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Supply chain attacks threaten open-source software ecosystems. This paper proposes a formal framework for quantifying trust in third-party software dependencies that is both formally checkable - formalized in satisfiability modulo theories (SMT) - while at the same time incorporating human factors, like the number of downloads, authors, and other metadata that are commonly used to identify trustworthy software in practice. We use data from both software analysis tools and metadata to build a first-order relational model of software dependencies; to obtain an overall "trust cost" combining these factors, we propose a formalization based on the minimum trust problem which asks for the minimum cost of a set of assumptions which can be used to prove that the code is safe. We implement these ideas in Cargo Sherlock, targeted for Rust libraries (crates), incorporating a list of candidate assumptions motivated by quantifiable trust metrics identified in prior work. Our evaluation shows that Cargo Sherlock can be used to identify synthetically generated supply chain attacks and known incidents involving typosquatted and poorly AI-maintained crates, and that its performance scales to Rust crates with many dependencies.</summary>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T04:59:10Z</published>
    <arxiv:comment>12 pages, 7 figures</arxiv:comment>
    <arxiv:primary_category term="cs.LO"/>
    <author>
      <name>Muhammad Hassnain</name>
    </author>
    <author>
      <name>Anirudh Basu</name>
    </author>
    <author>
      <name>Ethan Ng</name>
    </author>
    <author>
      <name>Caleb Stanford</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12552v1</id>
    <title>Large Language Newsvendor: Decision Biases and Cognitive Mechanisms</title>
    <updated>2025-12-14T04:51:53Z</updated>
    <link href="https://arxiv.org/abs/2512.12552v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12552v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Problem definition: Although large language models (LLMs) are increasingly integrated into business decision making, their potential to replicate and even amplify human cognitive biases cautions a significant, yet not well-understood, risk. This is particularly critical in high-stakes operational contexts like supply chain management. To address this, we investigate the decision-making patterns of leading LLMs using the canonical newsvendor problem in a dynamic setting, aiming to identify the nature and origins of their cognitive biases. Methodology/results: Through dynamic, multi-round experiments with GPT-4, GPT-4o, and LLaMA-8B, we tested for five established decision biases. We found that LLMs consistently replicated the classic ``Too Low/Too High'' ordering bias and significantly amplified other tendencies like demand-chasing behavior compared to human benchmarks. Our analysis uncovered a ``paradox of intelligence'': the more sophisticated GPT-4 demonstrated the greatest irrationality through overthinking, while the efficiency-optimized GPT-4o performed near-optimally. Because these biases persist even when optimal formulas are provided, we conclude they stem from architectural constraints rather than knowledge gaps. Managerial implications: First, managers should select models based on the specific task, as our results show that efficiency-optimized models can outperform more complex ones on certain optimization problems. Second, the significant amplification of bias by LLMs highlights the urgent need for robust human-in-the-loop oversight in high-stakes decisions to prevent costly errors. Third, our findings suggest that designing structured, rule-based prompts is a practical and effective strategy for managers to constrain models' heuristic tendencies and improve the reliability of AI-assisted decisions.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T04:51:53Z</published>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Jifei Liu</name>
    </author>
    <author>
      <name>Zhi Chen</name>
    </author>
    <author>
      <name>Yuanguang Zhong</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12551v1</id>
    <title>Assessing the Capability of Android Dynamic Analysis Tools to Combat Anti-Runtime Analysis Techniques</title>
    <updated>2025-12-14T04:43:40Z</updated>
    <link href="https://arxiv.org/abs/2512.12551v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12551v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>As the dominant mobile operating system, Android continues to attract a substantial influx of new applications each year. However, this growth is accompanied by increased attention from malicious actors, resulting in a significant rise in security threats to the Android ecosystem. Among these threats, the adoption of Anti-Runtime Analysis (ARA) techniques by malicious applications poses a serious challenge, as it hinders security professionals from effectively analyzing malicious behaviors using dynamic analysis tools. ARA technologies are designed to prevent the dynamic examination of applications, thus complicating efforts to ensure platform security. This paper presents a comprehensive empirical study that assesses the ability of widely-used Android dynamic analysis tools to bypass various ARA techniques. Our findings reveal a critical gap in the effectiveness of existing dynamic analysis tools to counter ARA mechanisms, highlighting an urgent need for more robust solutions. This work provides valuable insights into the limitations of existing tools and highlights the need for improved methods to counteract ARA technologies, thus advancing the field of software security and dynamic analysis.</summary>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T04:43:40Z</published>
    <arxiv:primary_category term="cs.SE"/>
    <author>
      <name>Dewen Suo</name>
    </author>
    <author>
      <name>Lei Xue</name>
    </author>
    <author>
      <name>Weihao Huang</name>
    </author>
    <author>
      <name>Runze Tan</name>
    </author>
    <author>
      <name>Guozi Sun</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12550v1</id>
    <title>Iterative Sampling Methods for Sinkhorn Distributionally Robust Optimization</title>
    <updated>2025-12-14T04:42:51Z</updated>
    <link href="https://arxiv.org/abs/2512.12550v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12550v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Distributionally robust optimization (DRO) has emerged as a powerful paradigm for reliable decision-making under uncertainty. This paper focuses on DRO with ambiguity sets defined via the Sinkhorn discrepancy: an entropy-regularized Wasserstein distance, referred to as Sinkhorn DRO. Existing work primarily addresses Sinkhorn DRO from a dual perspective, leveraging its formulation as a conditional stochastic optimization problem, for which many stochastic gradient methods are applicable. However, the theoretical analyses of such methods often rely on the boundedness of the loss function, and it is indirect to obtain the worst-case distribution associated with Sinkhorn DRO. In contrast, we study Sinkhorn DRO from the primal perspective, by reformulating it as a bilevel program with several infinite-dimensional lower-level subproblems over probability space. This formulation enables us to simultaneously obtain the optimal robust decision and the worst-case distribution, which is valuable in practical settings, such as generating stress-test scenarios or designing robust learning algorithms. We propose both double-loop and single-loop sampling-based algorithms with theoretical guarantees to solve this bilevel program. Finally, we demonstrate the effectiveness of our approach through a numerical study on adversarial classification.</summary>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T04:42:51Z</published>
    <arxiv:comment>29 pages</arxiv:comment>
    <arxiv:primary_category term="stat.ML"/>
    <author>
      <name>Jie Wang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12549v1</id>
    <title>Supervised Contrastive Frame Aggregation for Video Representation Learning</title>
    <updated>2025-12-14T04:38:40Z</updated>
    <link href="https://arxiv.org/abs/2512.12549v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12549v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We propose a supervised contrastive learning framework for video representation learning that leverages temporally global context. We introduce a video to image aggregation strategy that spatially arranges multiple frames from each video into a single input image. This design enables the use of pre trained convolutional neural network backbones such as ResNet50 and avoids the computational overhead of complex video transformer models. We then design a contrastive learning objective that directly compares pairwise projections generated by the model. Positive pairs are defined as projections from videos sharing the same label while all other projections are treated as negatives. Multiple natural views of the same video are created using different temporal frame samplings from the same underlying video. Rather than relying on data augmentation these frame level variations produce diverse positive samples with global context and reduce overfitting. Experiments on the Penn Action and HMDB51 datasets demonstrate that the proposed method outperforms existing approaches in classification accuracy while requiring fewer computational resources. The proposed Supervised Contrastive Frame Aggregation method learns effective video representations in both supervised and self supervised settings and supports video based tasks such as classification and captioning. The method achieves seventy six percent classification accuracy on Penn Action compared to forty three percent achieved by ViVIT and forty eight percent accuracy on HMDB51 compared to thirty seven percent achieved by ViVIT.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T04:38:40Z</published>
    <arxiv:comment>12 pages</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Shaif Chowdhury</name>
    </author>
    <author>
      <name>Mushfika Rahman</name>
    </author>
    <author>
      <name>Greg Hamerly</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12548v1</id>
    <title>World Models Unlock Optimal Foraging Strategies in Reinforcement Learning Agents</title>
    <updated>2025-12-14T04:36:06Z</updated>
    <link href="https://arxiv.org/abs/2512.12548v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12548v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Patch foraging involves the deliberate and planned process of determining the optimal time to depart from a resource-rich region and investigate potentially more beneficial alternatives. The Marginal Value Theorem (MVT) is frequently used to characterize this process, offering an optimality model for such foraging behaviors. Although this model has been widely used to make predictions in behavioral ecology, discovering the computational mechanisms that facilitate the emergence of optimal patch-foraging decisions in biological foragers remains under investigation. Here, we show that artificial foragers equipped with learned world models naturally converge to MVT-aligned strategies. Using a model-based reinforcement learning agent that acquires a parsimonious predictive representation of its environment, we demonstrate that anticipatory capabilities, rather than reward maximization alone, drive efficient patch-leaving behavior. Compared with standard model-free RL agents, these model-based agents exhibit decision patterns similar to many of their biological counterparts, suggesting that predictive world models can serve as a foundation for more explainable and biologically grounded decision-making in AI systems. Overall, our findings highlight the value of ecological optimality principles for advancing interpretable and adaptive AI.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T04:36:06Z</published>
    <arxiv:comment>14 pages, 6 figures</arxiv:comment>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Yesid Fonseca</name>
    </author>
    <author>
      <name>Manuel S. Ríos</name>
    </author>
    <author>
      <name>Nicanor Quijano</name>
    </author>
    <author>
      <name>Luis F. Giraldo</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12545v1</id>
    <title>Skillful Subseasonal-to-Seasonal Forecasting of Extreme Events with a Multi-Sphere Coupled Probabilistic Model</title>
    <updated>2025-12-14T04:28:51Z</updated>
    <link href="https://arxiv.org/abs/2512.12545v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12545v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Accurate subseasonal-to-seasonal (S2S) prediction of extreme events is critical for resource planning and disaster mitigation under accelerating climate change. However, such predictions remain challenging due to complex multi-sphere interactions and intrinsic atmospheric uncertainty. Here we present TianXing-S2S, a multi-sphere coupled probabilistic model for global S2S daily ensemble forecast. TianXing-S2S first encodes diverse multi-sphere predictors into a compact latent space, then employs a diffusion model to generate daily ensemble forecasts. A novel coupling module based on optimal transport (OT) is incorporated in the denoiser to optimize the interactions between atmospheric and multi-sphere boundary conditions. Across key atmospheric variables, TianXing-S2S outperforms both the European Centre for Medium-Range Weather Forecasts (ECMWF) S2S system and FuXi-S2S in 45-day daily-mean ensemble forecasts at 1.5 resolution. Our model achieves skillful subseasonal prediction of extreme events including heat waves and anomalous precipitation, identifying soil moisture as a critical precursor signal. Furthermore, we demonstrate that TianXing-S2S can generate stable rollout forecasts up to 180 days, establishing a robust framework for S2S research in a warming world.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ao-ph" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T04:28:51Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Bin Mu</name>
    </author>
    <author>
      <name>Yuxuan Chen</name>
    </author>
    <author>
      <name>Shijin Yuan</name>
    </author>
    <author>
      <name>Bo Qin</name>
    </author>
    <author>
      <name>Hao Guo</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12544v1</id>
    <title>HyperEdit: Unlocking Instruction-based Text Editing in LLMs via Hypernetworks</title>
    <updated>2025-12-14T04:28:39Z</updated>
    <link href="https://arxiv.org/abs/2512.12544v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12544v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Instruction-based text editing is increasingly critical for real-world applications such as code editors (e.g., Cursor), but Large Language Models (LLMs) continue to struggle with this task. Unlike free-form generation, editing requires faithfully implementing user instructions while preserving unchanged content, as even minor unintended modifications can break functionality. Existing approaches treat editing as generic text generation, leading to two key failures: they struggle to faithfully align edits with diverse user intents, and they often over-edit unchanged regions. We propose HyperEdit to address both issues. First, we introduce hypernetwork-based dynamic adaptation that generates request-specific parameters, enabling the model to tailor its editing strategy to each instruction. Second, we develop difference-aware regularization that focuses supervision on modified spans, preventing over-editing while ensuring precise, minimal changes. HyperEdit achieves a 9%--30% relative improvement in BLEU on modified regions over state-of-the-art baselines, despite utilizing only 3B parameters.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T04:28:39Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Yiming Zeng</name>
    </author>
    <author>
      <name>Jinghan Cao</name>
    </author>
    <author>
      <name>Zexin Li</name>
    </author>
    <author>
      <name>Wanhao Yu</name>
    </author>
    <author>
      <name>Zhankai Ye</name>
    </author>
    <author>
      <name>Dawei Xiang</name>
    </author>
    <author>
      <name>Ting Hua</name>
    </author>
    <author>
      <name>Xin Liu</name>
    </author>
    <author>
      <name>Shangqian Gao</name>
    </author>
    <author>
      <name>Tingting Yu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12543v1</id>
    <title>Effective Fine-Tuning with Eigenvector Centrality Based Pruning</title>
    <updated>2025-12-14T04:27:50Z</updated>
    <link href="https://arxiv.org/abs/2512.12543v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12543v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>In social media networks a small number of highly influential users can drive large scale changes in discourse across multiple communities. Small shifts in the behavior of these users are often sufficient to propagate widely throughout the network. A similar phenomenon occurs during neural network fine tuning. Conventional fine tuning of convolutional neural networks typically adds a new linear classification layer on top of a large pre trained model. Instead we argue that improved adaptation can be achieved by first pruning the network to retain only the most important neurons and then performing fine tuning.
  We propose a graph theory based method for pruning neural networks that is designed to improve fine tuning performance. In this method each neuron is represented as a node and edges encode similarity between neurons. Neurons are pruned based on importance scores computed using eigenvector centrality. The resulting pruned network is then fine tuned using only the most central neurons. We evaluate the proposed method on VGGNet EfficientNet and ResNet models using the TF Flowers Caltech one zero one and Oxford Flowers one zero two datasets. The proposed approach achieves higher classification accuracy while significantly reducing model complexity. On the Oxford Flowers one zero two dataset the method achieves forty eight percent classification accuracy compared to thirty percent accuracy obtained by the baseline VGGNet model.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T04:27:50Z</published>
    <arxiv:comment>12 pages</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Shaif Chowdhury</name>
    </author>
    <author>
      <name>Soham Biren Katlariwala</name>
    </author>
    <author>
      <name>Devleena Kashyap</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12539v1</id>
    <title>Anatomy Guided Coronary Artery Segmentation from CCTA Using Spatial Frequency Joint Modeling</title>
    <updated>2025-12-14T04:12:40Z</updated>
    <link href="https://arxiv.org/abs/2512.12539v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12539v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Accurate coronary artery segmentation from coronary computed tomography angiography is essential for quantitative coronary analysis and clinical decision support. Nevertheless, reliable segmentation remains challenging because of small vessel calibers, complex branching, blurred boundaries, and myocardial interference. We propose a coronary artery segmentation framework that integrates myocardial anatomical priors, structure aware feature encoding, and three dimensional wavelet inverse wavelet transformations. Myocardial priors and residual attention based feature enhancement are incorporated during encoding to strengthen coronary structure representation. Wavelet inverse wavelet based downsampling and upsampling enable joint spatial frequency modeling and preserve multi scale structural consistency, while a multi scale feature fusion module integrates semantic and geometric information in the decoding stage. The model is trained and evaluated on the public ImageCAS dataset using a 3D overlapping patch based strategy with a 7:1:2 split for training, validation, and testing. Experimental results demonstrate that the proposed method achieves a Dice coefficient of 0.8082, Sensitivity of 0.7946, Precision of 0.8471, and an HD95 of 9.77 mm, outperforming several mainstream segmentation models. Ablation studies further confirm the complementary contributions of individual components. The proposed method enables more stable and consistent coronary artery segmentation under complex geometric conditions, providing reliable segmentation results for subsequent coronary structure analysis tasks.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T04:12:40Z</published>
    <arxiv:comment>6 figures</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Huan Huang</name>
    </author>
    <author>
      <name>Michele Esposito</name>
    </author>
    <author>
      <name>Chen Zhao</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12537v1</id>
    <title>NagaNLP: Bootstrapping NLP for Low-Resource Nagamese Creole with Human-in-the-Loop Synthetic Data</title>
    <updated>2025-12-14T04:08:26Z</updated>
    <link href="https://arxiv.org/abs/2512.12537v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12537v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The vast majority of the world's languages, particularly creoles like Nagamese, remain severely under-resourced in Natural Language Processing (NLP), creating a significant barrier to their representation in digital technology. This paper introduces NagaNLP, a comprehensive open-source toolkit for Nagamese, bootstrapped through a novel methodology that relies on LLM-driven but human-validated synthetic data generation. We detail a multi-stage pipeline where an expert-guided LLM (Gemini) generates a candidate corpus, which is then refined and annotated by native speakers. This synthetic-hybrid approach yielded a 10K pair conversational dataset and a high-quality annotated corpus for foundational tasks. To assess the effectiveness of our methodology, we trained both discriminative and generative models. Our fine-tuned XLM-RoBERTa-base model establishes a new benchmark for Nagamese, achieving a 93.81\% accuracy (0.90 F1-Macro) on Part-of-Speech tagging and a 0.75 F1-Macro on Named Entity Recognition, massively outperforming strong zero-shot baselines. Furthermore, we fine-tuned a Llama-3.2-3B Instruct model, named NagaLLaMA, which demonstrates superior performance on conversational tasks, achieving a Perplexity of 3.85, an order of magnitude improvement over its few-shot counterpart (96.76). We release the NagaNLP toolkit, including all datasets, models, and code, providing a foundational resource for a previously underserved language and a reproducible framework for reducing data scarcity in other low-resource contexts.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T04:08:26Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Agniva Maiti</name>
    </author>
    <author>
      <name>Manya Pandey</name>
    </author>
    <author>
      <name>Murari Mandal</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12536v1</id>
    <title>Diverse LLMs vs. Vulnerabilities: Who Detects and Fixes Them Better?</title>
    <updated>2025-12-14T03:47:39Z</updated>
    <link href="https://arxiv.org/abs/2512.12536v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12536v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Large Language Models (LLMs) are increasingly being studied for Software Vulnerability Detection (SVD) and Repair (SVR). Individual LLMs have demonstrated code understanding abilities, but they frequently struggle when identifying complex vulnerabilities and generating fixes.
  This study presents DVDR-LLM, an ensemble framework that combines outputs from diverse LLMs to determine whether aggregating multiple models reduces error rates. Our evaluation reveals that DVDR-LLM achieves 10-12% higher detection accuracy compared to the average performance of individual models, with benefits increasing as code complexity grows. For multi-file vulnerabilities, the ensemble approach demonstrates significant improvements in recall (+18%) and F1 score (+11.8%) over individual models. However, the approach raises measurable trade-offs: reducing false positives in verification tasks while simultaneously increasing false negatives in detection tasks, requiring careful decision on the required level of agreement among the LLMs (threshold) for increased performance across different security contexts.
  Artifact: https://github.com/Erroristotle/DVDR_LLM</summary>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T03:47:39Z</published>
    <arxiv:primary_category term="cs.SE"/>
    <author>
      <name>Arastoo Zibaeirad</name>
    </author>
    <author>
      <name>Marco Vieira</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12534v1</id>
    <title>Animus3D: Text-driven 3D Animation via Motion Score Distillation</title>
    <updated>2025-12-14T03:22:34Z</updated>
    <link href="https://arxiv.org/abs/2512.12534v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12534v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We present Animus3D, a text-driven 3D animation framework that generates motion field given a static 3D asset and text prompt. Previous methods mostly leverage the vanilla Score Distillation Sampling (SDS) objective to distill motion from pretrained text-to-video diffusion, leading to animations with minimal movement or noticeable jitter. To address this, our approach introduces a novel SDS alternative, Motion Score Distillation (MSD). Specifically, we introduce a LoRA-enhanced video diffusion model that defines a static source distribution rather than pure noise as in SDS, while another inversion-based noise estimation technique ensures appearance preservation when guiding motion. To further improve motion fidelity, we incorporate explicit temporal and spatial regularization terms that mitigate geometric distortions across time and space. Additionally, we propose a motion refinement module to upscale the temporal resolution and enhance fine-grained details, overcoming the fixed-resolution constraints of the underlying video model. Extensive experiments demonstrate that Animus3D successfully animates static 3D assets from diverse text prompts, generating significantly more substantial and detailed motion than state-of-the-art baselines while maintaining high visual integrity. Code will be released at https://qiisun.github.io/animus3d_page.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T03:22:34Z</published>
    <arxiv:comment>SIGGRAPH Asia 2025</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Qi Sun</name>
    </author>
    <author>
      <name>Can Wang</name>
    </author>
    <author>
      <name>Jiaxiang Shang</name>
    </author>
    <author>
      <name>Wensen Feng</name>
    </author>
    <author>
      <name>Jing Liao</name>
    </author>
    <arxiv:doi>10.1145/3757377.3763916</arxiv:doi>
    <link rel="related" href="https://doi.org/10.1145/3757377.3763916" title="doi"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12532v1</id>
    <title>Strategic Server Deployment under Uncertainty in Mobile Edge Computing</title>
    <updated>2025-12-14T03:05:30Z</updated>
    <link href="https://arxiv.org/abs/2512.12532v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12532v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Server deployment is a fundamental task in mobile edge computing: where to place the edge servers and what user cells to assign to them. To make this decision is context-specific, but common goals are 1) computing efficiency: maximize the amount of workload processed by the edge, and 2) communication efficiency: minimize the communication cost between the cells and their assigned servers. We focus on practical scenarios where the user workload in each cell is unknown and time-varying, and so are the effective capacities of the servers. Our research problem is to choose a subset of candidate servers and assign them to the user cells such that the above goals are sustainably achieved under the above uncertainties. We formulate this problem as a stochastic bilevel optimization, which is strongly NP-hard and unseen in the literature. By approximating the objective function with submodular functions, we can utilize state-of-the-art greedy algorithms for submodular maximization to effectively solve our problem. We evaluate the proposed algorithm using real-world data, showing its superiority to alternative methods; the improvement can be as high as 55%</summary>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T03:05:30Z</published>
    <arxiv:primary_category term="cs.DC"/>
    <arxiv:journal_ref>International Journal of Parallel, Emergent and Distributed Systems 2025</arxiv:journal_ref>
    <author>
      <name>Duc A. Tran</name>
    </author>
    <author>
      <name>Dung Truong</name>
    </author>
    <author>
      <name>Duy Le</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12526v1</id>
    <title>Empirical Mode Decomposition and Graph Transformation of the MSCI World Index: A Multiscale Topological Analysis for Graph Neural Network Modeling</title>
    <updated>2025-12-14T02:35:38Z</updated>
    <link href="https://arxiv.org/abs/2512.12526v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12526v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>This study applies Empirical Mode Decomposition (EMD) to the MSCI World index and converts the resulting intrinsic mode functions (IMFs) into graph representations to enable modeling with graph neural networks (GNNs). Using CEEMDAN, we extract nine IMFs spanning high-frequency fluctuations to long-term trends. Each IMF is transformed into a graph using four time-series-to-graph methods: natural visibility, horizontal visibility, recurrence, and transition graphs. Topological analysis shows clear scale-dependent structure: high-frequency IMFs yield dense, highly connected small-world graphs, whereas low-frequency IMFs produce sparser networks with longer characteristic path lengths. Visibility-based methods are more sensitive to amplitude variability and typically generate higher clustering, while recurrence graphs better preserve temporal dependencies. These results provide guidance for designing GNN architectures tailored to the structural properties of decomposed components, supporting more effective predictive modeling of financial time series.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.CP" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T02:35:38Z</published>
    <arxiv:comment>19 pages, 3 figures, 6 tables</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Agustín M. de los Riscos</name>
    </author>
    <author>
      <name>Julio E. Sandubete</name>
    </author>
    <author>
      <name>Diego Carmona-Fernández</name>
    </author>
    <author>
      <name>León Beleña</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12523v1</id>
    <title>Noise-robust Contrastive Learning for Critical Transition Detection in Dynamical Systems</title>
    <updated>2025-12-14T02:28:54Z</updated>
    <link href="https://arxiv.org/abs/2512.12523v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12523v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Detecting critical transitions in complex, noisy time-series data is a fundamental challenge across science and engineering. Such transitions may be anticipated by the emergence of a low-dimensional order parameter, whose signature is often masked by high-amplitude stochastic variability. Standard contrastive learning approaches based on deep neural networks, while promising for detecting critical transitions, are often overparameterized and sensitive to irrelevant noise, leading to inaccurate identification of critical points. To address these limitations, we propose a neural network architecture, constructed using singular value decomposition technique, together with a strictly semi-orthogonality-constrained training algorithm, to enhance the performance of traditional contrastive learning. Extensive experiments demonstrate that the proposed method matches the performance of traditional contrastive learning techniques in identifying critical transitions, yet is considerably more lightweight and markedly more resistant to noise.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T02:28:54Z</published>
    <arxiv:comment>under revision</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Wenqi Fang</name>
    </author>
    <author>
      <name>Ye Li</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12510v1</id>
    <title>Can You Keep a Secret? Exploring AI for Care Coordination in Cognitive Decline</title>
    <updated>2025-12-14T01:26:55Z</updated>
    <link href="https://arxiv.org/abs/2512.12510v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12510v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The increasing number of older adults who experience cognitive decline places a burden on informal caregivers, whose support with tasks of daily living determines whether older adults can remain in their homes. To explore how agents might help lower-SES older adults to age-in-place, we interviewed ten pairs of older adults experiencing cognitive decline and their informal caregivers. We explored how they coordinate care, manage burdens, and sustain autonomy and privacy. Older adults exercised control by delegating tasks to specific caregivers, keeping information about all the care they received from their adult children. Many abandoned some tasks of daily living, lowering their quality of life to ease caregiver burden. One effective strategy, piggybacking, uses spontaneous overlaps in errands to get more work done with less caregiver effort. This raises the questions: (i) Can agents help with piggyback coordination? (ii) Would it keep older adults in their homes longer, while not increasing caregiver burden?</summary>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T01:26:55Z</published>
    <arxiv:comment>13 pages, 6 figures</arxiv:comment>
    <arxiv:primary_category term="cs.HC"/>
    <author>
      <name> Alicia</name>
      <arxiv:affiliation>Hyun Jin</arxiv:affiliation>
    </author>
    <author>
      <name> Lee</name>
    </author>
    <author>
      <name>Mai Lee Chang</name>
    </author>
    <author>
      <name>Sreehana Mandava</name>
    </author>
    <author>
      <name>Destiny Deshields</name>
    </author>
    <author>
      <name>Hugo Simão</name>
    </author>
    <author>
      <name>Aaron Steinfeld</name>
    </author>
    <author>
      <name>Jodi Forlizzi</name>
    </author>
    <author>
      <name>John Zimmerman</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12508v1</id>
    <title>Generative Spatiotemporal Data Augmentation</title>
    <updated>2025-12-14T01:18:48Z</updated>
    <link href="https://arxiv.org/abs/2512.12508v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12508v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We explore spatiotemporal data augmentation using video foundation models to diversify both camera viewpoints and scene dynamics. Unlike existing approaches based on simple geometric transforms or appearance perturbations, our method leverages off-the-shelf video diffusion models to generate realistic 3D spatial and temporal variations from a given image dataset. Incorporating these synthesized video clips as supplemental training data yields consistent performance gains in low-data settings, such as UAV-captured imagery where annotations are scarce. Beyond empirical improvements, we provide practical guidelines for (i) choosing an appropriate spatiotemporal generative setup, (ii) transferring annotations to synthetic frames, and (iii) addressing disocclusion - regions newly revealed and unlabeled in generated views. Experiments on COCO subsets and UAV-captured datasets show that, when applied judiciously, spatiotemporal augmentation broadens the data distribution along axes underrepresented by traditional and prior generative methods, offering an effective lever for improving model performance in data-scarce regimes.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T01:18:48Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Jinfan Zhou</name>
    </author>
    <author>
      <name>Lixin Luo</name>
    </author>
    <author>
      <name>Sungmin Eum</name>
    </author>
    <author>
      <name>Heesung Kwon</name>
    </author>
    <author>
      <name>Jeong Joon Park</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12507v1</id>
    <title>ATLAS: Automated Tree-based Language Analysis System for C and C++ source programs</title>
    <updated>2025-12-14T01:11:11Z</updated>
    <link href="https://arxiv.org/abs/2512.12507v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12507v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The growing complexity of modern software systems has highlighted the shortcomings of traditional programming analysis techniques, particularly for Software Engineering (SE) tasks. While machine learning and Large Language Models (LLMs) offer promising solutions, their effectiveness is limited by the way they interpret data. Unlike natural language, source code meaning is defined less by token adjacency and more by complex, long-range, and structural relationships and dependencies. This limitation is especially pronounced for C and C++, where flatter syntactic hierarchies, pointer aliasing, multi-level indirection, typedef-based type obfuscation, and function-pointer calls hinder accurate static analysis. To address these challenges, this paper introduces ATLAS, a Python-based Command-Line Interface (CLI) that (i) generates statement-level Control Flow Graphs (CFG) and type-aware Data Flow Graphs (DFG) that capture inter-functional dependencies for the entire program; (ii) has the ability to work on entire C and C++ projects comprising multiple files; (iii) works on both compilable and non-compilable code and (iv) produces a unified multi-view code representation using Abstract Syntax Trees (AST), CFG and DFG. By preserving essential structural and semantic information, ATLAS provides a practical foundation for improving downstream SE and machine-learning-based program understanding. Video demonstration: https://youtu.be/RACWQe5ELwY Tool repository: https://github.com/jaid-monwar/ATLAS-code-representation-tool</summary>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T01:11:11Z</published>
    <arxiv:comment>5 pages, 2 figures; Video demonstration: https://youtu.be/RACWQe5ELwY; Tool repository: https://github.com/jaid-monwar/ATLAS-code-representation-tool</arxiv:comment>
    <arxiv:primary_category term="cs.SE"/>
    <author>
      <name>Jaid Monwar Chowdhury</name>
    </author>
    <author>
      <name>Ahmad Farhan Shahriar Chowdhury</name>
    </author>
    <author>
      <name>Humayra Binte Monwar</name>
    </author>
    <author>
      <name>Mahmuda Naznin</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12506v1</id>
    <title>Explainable Artificial Intelligence for Economic Time Series: A Comprehensive Review and a Systematic Taxonomy of Methods and Concepts</title>
    <updated>2025-12-14T00:45:30Z</updated>
    <link href="https://arxiv.org/abs/2512.12506v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12506v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Explainable Artificial Intelligence (XAI) is increasingly required in computational economics, where machine-learning forecasters can outperform classical econometric models but remain difficult to audit and use for policy. This survey reviews and organizes the growing literature on XAI for economic time series, where autocorrelation, non-stationarity, seasonality, mixed frequencies, and regime shifts can make standard explanation techniques unreliable or economically implausible. We propose a taxonomy that classifies methods by (i) explanation mechanism: propagation-based approaches (e.g., Integrated Gradients, Layer-wise Relevance Propagation), perturbation and game-theoretic attribution (e.g., permutation importance, LIME, SHAP), and function-based global tools (e.g., Accumulated Local Effects); (ii) time-series compatibility, including preservation of temporal dependence, stability over time, and respect for data-generating constraints. We synthesize time-series-specific adaptations such as vector- and window-based formulations (e.g., Vector SHAP, WindowSHAP) that reduce lag fragmentation and computational cost while improving interpretability. We also connect explainability to causal inference and policy analysis through interventional attributions (Causal Shapley values) and constrained counterfactual reasoning. Finally, we discuss intrinsically interpretable architectures (notably attention-based transformers) and provide guidance for decision-grade applications such as nowcasting, stress testing, and regime monitoring, emphasizing attribution uncertainty and explanation dynamics as indicators of structural change.</summary>
    <category term="econ.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T00:45:30Z</published>
    <arxiv:comment>11 pages, 1 table</arxiv:comment>
    <arxiv:primary_category term="econ.GN"/>
    <author>
      <name>Agustín García-García</name>
    </author>
    <author>
      <name>Pablo Hidalgo</name>
    </author>
    <author>
      <name>Julio E. Sandubete</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12503v1</id>
    <title>KidsArtBench: Multi-Dimensional Children's Art Evaluation with Attribute-Aware MLLMs</title>
    <updated>2025-12-14T00:24:48Z</updated>
    <link href="https://arxiv.org/abs/2512.12503v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12503v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Multimodal Large Language Models (MLLMs) show remarkable progress across many visual-language tasks; however, their capacity to evaluate artistic expression remains limited. Aesthetic concepts are inherently abstract and open-ended, and multimodal artwork annotations are scarce. We introduce KidsArtBench, a new benchmark of over 1k children's artworks (ages 5-15) annotated by 12 expert educators across 9 rubric-aligned dimensions, together with expert comments for feedback. Unlike prior aesthetic datasets that provide single scalar scores on adult imagery, KidsArtBench targets children's artwork and pairs multi-dimensional annotations with comment supervision to enable both ordinal assessment and formative feedback. Building on this resource, we propose an attribute-specific multi-LoRA approach, where each attribute corresponds to a distinct evaluation dimension (e.g., Realism, Imagination) in the scoring rubric, with Regression-Aware Fine-Tuning (RAFT) to align predictions with ordinal scales. On Qwen2.5-VL-7B, our method increases correlation from 0.468 to 0.653, with the largest gains on perceptual dimensions and narrowed gaps on higher-order attributes. These results show that educator-aligned supervision and attribute-aware training yield pedagogically meaningful evaluations and establish a rigorous testbed for sustained progress in educational AI. We release data and code with ethics documentation.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T00:24:48Z</published>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Mingrui Ye</name>
    </author>
    <author>
      <name>Chanjin Zheng</name>
    </author>
    <author>
      <name>Zengyi Yu</name>
    </author>
    <author>
      <name>Chenyu Xiang</name>
    </author>
    <author>
      <name>Zhixue Zhao</name>
    </author>
    <author>
      <name>Zheng Yuan</name>
    </author>
    <author>
      <name>Helen Yannakoudakis</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12501v1</id>
    <title>SafeGen: Embedding Ethical Safeguards in Text-to-Image Generation</title>
    <updated>2025-12-14T00:18:10Z</updated>
    <link href="https://arxiv.org/abs/2512.12501v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12501v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Generative Artificial Intelligence (AI) has created unprecedented opportunities for creative expression, education, and research. Text-to-image systems such as DALL.E, Stable Diffusion, and Midjourney can now convert ideas into visuals within seconds, but they also present a dual-use dilemma, raising critical ethical concerns: amplifying societal biases, producing high-fidelity disinformation, and violating intellectual property. This paper introduces SafeGen, a framework that embeds ethical safeguards directly into the text-to-image generation pipeline, grounding its design in established principles for Trustworthy AI. SafeGen integrates two complementary components: BGE-M3, a fine-tuned text classifier that filters harmful or misleading prompts, and Hyper-SD, an optimized diffusion model that produces high fidelity, semantically aligned images. Built on a curated multilingual (English- Vietnamese) dataset and a fairness-aware training process, SafeGen demonstrates that creative freedom and ethical responsibility can be reconciled within a single workflow. Quantitative evaluations confirm its effectiveness, with Hyper-SD achieving IS = 3.52, FID = 22.08, and SSIM = 0.79, while BGE-M3 reaches an F1-Score of 0.81. An ablation study further validates the importance of domain-specific fine-tuning for both modules. Case studies illustrate SafeGen's practical impact in blocking unsafe prompts, generating inclusive teaching materials, and reinforcing academic integrity.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T00:18:10Z</published>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Dang Phuong Nam</name>
    </author>
    <author>
      <name>Nguyen Kieu</name>
    </author>
    <author>
      <name>Pham Thanh Hieu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12500v1</id>
    <title>Explainable AI as a Double-Edged Sword in Dermatology: The Impact on Clinicians versus The Public</title>
    <updated>2025-12-14T00:06:06Z</updated>
    <link href="https://arxiv.org/abs/2512.12500v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12500v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Artificial intelligence (AI) is increasingly permeating healthcare, from physician assistants to consumer applications. Since AI algorithm's opacity challenges human interaction, explainable AI (XAI) addresses this by providing AI decision-making insight, but evidence suggests XAI can paradoxically induce over-reliance or bias. We present results from two large-scale experiments (623 lay people; 153 primary care physicians, PCPs) combining a fairness-based diagnosis AI model and different XAI explanations to examine how XAI assistance, particularly multimodal large language models (LLMs), influences diagnostic performance. AI assistance balanced across skin tones improved accuracy and reduced diagnostic disparities. However, LLM explanations yielded divergent effects: lay users showed higher automation bias - accuracy boosted when AI was correct, reduced when AI erred - while experienced PCPs remained resilient, benefiting irrespective of AI accuracy. Presenting AI suggestions first also led to worse outcomes when the AI was incorrect for both groups. These findings highlight XAI's varying impact based on expertise and timing, underscoring LLMs as a "double-edged sword" in medical AI and informing future human-AI collaborative system design.</summary>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T00:06:06Z</published>
    <arxiv:primary_category term="cs.HC"/>
    <author>
      <name>Xuhai Xu</name>
    </author>
    <author>
      <name>Haoyu Hu</name>
    </author>
    <author>
      <name>Haoran Zhang</name>
    </author>
    <author>
      <name>Will Ke Wang</name>
    </author>
    <author>
      <name>Reina Wang</name>
    </author>
    <author>
      <name>Luis R. Soenksen</name>
    </author>
    <author>
      <name>Omar Badri</name>
    </author>
    <author>
      <name>Sheharbano Jafry</name>
    </author>
    <author>
      <name>Elise Burger</name>
    </author>
    <author>
      <name>Lotanna Nwandu</name>
    </author>
    <author>
      <name>Apoorva Mehta</name>
    </author>
    <author>
      <name>Erik P. Duhaime</name>
    </author>
    <author>
      <name>Asif Qasim</name>
    </author>
    <author>
      <name>Hause Lin</name>
    </author>
    <author>
      <name>Janis Pereira</name>
    </author>
    <author>
      <name>Jonathan Hershon</name>
    </author>
    <author>
      <name>Paulius Mui</name>
    </author>
    <author>
      <name>Alejandro A. Gru</name>
    </author>
    <author>
      <name>Noémie Elhadad</name>
    </author>
    <author>
      <name>Lena Mamykina</name>
    </author>
    <author>
      <name>Matthew Groh</name>
    </author>
    <author>
      <name>Philipp Tschandl</name>
    </author>
    <author>
      <name>Roxana Daneshjou</name>
    </author>
    <author>
      <name>Marzyeh Ghassemi</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12498v1</id>
    <title>Advancing Cache-Based Few-Shot Classification via Patch-Driven Relational Gated Graph Attention</title>
    <updated>2025-12-13T23:53:00Z</updated>
    <link href="https://arxiv.org/abs/2512.12498v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12498v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Few-shot image classification remains difficult under limited supervision and visual domain shift. Recent cache-based adaptation approaches (e.g., Tip-Adapter) address this challenge to some extent by learning lightweight residual adapters over frozen features, yet they still inherit CLIP's tendency to encode global, general-purpose representations that are not optimally discriminative to adapt the generalist to the specialist's domain in low-data regimes. We address this limitation with a novel patch-driven relational refinement that learns cache adapter weights from intra-image patch dependencies rather than treating an image embedding as a monolithic vector. Specifically, we introduce a relational gated graph attention network that constructs a patch graph and performs edge-aware attention to emphasize informative inter-patch interactions, producing context-enriched patch embeddings. A learnable multi-aggregation pooling then composes these into compact, task-discriminative representations that better align cache keys with the target few-shot classes. Crucially, the proposed graph refinement is used only during training to distil relational structure into the cache, incurring no additional inference cost beyond standard cache lookup. Final predictions are obtained by a residual fusion of cache similarity scores with CLIP zero-shot logits. Extensive evaluations on 11 benchmarks show consistent gains over state-of-the-art CLIP adapter and cache-based baselines while preserving zero-shot efficiency. We further validate battlefield relevance by introducing an Injured vs. Uninjured Soldier dataset for casualty recognition. It is motivated by the operational need to support triage decisions within the "platinum minutes" and the broader "golden hour" window in time-critical UAV-driven search-and-rescue and combat casualty care.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T23:53:00Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Tasweer Ahmad</name>
    </author>
    <author>
      <name>Arindam Sikdar</name>
    </author>
    <author>
      <name>Sandip Pradhan</name>
    </author>
    <author>
      <name>Ardhendu Behera</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12497v1</id>
    <title>Policy Optimization for Dynamic Heart Transplant Allocation</title>
    <updated>2025-12-13T23:51:31Z</updated>
    <link href="https://arxiv.org/abs/2512.12497v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12497v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Heart transplantation is a viable path for patients suffering from advanced heart failure, but this lifesaving option is severely limited due to donor shortage. Although the current allocation policy was recently revised in 2018, a major concern is that it does not adequately take into account pretransplant and post-transplant mortality. In this paper, we take an important step toward addressing these deficiencies.
  To begin with, we use historical data from UNOS to develop a new simulator that enables us to evaluate and compare the performance of different policies. We then leverage our simulator to demonstrate that the status quo policy is considerably inferior to the myopic policy that matches incoming donors to the patient who maximizes the number of years gained by the transplant. Moreover, we develop improved policies that account for the dynamic nature of the allocation process through the use of potentials -- a measure of a patient's utility in future allocations that we introduce. We also show that batching together even a handful of donors -- which is a viable option for a certain type of donors -- further enhances performance. Our simulator also allows us to evaluate the effect of critical, and often unexplored, factors in the allocation, such as geographic proximity and the tendency to reject offers by the transplant centers.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T23:51:31Z</published>
    <arxiv:comment>An extended abstract of this paper was presented at the scientific sessions of AHA 2025</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Ioannis Anagnostides</name>
    </author>
    <author>
      <name>Zachary W. Sollie</name>
    </author>
    <author>
      <name>Arman Kilic</name>
    </author>
    <author>
      <name>Tuomas Sandholm</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12493v1</id>
    <title>AI-Driven Early Warning Systems for Student Success: Discovering Static Feature Dominance in Temporal Prediction Models</title>
    <updated>2025-12-13T23:38:52Z</updated>
    <link href="https://arxiv.org/abs/2512.12493v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12493v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Early identification of at-risk students is critical for effective intervention in online learning environments. This study extends temporal prediction analysis to Week 20 (50% of course duration), comparing Decision Tree and Long Short- Term Memory (LSTM) models across six temporal snapshots. Our analysis reveals that different performance metrics matter at different intervention stages: high recall is critical for early intervention (Weeks 2-4), while balanced precision-recall is important for mid-course resource allocation (Weeks 8-16), and high precision becomes paramount in later stages (Week 20). We demonstrate that static demographic features dominate predictions (68% importance), enabling assessment-free early prediction. The LSTM model achieves 97% recall at Week 2, making it ideal for early intervention, while Decision Tree provides stable balanced performance (78% accuracy) during mid-course. By Week 20, both models converge to similar recall (68%), but LSTM achieves higher precision (90% vs 86%). Our findings also suggest that model selection should depend on intervention timing, and that early signals (Weeks 2-4) are sufficient for reliable initial prediction using primarily demographic and pre-enrollment information.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T23:38:52Z</published>
    <arxiv:comment>5 pages, 3 figures, KDD 2026</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Vaarunay Kaushal</name>
    </author>
    <author>
      <name>Rajib Mall</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12492v1</id>
    <title>Adaptive Detector-Verifier Framework for Zero-Shot Polyp Detection in Open-World Settings</title>
    <updated>2025-12-13T23:33:05Z</updated>
    <link href="https://arxiv.org/abs/2512.12492v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12492v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Polyp detectors trained on clean datasets often underperform in real-world endoscopy, where illumination changes, motion blur, and occlusions degrade image quality. Existing approaches struggle with the domain gap between controlled laboratory conditions and clinical practice, where adverse imaging conditions are prevalent. In this work, we propose AdaptiveDetector, a novel two-stage detector-verifier framework comprising a YOLOv11 detector with a vision-language model (VLM) verifier. The detector adaptively adjusts per-frame confidence thresholds under VLM guidance, while the verifier is fine-tuned with Group Relative Policy Optimization (GRPO) using an asymmetric, cost-sensitive reward function specifically designed to discourage missed detections -- a critical clinical requirement. To enable realistic assessment under challenging conditions, we construct a comprehensive synthetic testbed by systematically degrading clean datasets with adverse conditions commonly encountered in clinical practice, providing a rigorous benchmark for zero-shot evaluation. Extensive zero-shot evaluation on synthetically degraded CVC-ClinicDB and Kvasir-SEG images demonstrates that our approach improves recall by 14 to 22 percentage points over YOLO alone, while precision remains within 0.7 points below to 1.7 points above the baseline. This combination of adaptive thresholding and cost-sensitive reinforcement learning achieves clinically aligned, open-world polyp detection with substantially fewer false negatives, thereby reducing the risk of missed precancerous polyps and improving patient outcomes.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T23:33:05Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Shengkai Xu</name>
    </author>
    <author>
      <name>Hsiang Lun Kao</name>
    </author>
    <author>
      <name>Tianxiang Xu</name>
    </author>
    <author>
      <name>Honghui Zhang</name>
    </author>
    <author>
      <name>Junqiao Wang</name>
    </author>
    <author>
      <name>Runmeng Ding</name>
    </author>
    <author>
      <name>Guanyu Liu</name>
    </author>
    <author>
      <name>Tianyu Shi</name>
    </author>
    <author>
      <name>Zhenyu Yu</name>
    </author>
    <author>
      <name>Guofeng Pan</name>
    </author>
    <author>
      <name>Ziqian Bi</name>
    </author>
    <author>
      <name>Yuqi Ouyang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12489v1</id>
    <title>GoMS: Graph of Molecule Substructure Network for Molecule Property Prediction</title>
    <updated>2025-12-13T23:14:25Z</updated>
    <link href="https://arxiv.org/abs/2512.12489v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12489v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>While graph neural networks have shown remarkable success in molecular property prediction, current approaches like the Equivariant Subgraph Aggregation Networks (ESAN) treat molecules as bags of independent substructures, overlooking crucial relationships between these components. We present Graph of Molecule Substructures (GoMS), a novel architecture that explicitly models the interactions and spatial arrangements between molecular substructures. Unlike ESAN's bag-based representation, GoMS constructs a graph where nodes represent subgraphs and edges capture their structural relationships, preserving critical topological information about how substructures are connected and overlap within the molecule. Through extensive experiments on public molecular datasets, we demonstrate that GoMS outperforms ESAN and other baseline methods, with particularly improvements for large molecules containing more than 100 atoms. The performance gap widens as molecular size increases, demonstrating GoMS's effectiveness for modeling industrial-scale molecules. Our theoretical analysis demonstrates that GoMS can distinguish molecules with identical subgraph compositions but different spatial arrangements. Our approach shows particular promise for materials science applications involving complex molecules where properties emerge from the interplay between multiple functional units. By capturing substructure relationships that are lost in bag-based approaches, GoMS represents a significant advance toward scalable and interpretable molecular property prediction for real-world applications.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T23:14:25Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Shuhui Qu</name>
    </author>
    <author>
      <name>Cheolwoo Park</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12488v1</id>
    <title>The American Ghost in the Machine: How language models align culturally and the effects of cultural prompting</title>
    <updated>2025-12-13T23:11:41Z</updated>
    <link href="https://arxiv.org/abs/2512.12488v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12488v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Culture is the bedrock of human interaction; it dictates how we perceive and respond to everyday interactions. As the field of human-computer interaction grows via the rise of generative Large Language Models (LLMs), the cultural alignment of these models become an important field of study. This work, using the VSM13 International Survey and Hofstede's cultural dimensions, identifies the cultural alignment of popular LLMs (DeepSeek-V3, V3.1, GPT-5, GPT-4.1, GPT-4, Claude Opus 4, Llama 3.1, and Mistral Large). We then use cultural prompting, or using system prompts to shift the cultural alignment of a model to a desired country, to test the adaptability of these models to other cultures, namely China, France, India, Iran, Japan, and the United States. We find that the majority of the eight LLMs tested favor the United States when the culture is not specified, with varying results when prompted for other cultures. When using cultural prompting, seven of the eight models shifted closer to the expected culture. We find that models had trouble aligning with Japan and China, despite two of the models tested originating with the Chinese company DeepSeek.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T23:11:41Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>James Luther</name>
    </author>
    <author>
      <name>Donald Brown</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12487v1</id>
    <title>More Than the Final Answer: Improving Visual Extraction and Logical Consistency in Vision-Language Models</title>
    <updated>2025-12-13T23:06:18Z</updated>
    <link href="https://arxiv.org/abs/2512.12487v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12487v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Reinforcement learning from verifiable rewards (RLVR) has recently been extended from text-only LLMs to vision-language models (VLMs) to elicit long-chain multimodal reasoning. However, RLVR-trained VLMs still exhibit two persistent failure modes: inaccurate visual extraction (missing or hallucinating details) and logically inconsistent chains-of-thought, largely because verifiable signals supervise only the final answer. We propose PeRL-VL (Perception and Reasoning Learning for Vision-Language Models), a decoupled framework that separately improves visual perception and textual reasoning on top of RLVR. For perception, PeRL-VL introduces a VLM-based description reward that scores the model's self-generated image descriptions for faithfulness and sufficiency. For reasoning, PeRL-VL adds a text-only Reasoning SFT stage on logic-rich chain-of-thought data, enhancing coherence and logical consistency independently of vision. Across diverse multimodal benchmarks, PeRL-VL improves average Pass@1 accuracy from 63.3% (base Qwen2.5-VL-7B) to 68.8%, outperforming standard RLVR, text-only reasoning SFT, and naive multimodal distillation from GPT-4o.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T23:06:18Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Hoang Anh Just</name>
    </author>
    <author>
      <name>Yifei Fan</name>
    </author>
    <author>
      <name>Handong Zhao</name>
    </author>
    <author>
      <name>Jiuxiang Gu</name>
    </author>
    <author>
      <name>Ruiyi Zhang</name>
    </author>
    <author>
      <name>Simon Jenni</name>
    </author>
    <author>
      <name>Kushal Kafle</name>
    </author>
    <author>
      <name>Ruoxi Jia</name>
    </author>
    <author>
      <name>Jing Shi</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12483v1</id>
    <title>Mage: Cracking Elliptic Curve Cryptography with Cross-Axis Transformers</title>
    <updated>2025-12-13T22:45:35Z</updated>
    <link href="https://arxiv.org/abs/2512.12483v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12483v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>With the advent of machine learning and quantum computing, the 21st century has gone from a place of relative algorithmic security, to one of speculative unease and possibly, cyber catastrophe.
  Modern algorithms like Elliptic Curve Cryptography (ECC) are the bastion of current cryptographic security protocols that form the backbone of consumer protection ranging from Hypertext Transfer Protocol Secure (HTTPS) in the modern internet browser, to cryptographic financial instruments like Bitcoin.
  And there's been very little work put into testing the strength of these ciphers. Practically the only study that I could find was on side-channel recognition, a joint paper from the University of Milan, Italy and King's College, London\cite{battistello2025ecc}.
  These algorithms are already considered bulletproof by many consumers, but exploits already exist for them, and with computing power and distributed, federated compute on the rise, it's only a matter of time before these current bastions fade away into obscurity, and it's on all of us to stand up when we notice something is amiss, lest we see such passages claim victims in that process.
  In this paper, we seek to explore the use of modern language model architecture in cracking the association between a known public key, and its associated private key, by intuitively learning to reverse engineer the public keypair generation process, effectively solving the curve.
  Additonally, we attempt to ascertain modern machine learning's ability to memorize public-private secp256r1 keypairs, and to then test their ability to reverse engineer the public keypair generation process.
  It is my belief that proof-for would be equally valuable as proof-against in either of these categories.
  Finally, we'll conclude with some number crunching on where we see this particular field heading in the future.</summary>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T22:45:35Z</published>
    <arxiv:comment>7 pages</arxiv:comment>
    <arxiv:primary_category term="cs.CR"/>
    <author>
      <name>Lily Erickson</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12477v1</id>
    <title>MetaHGNIE: Meta-Path Induced Hypergraph Contrastive Learning in Heterogeneous Knowledge Graphs</title>
    <updated>2025-12-13T22:21:33Z</updated>
    <link href="https://arxiv.org/abs/2512.12477v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12477v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Node importance estimation (NIE) in heterogeneous knowledge graphs is a critical yet challenging task, essential for applications such as recommendation, knowledge reasoning, and question answering. Existing methods often rely on pairwise connections, neglecting high-order dependencies among multiple entities and relations, and they treat structural and semantic signals independently, hindering effective cross-modal integration. To address these challenges, we propose MetaHGNIE, a meta-path induced hypergraph contrastive learning framework for disentangling and aligning structural and semantic information. MetaHGNIE constructs a higher-order knowledge graph via meta-path sequences, where typed hyperedges capture multi-entity relational contexts. Structural dependencies are aggregated with local attention, while semantic representations are encoded through a hypergraph transformer equipped with sparse chunking to reduce redundancy. Finally, a multimodal fusion module integrates structural and semantic embeddings under contrastive learning with auxiliary supervision, ensuring robust cross-modal alignment. Extensive experiments on benchmark NIE datasets demonstrate that MetaHGNIE consistently outperforms state-of-the-art baselines. These results highlight the effectiveness of explicitly modeling higher-order interactions and cross-modal alignment in heterogeneous knowledge graphs. Our code is available at https://github.com/SEU-WENJIA/DualHNIE</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T22:21:33Z</published>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Jiawen Chen</name>
    </author>
    <author>
      <name>Yanyan He</name>
    </author>
    <author>
      <name>Qi Shao</name>
    </author>
    <author>
      <name>Mengli Wei</name>
    </author>
    <author>
      <name>Duxin Chen</name>
    </author>
    <author>
      <name>Wenwu Yu</name>
    </author>
    <author>
      <name>Yanlong Zhao</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12476v1</id>
    <title>HetRL: Efficient Reinforcement Learning for LLMs in Heterogeneous Environments</title>
    <updated>2025-12-13T22:20:51Z</updated>
    <link href="https://arxiv.org/abs/2512.12476v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12476v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>As large language models (LLMs) continue to scale and new GPUs are released even more frequently, there is an increasing demand for LLM post-training in heterogeneous environments to fully leverage underutilized mid-range or previous-generation GPUs across regions and alleviate the shortage of homogeneous high-end GPUs within a single region. However, achieving high-performance reinforcement learning (RL) training for LLMs on such computing resources remains challenging because the workflow involves multiple models and tasks with complex computation and data dependencies. In this paper, we present HetRL, a distributed system for efficient RL training in infrastructures with heterogeneous GPUs and networks. HetRL formulates the scheduling of RL training in heterogeneous environments as a constrained joint optimization problem and introduces a novel scheduling algorithm that (1) decomposes the complex search space with a multi-level search framework; and (2) allocates the search budget via successive halving. Our extensive evaluation, consuming 20,000 GPU-hours, shows that HetRL delivers up to 9.17x the throughput of state-of-the-art systems, and 3.17x on average, under various workloads and settings.</summary>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T22:20:51Z</published>
    <arxiv:primary_category term="cs.DC"/>
    <author>
      <name>Yongjun He</name>
    </author>
    <author>
      <name>Shuai Zhang</name>
    </author>
    <author>
      <name>Jiading Gai</name>
    </author>
    <author>
      <name>Xiyuan Zhang</name>
    </author>
    <author>
      <name>Boran Han</name>
    </author>
    <author>
      <name>Bernie Wang</name>
    </author>
    <author>
      <name>Huzefa Rangwala</name>
    </author>
    <author>
      <name>George Karypis</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12474v1</id>
    <title>AI-Driven Real-Time Kick Classification in Olympic Taekwondo Using Sensor Fusion</title>
    <updated>2025-12-13T22:17:51Z</updated>
    <link href="https://arxiv.org/abs/2512.12474v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12474v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Olympic Taekwondo has faced challenges in spectator engagement due to static, defensive gameplay and contentious scoring. Current Protector and Scoring Systems (PSS) rely on impact sensors and simplistic logic, encouraging safe strategies that diminish the sport's dynamism. This paper proposes an AI-powered scoring system that integrates existing PSS sensors with additional accelerometers, gyroscopes, magnetic/RFID, and impact force sensors in a sensor fusion framework. The system classifies kicks in real-time to identify technique type, contact location, impact force, and even the part of the foot used. A machine learning pipeline employing sensor fusion and Support Vector Machines (SVMs) is detailed, enabling automatic kick technique recognition for scoring. We present a novel kick scoring rubric that awards points based on specific kick techniques (e.g., turning and spinning kicks) to incentivize dynamic attacks. Drawing on a 2024 study achieving 96-98% accuracy, we validate the feasibility of real-time kick classification and further propose enhancements to this methodology, such as ensemble SVM classifiers and expanded datasets, to achieve the high-stakes accuracy required by the sport. We analyze how the proposed system can improve scoring fairness, reduce rule exploitation and illegitimate tactics, encourage more dynamic techniques, and enhance spectator understanding and excitement. The paper includes system design illustrations, a kick scoring table from an AI-augmented rule set, and discusses anticipated impacts on Olympic Taekwondo.</summary>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T22:17:51Z</published>
    <arxiv:comment>13 pages, 4 figures</arxiv:comment>
    <arxiv:primary_category term="eess.SY"/>
    <author>
      <name>Jamsheed Mistri</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12469v1</id>
    <title>Sparse Concept Anchoring for Interpretable and Controllable Neural Representations</title>
    <updated>2025-12-13T21:43:17Z</updated>
    <link href="https://arxiv.org/abs/2512.12469v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12469v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We introduce Sparse Concept Anchoring, a method that biases latent space to position a targeted subset of concepts while allowing others to self-organize, using only minimal supervision (labels for &lt;0.1% of examples per anchored concept). Training combines activation normalization, a separation regularizer, and anchor or subspace regularizers that attract rare labeled examples to predefined directions or axis-aligned subspaces. The anchored geometry enables two practical interventions: reversible behavioral steering that projects out a concept's latent component at inference, and permanent removal via targeted weight ablation of anchored dimensions. Experiments on structured autoencoders show selective attenuation of targeted concepts with negligible impact on orthogonal features, and complete elimination with reconstruction error approaching theoretical bounds. Sparse Concept Anchoring therefore provides a practical pathway to interpretable, steerable behavior in learned representations.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T21:43:17Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Sandy Fraser</name>
    </author>
    <author>
      <name>Patryk Wielopolski</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12465v1</id>
    <title>Exploring the Design Space of Transition Matching</title>
    <updated>2025-12-13T21:34:47Z</updated>
    <link href="https://arxiv.org/abs/2512.12465v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12465v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Transition Matching (TM) is an emerging paradigm for generative modeling that generalizes diffusion and flow-matching models as well as continuous-state autoregressive models. TM, similar to previous paradigms, gradually transforms noise samples to data samples, however it uses a second ``internal'' generative model to implement the transition steps, making the transitions more expressive compared to diffusion and flow models. To make this paradigm tractable, TM employs a large backbone network and a smaller "head" module to efficiently execute the generative transition step. In this work, we present a large-scale, systematic investigation into the design, training and sampling of the head in TM frameworks, focusing on its time-continuous bidirectional variant. Through comprehensive ablations and experimentation involving training 56 different 1.7B text-to-image models (resulting in 549 unique evaluations) we evaluate the affect of the head module architecture and modeling during training as-well as a useful family of stochastic TM samplers. We analyze the impact on generation quality, training, and inference efficiency. We find that TM with an MLP head, trained with a particular time weighting and sampled with high frequency sampler provides best ranking across all metrics reaching state-of-the-art among all tested baselines, while Transformer head with sequence scaling and low frequency sampling is a runner up excelling at image aesthetics. Lastly, we believe the experiments presented highlight the design aspects that are likely to provide most quality and efficiency gains, while at the same time indicate what design choices are not likely to provide further gains.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T21:34:47Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Uriel Singer</name>
    </author>
    <author>
      <name>Yaron Lipman</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12463v1</id>
    <title>Understanding Overparametrization in Survival Models through Double-Descent</title>
    <updated>2025-12-13T21:23:02Z</updated>
    <link href="https://arxiv.org/abs/2512.12463v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12463v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Classical statistical learning theory predicts a U-shaped relationship between test loss and model capacity, driven by the bias-variance trade-off. Recent advances in modern machine learning have revealed a more complex pattern, double-descent, in which test loss, after peaking near the interpolation threshold, decreases again as model capacity continues to grow. While this behavior has been extensively analyzed in regression and classification, its manifestation in survival analysis remains unexplored. This study investigates double-descent in four representative survival models: DeepSurv, PC-Hazard, Nnet-Survival, and N-MTLR. We rigorously define interpolation and finite-norm interpolation, two key characteristics of loss-based models to understand double-descent. We then show the existence (or absence) of (finite-norm) interpolation of all four models. Our findings clarify how likelihood-based losses and model implementation jointly determine the feasibility of interpolation and show that overfitting should not be regarded as benign for survival models. All theoretical results are supported by numerical experiments that highlight the distinct generalization behaviors of survival models.</summary>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T21:23:02Z</published>
    <arxiv:primary_category term="stat.ML"/>
    <author>
      <name>Yin Liu</name>
    </author>
    <author>
      <name>Jianwen Cai</name>
    </author>
    <author>
      <name>Didong Li</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12462v1</id>
    <title>Dynamical modeling of nonlinear latent factors in multiscale neural activity with real-time inference</title>
    <updated>2025-12-13T21:20:21Z</updated>
    <link href="https://arxiv.org/abs/2512.12462v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12462v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Real-time decoding of target variables from multiple simultaneously recorded neural time-series modalities, such as discrete spiking activity and continuous field potentials, is important across various neuroscience applications. However, a major challenge for doing so is that different neural modalities can have different timescales (i.e., sampling rates) and different probabilistic distributions, or can even be missing at some time-steps. Existing nonlinear models of multimodal neural activity do not address different timescales or missing samples across modalities. Further, some of these models do not allow for real-time decoding. Here, we develop a learning framework that can enable real-time recursive decoding while nonlinearly aggregating information across multiple modalities with different timescales and distributions and with missing samples. This framework consists of 1) a multiscale encoder that nonlinearly aggregates information after learning within-modality dynamics to handle different timescales and missing samples in real time, 2) a multiscale dynamical backbone that extracts multimodal temporal dynamics and enables real-time recursive decoding, and 3) modality-specific decoders to account for different probabilistic distributions across modalities. In both simulations and three distinct multiscale brain datasets, we show that our model can aggregate information across modalities with different timescales and distributions and missing samples to improve real-time target decoding. Further, our method outperforms various linear and nonlinear multimodal benchmarks in doing so.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T21:20:21Z</published>
    <arxiv:comment>Published at the 39th Annual Conference on Neural Information Processing Systems 2025. Code is available at https://github.com/ShanechiLab/mrine</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <arxiv:journal_ref>NeurIPS 2025</arxiv:journal_ref>
    <author>
      <name>Eray Erturk</name>
    </author>
    <author>
      <name>Maryam M. Shanechi</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12461v1</id>
    <title>Cross-Modal Representational Knowledge Distillation for Enhanced Spike-Informed LFP Modeling</title>
    <updated>2025-12-13T21:20:13Z</updated>
    <link href="https://arxiv.org/abs/2512.12461v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12461v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Local field potentials (LFPs) can be routinely recorded alongside spiking activity in intracortical neural experiments, measure a larger complementary spatiotemporal scale of brain activity for scientific inquiry, and can offer practical advantages over spikes, including greater long-term stability, robustness to electrode degradation, and lower power requirements. Despite these advantages, recent neural modeling frameworks have largely focused on spiking activity since LFP signals pose inherent modeling challenges due to their aggregate, population-level nature, often leading to lower predictive power for downstream task variables such as motor behavior. To address this challenge, we introduce a cross-modal knowledge distillation framework that transfers high-fidelity representational knowledge from pretrained multi-session spike transformer models to LFP transformer models. Specifically, we first train a teacher spike model across multiple recording sessions using a masked autoencoding objective with a session-specific neural tokenization strategy. We then align the latent representations of the student LFP model to those of the teacher spike model. Our results show that the Distilled LFP models consistently outperform single- and multi-session LFP baselines in both fully unsupervised and supervised settings, and can generalize to other sessions without additional distillation while maintaining superior performance. These findings demonstrate that cross-modal knowledge distillation is a powerful and scalable approach for leveraging high-performing spike models to develop more accurate LFP models.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T21:20:13Z</published>
    <arxiv:comment>Published at the 39th Annual Conference on Neural Information Processing Systems 2025. Code is available at https://github.com/ShanechiLab/CrossModalDistillation</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <arxiv:journal_ref>NeurIPS 2025</arxiv:journal_ref>
    <author>
      <name>Eray Erturk</name>
    </author>
    <author>
      <name>Saba Hashemi</name>
    </author>
    <author>
      <name>Maryam M. Shanechi</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12459v1</id>
    <title>From Particles to Fields: Reframing Photon Mapping with Continuous Gaussian Photon Fields</title>
    <updated>2025-12-13T21:09:09Z</updated>
    <link href="https://arxiv.org/abs/2512.12459v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12459v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Accurately modeling light transport is essential for realistic image synthesis. Photon mapping provides physically grounded estimates of complex global illumination effects such as caustics and specular-diffuse interactions, yet its per-view radiance estimation remains computationally inefficient when rendering multiple views of the same scene. The inefficiency arises from independent photon tracing and stochastic kernel estimation at each viewpoint, leading to inevitable redundant computation. To accelerate multi-view rendering, we reformulate photon mapping as a continuous and reusable radiance function. Specifically, we introduce the Gaussian Photon Field (GPF), a learnable representation that encodes photon distributions as anisotropic 3D Gaussian primitives parameterized by position, rotation, scale, and spectrum. GPF is initialized from physically traced photons in the first SPPM iteration and optimized using multi-view supervision of final radiance, distilling photon-based light transport into a continuous field. Once trained, the field enables differentiable radiance evaluation along camera rays without repeated photon tracing or iterative refinement. Extensive experiments on scenes with complex light transport, such as caustics and specular-diffuse interactions, demonstrate that GPF attains photon-level accuracy while reducing computation by orders of magnitude, unifying the physical rigor of photon-based rendering with the efficiency of neural scene representations.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T21:09:09Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Jiachen Tao</name>
    </author>
    <author>
      <name>Benjamin Planche</name>
    </author>
    <author>
      <name>Van Nguyen Nguyen</name>
    </author>
    <author>
      <name>Junyi Wu</name>
    </author>
    <author>
      <name>Yuchun Liu</name>
    </author>
    <author>
      <name>Haoxuan Wang</name>
    </author>
    <author>
      <name>Zhongpai Gao</name>
    </author>
    <author>
      <name>Gengyu Zhang</name>
    </author>
    <author>
      <name>Meng Zheng</name>
    </author>
    <author>
      <name>Feiran Wang</name>
    </author>
    <author>
      <name>Anwesa Choudhuri</name>
    </author>
    <author>
      <name>Zhenghao Zhao</name>
    </author>
    <author>
      <name>Weitai Kang</name>
    </author>
    <author>
      <name>Terrence Chen</name>
    </author>
    <author>
      <name>Yan Yan</name>
    </author>
    <author>
      <name>Ziyan Wu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12458v1</id>
    <title>Breaking the Curse of Dimensionality: On the Stability of Modern Vector Retrieval</title>
    <updated>2025-12-13T21:05:21Z</updated>
    <link href="https://arxiv.org/abs/2512.12458v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12458v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Modern vector databases enable efficient retrieval over high-dimensional neural embeddings, powering applications from web search to retrieval-augmented generation. However, classical theory predicts such tasks should suffer from the curse of dimensionality, where distances between points become nearly indistinguishable, thereby crippling efficient nearest-neighbor search. We revisit this paradox through the lens of stability, the property that small perturbations to a query do not radically alter its nearest neighbors. Building on foundational results, we extend stability theory to three key retrieval settings widely used in practice: (i) multi-vector search, where we prove that the popular Chamfer distance metric preserves single-vector stability, while average pooling aggregation may destroy it; (ii) filtered vector search, where we show that sufficiently large penalties for mismatched filters can induce stability even when the underlying search is unstable; and (iii) sparse vector search, where we formalize and prove novel sufficient stability conditions. Across synthetic and real datasets, our experimental results match our theoretical predictions, offering concrete guidance for model and system design to avoid the curse of dimensionality.</summary>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T21:05:21Z</published>
    <arxiv:comment>27 pages</arxiv:comment>
    <arxiv:primary_category term="cs.IR"/>
    <author>
      <name>Vihan Lakshman</name>
    </author>
    <author>
      <name>Blaise Munyampirwa</name>
    </author>
    <author>
      <name>Julian Shun</name>
    </author>
    <author>
      <name>Benjamin Coleman</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12448v1</id>
    <title>Optimized Architectures for Kolmogorov-Arnold Networks</title>
    <updated>2025-12-13T20:14:08Z</updated>
    <link href="https://arxiv.org/abs/2512.12448v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12448v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Efforts to improve Kolmogorov-Arnold networks (KANs) with architectural enhancements have been stymied by the complexity those enhancements bring, undermining the interpretability that makes KANs attractive in the first place. Here we study overprovisioned architectures combined with sparsification to learn compact, interpretable KANs without sacrificing accuracy. Crucially, we focus on differentiable sparsification, turning architecture search into an end-to-end optimization problem. Across function approximation benchmarks, dynamical systems forecasting, and real-world prediction tasks, we demonstrate competitive or superior accuracy while discovering substantially smaller models. Overprovisioning and sparsification are synergistic, with the combination outperforming either alone. The result is a principled path toward models that are both more expressive and more interpretable, addressing a key tension in scientific machine learning.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T20:14:08Z</published>
    <arxiv:comment>12 pages, 1 figure, 3 tables</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>James Bagrow</name>
    </author>
    <author>
      <name>Josh Bongard</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12447v1</id>
    <title>Large language models have learned to use language</title>
    <updated>2025-12-13T20:09:10Z</updated>
    <link href="https://arxiv.org/abs/2512.12447v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12447v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Acknowledging that large language models have learned to use language can open doors to breakthrough language science. Achieving these breakthroughs may require abandoning some long-held ideas about how language knowledge is evaluated and reckoning with the difficult fact that we have entered a post-Turing test era.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T20:09:10Z</published>
    <arxiv:comment>Commentary on Futrell &amp; Mahowald's How Linguistics Learned to Stop Worrying and Love the Language Models (BBS, Forthcoming)</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Gary Lupyan</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12445v1</id>
    <title>Knowledge-Guided Masked Autoencoder with Linear Spectral Mixing and Spectral-Angle-Aware Reconstruction</title>
    <updated>2025-12-13T19:59:04Z</updated>
    <link href="https://arxiv.org/abs/2512.12445v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12445v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Integrating domain knowledge into deep learning has emerged as a promising direction for improving model interpretability, generalization, and data efficiency. In this work, we present a novel knowledge-guided ViT-based Masked Autoencoder that embeds scientific domain knowledge within the self-supervised reconstruction process. Instead of relying solely on data-driven optimization, our proposed approach incorporates the Linear Spectral Mixing Model (LSMM) as a physical constraint and physically-based Spectral Angle Mapper (SAM), ensuring that learned representations adhere to known structural relationships between observed signals and their latent components. The framework jointly optimizes LSMM and SAM loss with a conventional Huber loss objective, promoting both numerical accuracy and geometric consistency in the feature space. This knowledge-guided design enhances reconstruction fidelity, stabilizes training under limited supervision, and yields interpretable latent representations grounded in physical principles. The experimental findings indicate that the proposed model substantially enhances reconstruction quality and improves downstream task performance, highlighting the promise of embedding physics-informed inductive biases within transformer-based self-supervised learning.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T19:59:04Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Abdul Matin</name>
    </author>
    <author>
      <name>Rupasree Dey</name>
    </author>
    <author>
      <name>Tanjim Bin Faruk</name>
    </author>
    <author>
      <name>Shrideep Pallickara</name>
    </author>
    <author>
      <name>Sangmi Lee Pallickara</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12444v1</id>
    <title>Can GPT replace human raters? Validity and reliability of machine-generated norms for metaphors</title>
    <updated>2025-12-13T19:56:31Z</updated>
    <link href="https://arxiv.org/abs/2512.12444v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12444v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>As Large Language Models (LLMs) are increasingly being used in scientific research, the issue of their trustworthiness becomes crucial. In psycholinguistics, LLMs have been recently employed in automatically augmenting human-rated datasets, with promising results obtained by generating ratings for single words. Yet, performance for ratings of complex items, i.e., metaphors, is still unexplored. Here, we present the first assessment of the validity and reliability of ratings of metaphors on familiarity, comprehensibility, and imageability, generated by three GPT models for a total of 687 items gathered from the Italian Figurative Archive and three English studies. We performed a thorough validation in terms of both alignment with human data and ability to predict behavioral and electrophysiological responses. We found that machine-generated ratings positively correlated with human-generated ones. Familiarity ratings reached moderate-to-strong correlations for both English and Italian metaphors, although correlations weakened for metaphors with high sensorimotor load. Imageability showed moderate correlations in English and moderate-to-strong in Italian. Comprehensibility for English metaphors exhibited the strongest correlations. Overall, larger models outperformed smaller ones and greater human-model misalignment emerged with familiarity and imageability. Machine-generated ratings significantly predicted response times and the EEG amplitude, with a strength comparable to human ratings. Moreover, GPT ratings obtained across independent sessions were highly stable. We conclude that GPT, especially larger models, can validly and reliably replace - or augment - human subjects in rating metaphor properties. Yet, LLMs align worse with humans when dealing with conventionality and multimodal aspects of metaphorical meaning, calling for careful consideration of the nature of stimuli.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T19:56:31Z</published>
    <arxiv:comment>30 pages, 5 figures</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Veronica Mangiaterra</name>
    </author>
    <author>
      <name>Hamad Al-Azary</name>
    </author>
    <author>
      <name>Chiara Barattieri di San Pietro</name>
    </author>
    <author>
      <name>Paolo Canal</name>
    </author>
    <author>
      <name>Valentina Bambini</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12443v1</id>
    <title>AI Transparency Atlas: Framework, Scoring, and Real-Time Model Card Evaluation Pipeline</title>
    <updated>2025-12-13T19:48:44Z</updated>
    <link href="https://arxiv.org/abs/2512.12443v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12443v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>AI model documentation is fragmented across platforms and inconsistent in structure, preventing policymakers, auditors, and users from reliably assessing safety claims, data provenance, and version-level changes. We analyzed documentation from five frontier models (Gemini 3, Grok 4.1, Llama 4, GPT-5, and Claude 4.5) and 100 Hugging Face model cards, identifying 947 unique section names with extreme naming variation. Usage information alone appeared under 97 distinct labels. Using the EU AI Act Annex IV and the Stanford Transparency Index as baselines, we developed a weighted transparency framework with 8 sections and 23 subsections that prioritizes safety-critical disclosures (Safety Evaluation: 25%, Critical Risk: 20%) over technical specifications. We implemented an automated multi-agent pipeline that extracts documentation from public sources and scores completeness through LLM-based consensus. Evaluating 50 models across vision, multimodal, open-source, and closed-source systems cost less than $3 in total and revealed systematic gaps. Frontier labs (xAI, Microsoft, Anthropic) achieve approximately 80% compliance, while most providers fall below 60%. Safety-critical categories show the largest deficits: deception behaviors, hallucinations, and child safety evaluations account for 148, 124, and 116 aggregate points lost, respectively, across all evaluated models.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T19:48:44Z</published>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Akhmadillo Mamirov</name>
    </author>
    <author>
      <name>Faiaz Azmain</name>
    </author>
    <author>
      <name>Hanyu Wang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12442v1</id>
    <title>Efficient Level-Crossing Probability Calculation for Gaussian Process Modeled Data</title>
    <updated>2025-12-13T19:48:25Z</updated>
    <link href="https://arxiv.org/abs/2512.12442v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12442v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Almost all scientific data have uncertainties originating from different sources. Gaussian process regression (GPR) models are a natural way to model data with Gaussian-distributed uncertainties. GPR also has the benefit of reducing I/O bandwidth and storage requirements for large scientific simulations. However, the reconstruction from the GPR models suffers from high computation complexity. To make the situation worse, classic approaches for visualizing the data uncertainties, like probabilistic marching cubes, are also computationally very expensive, especially for data of high resolutions. In this paper, we accelerate the level-crossing probability calculation efficiency on GPR models by subdividing the data spatially into a hierarchical data structure and only reconstructing values adaptively in the regions that have a non-zero probability. For each region, leveraging the known GPR kernel and the saved data observations, we propose a novel approach to efficiently calculate an upper bound for the level-crossing probability inside the region and use this upper bound to make the subdivision and reconstruction decisions. We demonstrate that our value occurrence probability estimation is accurate with a low computation cost by experiments that calculate the level-crossing probability fields on different datasets.</summary>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T19:48:25Z</published>
    <arxiv:comment>IEEE PacificVis 2024</arxiv:comment>
    <arxiv:primary_category term="stat.ML"/>
    <author>
      <name>Haoyu Li</name>
    </author>
    <author>
      <name>Isaac J Michaud</name>
    </author>
    <author>
      <name>Ayan Biswas</name>
    </author>
    <author>
      <name>Han-Wei Shen</name>
    </author>
    <arxiv:doi>10.1109/PacificVis60374.2024.00035</arxiv:doi>
    <link rel="related" href="https://doi.org/10.1109/PacificVis60374.2024.00035" title="doi"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12436v1</id>
    <title>Rough Sets for Explainability of Spectral Graph Clustering</title>
    <updated>2025-12-13T19:29:04Z</updated>
    <link href="https://arxiv.org/abs/2512.12436v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12436v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Graph Spectral Clustering methods (GSC) allow representing clusters of diverse shapes, densities, etc. However, the results of such algorithms, when applied e.g. to text documents, are hard to explain to the user, especially due to embedding in the spectral space which has no obvious relation to document contents. Furthermore, the presence of documents without clear content meaning and the stochastic nature of the clustering algorithms deteriorate explainability. This paper proposes an enhancement to the explanation methodology, proposed in an earlier research of our team. It allows us to overcome the latter problems by taking inspiration from rough set theory.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T19:29:04Z</published>
    <arxiv:comment>24 figures, 21tables</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Bartłomiej Starosta</name>
    </author>
    <author>
      <name>Sławomir T. Wierzchoń</name>
    </author>
    <author>
      <name>Piotr Borkowski</name>
    </author>
    <author>
      <name>Dariusz Czerski</name>
    </author>
    <author>
      <name>Marcin Sydow</name>
    </author>
    <author>
      <name>Eryk Laskowski</name>
    </author>
    <author>
      <name>Mieczysław A. Kłopotek</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12435v1</id>
    <title>Co-Hub Node Based Multiview Graph Learning with Theoretical Guarantees</title>
    <updated>2025-12-13T19:25:35Z</updated>
    <link href="https://arxiv.org/abs/2512.12435v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12435v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Identifying the graphical structure underlying the observed multivariate data is essential in numerous applications. Current methodologies are predominantly confined to deducing a singular graph under the presumption that the observed data are uniform. However, many contexts involve heterogeneous datasets that feature multiple closely related graphs, typically referred to as multiview graphs. Previous research on multiview graph learning promotes edge-based similarity across layers using pairwise or consensus-based regularizers. However, multiview graphs frequently exhibit a shared node-based architecture across different views, such as common hub nodes. Such commonalities can enhance the precision of learning and provide interpretive insight. In this paper, we propose a co-hub node model, positing that different views share a common group of hub nodes. The associated optimization framework is developed by enforcing structured sparsity on the connections of these co-hub nodes. Moreover, we present a theoretical examination of layer identifiability and determine bounds on estimation error. The proposed methodology is validated using both synthetic graph data and fMRI time series data from multiple subjects to discern several closely related graphs.</summary>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T19:25:35Z</published>
    <arxiv:primary_category term="stat.ML"/>
    <author>
      <name>Bisakh Banerjee</name>
    </author>
    <author>
      <name>Mohammad Alwardat</name>
    </author>
    <author>
      <name>Tapabrata Maiti</name>
    </author>
    <author>
      <name>Selin Aviyente</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12432v1</id>
    <title>Data-driven modelling of autonomous and forced dynamical systems</title>
    <updated>2025-12-13T19:20:48Z</updated>
    <link href="https://arxiv.org/abs/2512.12432v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12432v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The paper demonstrates that invariant foliations are accurate, data-efficient and practical tools for data-driven modelling of physical systems. Invariant foliations can be fitted to data that either fill the phase space or cluster about an invariant manifold. Invariant foliations can be fitted to a single trajectory or multiple trajectories. Over and underfitting are eliminated by appropriately choosing a function representation and its hyperparameters, such as polynomial orders. The paper extends invariant foliations to forced and parameter dependent systems. It is assumed that forcing is provided by a volume preserving map, and therefore the forcing can be periodic, quasi-periodic or even chaotic. The method utilises full trajectories, hence it is able to predict long-term dynamics accurately. We take into account if a forced system is reducible to an autonomous system about a steady state, similar to how Floquet theory guarantees reducibility for periodically forced systems. In order to find an invariant manifold, multiple invariant foliations are calculated in the neighbourhood of the invariant manifold. Some of the invariant foliations can be linear, while others nonlinear but only defined in a small neighbourhood of an invariant manifold, which reduces the number of parameters to be identified. An invariant manifold is recovered as the zero level set of one or more of the foliations. To interpret the results, the identified mathematical models are transformed to a canonical form and instantaneous frequency and damping information are calculated.</summary>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T19:20:48Z</published>
    <arxiv:comment>Keywords: Invariant foliation, Invariant manifold, Reduced order model, Instantaneous frequency, Tensor approximation, Machine learning, JEPA</arxiv:comment>
    <arxiv:primary_category term="math.DS"/>
    <author>
      <name>Robert Szalai</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12430v1</id>
    <title>Endless World: Real-Time 3D-Aware Long Video Generation</title>
    <updated>2025-12-13T19:06:12Z</updated>
    <link href="https://arxiv.org/abs/2512.12430v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12430v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Producing long, coherent video sequences with stable 3D structure remains a major challenge, particularly in streaming scenarios. Motivated by this, we introduce Endless World, a real-time framework for infinite, 3D-consistent video generation.To support infinite video generation, we introduce a conditional autoregressive training strategy that aligns newly generated content with existing video frames. This design preserves long-range dependencies while remaining computationally efficient, enabling real-time inference on a single GPU without additional training overhead.Moreover, our Endless World integrates global 3D-aware attention to provide continuous geometric guidance across time. Our 3D injection mechanism enforces physical plausibility and geometric consistency throughout extended sequences, addressing key challenges in long-horizon and dynamic scene synthesis.Extensive experiments demonstrate that Endless World produces long, stable, and visually coherent videos, achieving competitive or superior performance to existing methods in both visual fidelity and spatial consistency. Our project has been available on https://bwgzk-keke.github.io/EndlessWorld/.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T19:06:12Z</published>
    <arxiv:comment>10 pages,7 figures</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Ke Zhang</name>
    </author>
    <author>
      <name>Yiqun Mei</name>
    </author>
    <author>
      <name>Jiacong Xu</name>
    </author>
    <author>
      <name>Vishal M. Patel</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12428v1</id>
    <title>Learning Dynamics in Memristor-Based Equilibrium Propagation</title>
    <updated>2025-12-13T18:57:05Z</updated>
    <link href="https://arxiv.org/abs/2512.12428v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12428v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Memristor-based in-memory computing has emerged as a promising paradigm to overcome the constraints of the von Neumann bottleneck and the memory wall by enabling fully parallelisable and energy-efficient vector-matrix multiplications. We investigate the effect of nonlinear, memristor-driven weight updates on the convergence behaviour of neural networks trained with equilibrium propagation (EqProp). Six memristor models were characterised by their voltage-current hysteresis and integrated into the EBANA framework for evaluation on two benchmark classification tasks. EqProp can achieve robust convergence under nonlinear weight updates, provided that memristors exhibit a sufficiently wide resistance range of at least an order of magnitude.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T18:57:05Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Michael Döll</name>
    </author>
    <author>
      <name>Andreas Müller</name>
    </author>
    <author>
      <name>Bernd Ulmann</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12425v1</id>
    <title>BokehDepth: Enhancing Monocular Depth Estimation through Bokeh Generation</title>
    <updated>2025-12-13T18:39:23Z</updated>
    <link href="https://arxiv.org/abs/2512.12425v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12425v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Bokeh and monocular depth estimation are tightly coupled through the same lens imaging geometry, yet current methods exploit this connection in incomplete ways. High-quality bokeh rendering pipelines typically depend on noisy depth maps, which amplify estimation errors into visible artifacts, while modern monocular metric depth models still struggle on weakly textured, distant and geometrically ambiguous regions where defocus cues are most informative. We introduce BokehDepth, a two-stage framework that decouples bokeh synthesis from depth prediction and treats defocus as an auxiliary supervision-free geometric cue. In Stage-1, a physically guided controllable bokeh generator, built on a powerful pretrained image editing backbone, produces depth-free bokeh stacks with calibrated bokeh strength from a single sharp input. In Stage-2, a lightweight defocus-aware aggregation module plugs into existing monocular depth encoders, fuses features along the defocus dimension, and exposes stable depth-sensitive variations while leaving downstream decoder unchanged. Across challenging benchmarks, BokehDepth improves visual fidelity over depth-map-based bokeh baselines and consistently boosts the metric accuracy and robustness of strong monocular depth foundation models.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T18:39:23Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Hangwei Zhang</name>
    </author>
    <author>
      <name>Armando Teles Fortes</name>
    </author>
    <author>
      <name>Tianyi Wei</name>
    </author>
    <author>
      <name>Xingang Pan</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12424v1</id>
    <title>ViInfographicVQA: A Benchmark for Single and Multi-image Visual Question Answering on Vietnamese Infographics</title>
    <updated>2025-12-13T18:37:47Z</updated>
    <link href="https://arxiv.org/abs/2512.12424v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12424v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Infographic Visual Question Answering (InfographicVQA) evaluates a model's ability to read and reason over data-rich, layout-heavy visuals that combine text, charts, icons, and design elements. Compared with scene-text or natural-image VQA, infographics require stronger integration of OCR, layout understanding, and numerical and semantic reasoning. We introduce ViInfographicVQA, the first benchmark for Vietnamese InfographicVQA, comprising over 6747 real-world infographics and 20409 human-verified question-answer pairs across economics, healthcare, education, and more. The benchmark includes two evaluation settings. The Single-image task follows the traditional setup in which each question is answered using a single infographic. The Multi-image task requires synthesizing evidence across multiple semantically related infographics and is, to our knowledge, the first Vietnamese evaluation of cross-image reasoning in VQA. We evaluate a range of recent vision-language models on this benchmark, revealing substantial performance disparities, with the most significant errors occurring on Multi-image questions that involve cross-image integration and non-span reasoning. ViInfographicVQA contributes benchmark results for Vietnamese InfographicVQA and sheds light on the limitations of current multimodal models in low-resource contexts, encouraging future exploration of layout-aware and cross-image reasoning methods.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T18:37:47Z</published>
    <arxiv:comment>10 pages, 4 figures, Accepted to AI4Research @ AAAI</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Tue-Thu Van-Dinh</name>
    </author>
    <author>
      <name>Hoang-Duy Tran</name>
    </author>
    <author>
      <name>Truong-Binh Duong</name>
    </author>
    <author>
      <name>Mai-Hanh Pham</name>
    </author>
    <author>
      <name>Binh-Nam Le-Nguyen</name>
    </author>
    <author>
      <name>Quoc-Thai Nguyen</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12413v1</id>
    <title>Understanding Critical Thinking in Generative Artificial Intelligence Use: Development, Validation, and Correlates of the Critical Thinking in AI Use Scale</title>
    <updated>2025-12-13T17:56:12Z</updated>
    <link href="https://arxiv.org/abs/2512.12413v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12413v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Generative AI tools are increasingly embedded in everyday work and learning, yet their fluency, opacity, and propensity to hallucinate mean that users must critically evaluate AI outputs rather than accept them at face value. The present research conceptualises critical thinking in AI use as a dispositional tendency to verify the source and content of AI-generated information, to understand how models work and where they fail, and to reflect on the broader implications of relying on AI. Across six studies (N = 1365), we developed and validated the 13-item critical thinking in AI use scale and mapped its nomological network. Study 1 generated and content-validated scale items. Study 2 supported a three-factor structure (Verification, Motivation, and Reflection). Studies 3, 4, and 5 confirmed this higher-order model, demonstrated internal consistency and test-retest reliability, strong factor loadings, sex invariance, and convergent and discriminant validity. Studies 3 and 4 further revealed that critical thinking in AI use was positively associated with openness, extraversion, positive trait affect, and frequency of AI use. Lastly, Study 6 demonstrated criterion validity of the scale, with higher critical thinking in AI use scores predicting more frequent and diverse verification strategies, greater veracity-judgement accuracy in a novel and naturalistic ChatGPT-powered fact-checking task, and deeper reflection about responsible AI. Taken together, the current work clarifies why and how people exercise oversight over generative AI outputs and provides a validated scale and ecologically grounded task paradigm to support theory testing, cross-group, and longitudinal research on critical engagement with generative AI outputs.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T17:56:12Z</published>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Gabriel R. Lau</name>
    </author>
    <author>
      <name>Wei Yan Low</name>
    </author>
    <author>
      <name>Louis Tay</name>
    </author>
    <author>
      <name>Ysabel Guevarra</name>
    </author>
    <author>
      <name>Dragan Gašević</name>
    </author>
    <author>
      <name>Andree Hartanto</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12411v1</id>
    <title>Feeling the Strength but Not the Source: Partial Introspection in LLMs</title>
    <updated>2025-12-13T17:51:13Z</updated>
    <link href="https://arxiv.org/abs/2512.12411v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12411v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Recent work from Anthropic claims that frontier models can sometimes detect and name injected "concepts" represented as activation directions. We test the robustness of these claims. First, we reproduce Anthropic's multi-turn "emergent introspection" result on Meta-Llama-3.1-8B-Instruct, finding that the model identifies and names the injected concept 20 percent of the time under Anthropic's original pipeline, exactly matching their reported numbers and thus showing that introspection is not exclusive to very large or capable models. Second, we systematically vary the inference prompt and find that introspection is fragile: performance collapses on closely related tasks such as multiple-choice identification of the injected concept or different prompts of binary discrimination of whether a concept was injected at all. Third, we identify a contrasting regime of partial introspection: the same model can reliably classify the strength of the coefficient of a normalized injected concept vector (as weak / moderate / strong / very strong) with up to 70 percent accuracy, far above the 25 percent chance baseline. Together, these results provide more evidence for Anthropic's claim that language models effectively compute a function of their baseline, internal representations during introspection; however, these self-reports about those representations are narrow and prompt-sensitive. Our code is available at https://github.com/elyhahami18/CS2881-Introspection.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T17:51:13Z</published>
    <arxiv:comment>7 pages (+ 5 pages for appendix), 5 figures, 1 table</arxiv:comment>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Ely Hahami</name>
    </author>
    <author>
      <name>Lavik Jain</name>
    </author>
    <author>
      <name>Ishaan Sinha</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12410v1</id>
    <title>A Graph Attention Network-Based Framework for Reconstructing Missing LiDAR Beams</title>
    <updated>2025-12-13T17:50:57Z</updated>
    <link href="https://arxiv.org/abs/2512.12410v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12410v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Vertical beam dropout in spinning LiDAR sensors triggered by hardware aging, dust, snow, fog, or bright reflections removes entire vertical slices from the point cloud and severely degrades 3D perception in autonomous vehicles. This paper proposes a Graph Attention Network (GAT)-based framework that reconstructs these missing vertical channels using only the current LiDAR frame, with no camera images or temporal information required. Each LiDAR sweep is represented as an unstructured spatial graph: points are nodes and edges connect nearby points while preserving the original beam-index ordering. A multi-layer GAT learns adaptive attention weights over local geometric neighborhoods and directly regresses the missing elevation (z) values at dropout locations. Trained and evaluated on 1,065 raw KITTI sequences with simulated channel dropout, the method achieves an average height RMSE of 11.67 cm, with 87.98% of reconstructed points falling within a 10 cm error threshold. Inference takes 14.65 seconds per frame on a single GPU, and reconstruction quality remains stable for different neighborhood sizes k. These results show that a pure graph attention model operating solely on raw point-cloud geometry can effectively recover dropped vertical beams under realistic sensor degradation.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T17:50:57Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Khalfalla Awedat</name>
    </author>
    <author>
      <name>Mohamed Abidalrekab</name>
    </author>
    <author>
      <name>Mohammad El-Yabroudi</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12409v1</id>
    <title>Reputation-Based Leader Election under Partial Synchrony: Towards a Protocol-Independent Abstraction with Enhanced Guarantees</title>
    <updated>2025-12-13T17:45:33Z</updated>
    <link href="https://arxiv.org/abs/2512.12409v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12409v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Leader election serves a well-defined role in leader-based Byzantine Fault Tolerant (BFT) protocols. Existing reputation-based leader election frameworks for partially synchronous BFTs suffer from either protocol-specific proofs, narrow applicability, or unbounded recovery after network stabilization, leaving an open problem. This paper presents a novel protocol-independent abstraction formalizing generic correctness properties and effectiveness guarantees for leader election under partial synchrony, enabling protocol-independent analysis and design. Building on this, we design the Sliding Window Leader Election (SWLE) mechanism. SWLE dynamically adjusts leader nominations via consensus-behavior-based reputation scores, enforcing Byzantine-cost amplification. We demonstrate SWLE introduces minimal extra overhead to the base protocol and prove it satisfies all abstraction properties and provides superior effectiveness. We show, with a 16-server deployment across 4 different regions in northern China, SWLE achieves up to 4.2x higher throughput, 75% lower latency and 27% Byzantine leader frequency compared to the state-of-the-art solution under common Byzantine faults, while maintaining efficiency in fault-free scenarios.</summary>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T17:45:33Z</published>
    <arxiv:primary_category term="cs.DC"/>
    <author>
      <name>Xuyang Liu</name>
    </author>
    <author>
      <name>Zijian Zhang</name>
    </author>
    <author>
      <name>Zhen Li</name>
    </author>
    <author>
      <name>Jiahang Sun</name>
    </author>
    <author>
      <name>Jiamou Liu</name>
    </author>
    <author>
      <name>Peng Jiang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12405v1</id>
    <title>Can Graphs Improve Tabular Foundation Models?</title>
    <updated>2025-12-13T17:34:18Z</updated>
    <link href="https://arxiv.org/abs/2512.12405v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12405v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Tabular data are central to many real-world systems. While recent tabular transformers and in-context learners such as SAINT, TP-BERTa, TabPFN, TabICL, and MITRA incorporate limited inter-row reasoning, most approaches still lack an explicit mechanism to model relationships among instances, even though similar samples often share related outcomes. We investigate whether introducing \emph{simple graph priors} can enhance \emph{pretrained tabular transformers}. Concretely, we introduce {BOLERO}, a lightweight, static bipartite graph head that augments {RoBERTa-Tab} (a RoBERTa-style tabular backbone pretrained with masked-token prediction.) Each instance connects to feature/value anchors; a small GNN refines row representations, while the backbone remains frozen. We evaluate on 80 classification and 64 regression datasets from the TP-BERTa benchmark suites, comparing against strong baselines including XGBoost, CatBoost, TabPFN-v2, MITRA, TabICL, TP-BERTa, and RoBERTa-Tab. To ensure statistically sound conclusions, we follow best practices for multi-dataset evaluation: pairwise Wilcoxon signed-rank tests on per-dataset score differences and effect sizes (median improvement with confidence intervals), rather than mean-rank post-hoc tests that depend on the competitor pool. BOLERO achieves the highest number of statistically significant wins across both classification and regression, demonstrating that lightweight graph priors meaningfully improve pretrained tabular transformers.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T17:34:18Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Franck Le</name>
    </author>
    <author>
      <name>Keith Grueneberg</name>
    </author>
    <author>
      <name>Erich Nahum</name>
    </author>
    <author>
      <name>Vadim Sheinin</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12402v1</id>
    <title>DeepVekua: Geometric-Spectral Representation Learning for Physics-Informed Fields</title>
    <updated>2025-12-13T17:29:55Z</updated>
    <link href="https://arxiv.org/abs/2512.12402v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12402v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We present DeepVekua, a hybrid architecture that unifies geometric deep learning with spectral analysis to solve partial differential equations (PDEs) in sparse data regimes. By learning a diffeomorphic coordinate transformation that maps complex geometries to a latent harmonic space, our method outperforms state-of-the-art implicit representations on advection-diffusion systems. Unlike standard coordinate-based networks which struggle with spectral bias, DeepVekua separates the learning of geometry from the learning of physics, solving for optimal spectral weights in closed form. We demonstrate a 100x improvement over spectral baselines. The code is available at https://github.com/VladimerKhasia/vekuanet.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T17:29:55Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Vladimer Khasia</name>
    </author>
    <arxiv:doi>10.5281/zenodo.17918695</arxiv:doi>
    <link rel="related" href="https://doi.org/10.5281/zenodo.17918695" title="doi"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12395v1</id>
    <title>ArtGen: Conditional Generative Modeling of Articulated Objects in Arbitrary Part-Level States</title>
    <updated>2025-12-13T17:00:03Z</updated>
    <link href="https://arxiv.org/abs/2512.12395v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12395v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Generating articulated assets is crucial for robotics, digital twins, and embodied intelligence. Existing generative models often rely on single-view inputs representing closed states, resulting in ambiguous or unrealistic kinematic structures due to the entanglement between geometric shape and joint dynamics. To address these challenges, we introduce ArtGen, a conditional diffusion-based framework capable of generating articulated 3D objects with accurate geometry and coherent kinematics from single-view images or text descriptions at arbitrary part-level states. Specifically, ArtGen employs cross-state Monte Carlo sampling to explicitly enforce global kinematic consistency, reducing structural-motion entanglement. Additionally, we integrate a Chain-of-Thought reasoning module to infer robust structural priors, such as part semantics, joint types, and connectivity, guiding a sparse-expert Diffusion Transformer to specialize in diverse kinematic interactions. Furthermore, a compositional 3D-VAE latent prior enhanced with local-global attention effectively captures fine-grained geometry and global part-level relationships. Extensive experiments on the PartNet-Mobility benchmark demonstrate that ArtGen significantly outperforms state-of-the-art methods.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T17:00:03Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Haowen Wang</name>
    </author>
    <author>
      <name>Xiaoping Yuan</name>
    </author>
    <author>
      <name>Fugang Zhang</name>
    </author>
    <author>
      <name>Rui Jian</name>
    </author>
    <author>
      <name>Yuanwei Zhu</name>
    </author>
    <author>
      <name>Xiuquan Qiao</name>
    </author>
    <author>
      <name>Yakun Huang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12394v1</id>
    <title>The Morphemic Origin of Zipf's Law: A Factorized Combinatorial Framework</title>
    <updated>2025-12-13T16:58:06Z</updated>
    <link href="https://arxiv.org/abs/2512.12394v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12394v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We present a simple structure based model of how words are formed from morphemes. The model explains two major empirical facts: the typical distribution of word lengths and the appearance of Zipf like rank frequency curves. In contrast to classical explanations based on random text or communication efficiency, our approach uses only the combinatorial organization of prefixes, roots, suffixes and inflections. In this Morphemic Combinatorial Word Model, a word is created by activating several positional slots. Each slot turns on with a certain probability and selects one morpheme from its inventory. Morphemes are treated as stable building blocks that regularly appear in word formation and have characteristic positions. This mechanism produces realistic word length patterns with a concentrated middle zone and a thin long tail, closely matching real languages. Simulations with synthetic morpheme inventories also generate rank frequency curves with Zipf like exponents around 1.1-1.4, similar to English, Russian and Romance languages. The key result is that Zipf like behavior can emerge without meaning, communication pressure or optimization principles. The internal structure of morphology alone, combined with probabilistic activation of slots, is sufficient to create the robust statistical patterns observed across languages.</summary>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T16:58:06Z</published>
    <arxiv:primary_category term="stat.ME"/>
    <author>
      <name>Vladimir Berman</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12387v1</id>
    <title>Anchoring Values in Temporal and Group Dimensions for Flow Matching Model Alignment</title>
    <updated>2025-12-13T16:31:26Z</updated>
    <link href="https://arxiv.org/abs/2512.12387v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12387v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Group Relative Policy Optimization (GRPO) has proven highly effective in enhancing the alignment capabilities of Large Language Models (LLMs). However, current adaptations of GRPO for the flow matching-based image generation neglect a foundational conflict between its core principles and the distinct dynamics of the visual synthesis process. This mismatch leads to two key limitations: (i) Uniformly applying a sparse terminal reward across all timesteps impairs temporal credit assignment, ignoring the differing criticality of generation phases from early structure formation to late-stage tuning. (ii) Exclusive reliance on relative, intra-group rewards causes the optimization signal to fade as training converges, leading to the optimization stagnation when reward diversity is entirely depleted. To address these limitations, we propose Value-Anchored Group Policy Optimization (VGPO), a framework that redefines value estimation across both temporal and group dimensions. Specifically, VGPO transforms the sparse terminal reward into dense, process-aware value estimates, enabling precise credit assignment by modeling the expected cumulative reward at each generative stage. Furthermore, VGPO replaces standard group normalization with a novel process enhanced by absolute values to maintain a stable optimization signal even as reward diversity declines. Extensive experiments on three benchmarks demonstrate that VGPO achieves state-of-the-art image quality while simultaneously improving task-specific accuracy, effectively mitigating reward hacking. Project webpage: https://yawen-shao.github.io/VGPO/.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T16:31:26Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Yawen Shao</name>
    </author>
    <author>
      <name>Jie Xiao</name>
    </author>
    <author>
      <name>Kai Zhu</name>
    </author>
    <author>
      <name>Yu Liu</name>
    </author>
    <author>
      <name>Wei Zhai</name>
    </author>
    <author>
      <name>Yang Cao</name>
    </author>
    <author>
      <name>Zheng-Jun Zha</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12386v1</id>
    <title>Speedrunning ImageNet Diffusion</title>
    <updated>2025-12-13T16:30:28Z</updated>
    <link href="https://arxiv.org/abs/2512.12386v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12386v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Recent advances have significantly improved the training efficiency of diffusion transformers. However, these techniques have largely been studied in isolation, leaving unexplored the potential synergies from combining multiple approaches. We present SR-DiT (Speedrun Diffusion Transformer), a framework that systematically integrates token routing, architectural improvements, and training modifications on top of representation alignment. Our approach achieves FID 3.49 and KDD 0.319 on ImageNet-256 using only a 140M parameter model at 400K iterations without classifier-free guidance - comparable to results from 685M parameter models trained significantly longer. To our knowledge, this is a state-of the-art result at this model size. Through extensive ablation studies, we identify which technique combinations are most effective and document both synergies and incompatibilities. We release our framework as a computationally accessible baseline for future research.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T16:30:28Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Swayam Bhanded</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12384v1</id>
    <title>The Data Efficiency Frontier of Financial Foundation Models: Scaling Laws from Continued Pretraining</title>
    <updated>2025-12-13T16:28:50Z</updated>
    <link href="https://arxiv.org/abs/2512.12384v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12384v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Domain-adaptive pretraining (DAPT) offers a practical path to specializing large language models for high-value domains without full retraining. We conduct an early-stage scaling-law analysis of continued pretraining on U.S. SEC filings, training 1B and 3B-parameter Llama-3.2 models on a 400M-token financial corpus with validation checkpoints at 50M, 100M, 200M, and 400M tokens. Results show consistent improvements in SEC-domain validation loss for both models, with the largest gains occurring within the first 200M tokens and diminishing returns thereafter. Power-law fits reveal shallow exponents, indicating that financial language is highly regular and efficiently learnable under continued pretraining. General-domain validation loss remains effectively unchanged across all token budgets, suggesting minimal drift and no signs of catastrophic forgetting. A data-efficiency frontier further shows that both models move toward improved specialization with negligible mixed-domain degradation. Together, these findings provide early empirical guidance for scaling financial foundation models, suggesting that meaningful domain adaptation can be achieved with comparatively modest token budgets and that larger model scales (7B-70B) remain tractable under projected data requirements.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T16:28:50Z</published>
    <arxiv:comment>8 pages, 4 figures, 1 table</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Jesse Ponnock</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12381v1</id>
    <title>Entropy Collapse: A Universal Failure Mode of Intelligent Systems</title>
    <updated>2025-12-13T16:12:27Z</updated>
    <link href="https://arxiv.org/abs/2512.12381v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12381v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Intelligent systems are widely assumed to improve through learning, coordination, and optimization. However, across domains -- from artificial intelligence to economic institutions and biological evolution -- increasing intelligence often precipitates paradoxical degradation: systems become rigid, lose adaptability, and fail unexpectedly.
  We identify \emph{entropy collapse} as a universal dynamical failure mode arising when feedback amplification outpaces bounded novelty regeneration. Under minimal domain-agnostic assumptions, we show that intelligent systems undergo a sharp transition from high-entropy adaptive regimes to low-entropy collapsed regimes. Collapse is formalized as convergence toward a stable low-entropy manifold, not a zero-entropy state, implying a contraction of effective adaptive dimensionality rather than loss of activity or scale.
  We analytically establish critical thresholds, dynamical irreversibility, and attractor structure and demonstrate universality across update mechanisms through minimal simulations. This framework unifies diverse phenomena -- model collapse in AI, institutional sclerosis in economics, and genetic bottlenecks in evolution -- as manifestations of the same underlying process.
  By reframing collapse as a structural cost of intelligence, our results clarify why late-stage interventions systematically fail and motivate entropy-aware design principles for sustaining long-term adaptability in intelligent systems.
  \noindent\textbf{Keywords:} entropy collapse; intelligent systems; feedback amplification; phase transitions; effective dimensionality; complex systems; model collapse; institutional sclerosis</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T16:12:27Z</published>
    <arxiv:comment>18 pages, 5 figures</arxiv:comment>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Truong Xuan Khanh</name>
    </author>
    <author>
      <name>Truong Quynh Hoa</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12378v1</id>
    <title>M4Human: A Large-Scale Multimodal mmWave Radar Benchmark for Human Mesh Reconstruction</title>
    <updated>2025-12-13T16:08:59Z</updated>
    <link href="https://arxiv.org/abs/2512.12378v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12378v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Human mesh reconstruction (HMR) provides direct insights into body-environment interaction, which enables various immersive applications. While existing large-scale HMR datasets rely heavily on line-of-sight RGB input, vision-based sensing is limited by occlusion, lighting variation, and privacy concerns. To overcome these limitations, recent efforts have explored radio-frequency (RF) mmWave radar for privacy-preserving indoor human sensing. However, current radar datasets are constrained by sparse skeleton labels, limited scale, and simple in-place actions. To advance the HMR research community, we introduce M4Human, the current largest-scale (661K-frame) ($9\times$ prior largest) multimodal benchmark, featuring high-resolution mmWave radar, RGB, and depth data. M4Human provides both raw radar tensors (RT) and processed radar point clouds (RPC) to enable research across different levels of RF signal granularity. M4Human includes high-quality motion capture (MoCap) annotations with 3D meshes and global trajectories, and spans 20 subjects and 50 diverse actions, including in-place, sit-in-place, and free-space sports or rehabilitation movements. We establish benchmarks on both RT and RPC modalities, as well as multimodal fusion with RGB-D modalities. Extensive results highlight the significance of M4Human for radar-based human modeling while revealing persistent challenges under fast, unconstrained motion. The dataset and code will be released after the paper publication.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T16:08:59Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Junqiao Fan</name>
    </author>
    <author>
      <name>Yunjiao Zhou</name>
    </author>
    <author>
      <name>Yizhuo Yang</name>
    </author>
    <author>
      <name>Xinyuan Cui</name>
    </author>
    <author>
      <name>Jiarui Zhang</name>
    </author>
    <author>
      <name>Lihua Xie</name>
    </author>
    <author>
      <name>Jianfei Yang</name>
    </author>
    <author>
      <name>Chris Xiaoxuan Lu</name>
    </author>
    <author>
      <name>Fangqiang Ding</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12375v1</id>
    <title>V-Warper: Appearance-Consistent Video Diffusion Personalization via Value Warping</title>
    <updated>2025-12-13T16:05:52Z</updated>
    <link href="https://arxiv.org/abs/2512.12375v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12375v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Video personalization aims to generate videos that faithfully reflect a user-provided subject while following a text prompt. However, existing approaches often rely on heavy video-based finetuning or large-scale video datasets, which impose substantial computational cost and are difficult to scale. Furthermore, they still struggle to maintain fine-grained appearance consistency across frames. To address these limitations, we introduce V-Warper, a training-free coarse-to-fine personalization framework for transformer-based video diffusion models. The framework enhances fine-grained identity fidelity without requiring any additional video training. (1) A lightweight coarse appearance adaptation stage leverages only a small set of reference images, which are already required for the task. This step encodes global subject identity through image-only LoRA and subject-embedding adaptation. (2) A inference-time fine appearance injection stage refines visual fidelity by computing semantic correspondences from RoPE-free mid-layer query--key features. These correspondences guide the warping of appearance-rich value representations into semantically aligned regions of the generation process, with masking ensuring spatial reliability. V-Warper significantly improves appearance fidelity while preserving prompt alignment and motion dynamics, and it achieves these gains efficiently without large-scale video finetuning.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T16:05:52Z</published>
    <arxiv:comment>Project Page: https://cvlab-kaist.github.io/V-Warper</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Hyunkoo Lee</name>
    </author>
    <author>
      <name>Wooseok Jang</name>
    </author>
    <author>
      <name>Jini Yang</name>
    </author>
    <author>
      <name>Taehwan Kim</name>
    </author>
    <author>
      <name>Sangoh Kim</name>
    </author>
    <author>
      <name>Sangwon Jung</name>
    </author>
    <author>
      <name>Seungryong Kim</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12372v1</id>
    <title>STAGE: Storyboard-Anchored Generation for Cinematic Multi-shot Narrative</title>
    <updated>2025-12-13T15:57:29Z</updated>
    <link href="https://arxiv.org/abs/2512.12372v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12372v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>While recent advancements in generative models have achieved remarkable visual fidelity in video synthesis, creating coherent multi-shot narratives remains a significant challenge. To address this, keyframe-based approaches have emerged as a promising alternative to computationally intensive end-to-end methods, offering the advantages of fine-grained control and greater efficiency. However, these methods often fail to maintain cross-shot consistency and capture cinematic language. In this paper, we introduce STAGE, a SToryboard-Anchored GEneration workflow to reformulate the keyframe-based multi-shot video generation task. Instead of using sparse keyframes, we propose STEP2 to predict a structural storyboard composed of start-end frame pairs for each shot. We introduce the multi-shot memory pack to ensure long-range entity consistency, the dual-encoding strategy for intra-shot coherence, and the two-stage training scheme to learn cinematic inter-shot transition. We also contribute the large-scale ConStoryBoard dataset, including high-quality movie clips with fine-grained annotations for story progression, cinematic attributes, and human preferences. Extensive experiments demonstrate that STAGE achieves superior performance in structured narrative control and cross-shot coherence.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T15:57:29Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Peixuan Zhang</name>
    </author>
    <author>
      <name>Zijian Jia</name>
    </author>
    <author>
      <name>Kaiqi Liu</name>
    </author>
    <author>
      <name>Shuchen Weng</name>
    </author>
    <author>
      <name>Si Li</name>
    </author>
    <author>
      <name>Boxin Shi</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12367v1</id>
    <title>JPEG-Inspired Cloud-Edge Holography</title>
    <updated>2025-12-13T15:49:41Z</updated>
    <link href="https://arxiv.org/abs/2512.12367v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12367v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Computer-generated holography (CGH) presents a transformative solution for near-eye displays in augmented and virtual reality. Recent advances in deep learning have greatly improved CGH in reconstructed quality and computational efficiency. However, deploying neural CGH pipelines directly on compact, eyeglass-style devices is hindered by stringent constraints on computation and energy consumption, while cloud offloading followed by transmission with natural image codecs often distorts phase information and requires high bandwidth to maintain reconstruction quality. Neural compression methods can reduce bandwidth but impose heavy neural decoders at the edge, increasing inference latency and hardware demand. In this work, we introduce JPEG-Inspired Cloud-Edge Holography, an efficient pipeline designed around a learnable transform codec that retains the block-structured and hardware-friendly nature of JPEG. Our system shifts all heavy neural processing to the cloud, while the edge device performs only lightweight decoding without any neural inference. To further improve throughput, we implement custom CUDA kernels for entropy coding on both cloud and edge. This design achieves a peak signal-to-noise ratio of 32.15 dB at $&lt;$ 2 bits per pixel with decode latency as low as 4.2 ms. Both numerical simulations and optical experiments confirm the high reconstruction quality of the holograms. By aligning CGH with a codec that preserves JPEG's structural efficiency while extending it with learnable components, our framework enables low-latency, bandwidth-efficient hologram streaming on resource-constrained wearable devices-using only simple block-based decoding readily supported by modern system-on-chips, without requiring neural decoders or specialized hardware.</summary>
    <category term="physics.optics" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T15:49:41Z</published>
    <arxiv:primary_category term="physics.optics"/>
    <author>
      <name>Shuyang Xie</name>
    </author>
    <author>
      <name>Jie Zhou</name>
    </author>
    <author>
      <name>Jun Wang</name>
    </author>
    <author>
      <name>Renjing Xu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12366v1</id>
    <title>ElasticVR: Elastic Task Computing in Multi-User Multi-Connectivity Wireless Virtual Reality (VR) Systems</title>
    <updated>2025-12-13T15:29:55Z</updated>
    <link href="https://arxiv.org/abs/2512.12366v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12366v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Diverse emerging VR applications integrate streaming of high fidelity 360 video content that requires ample amounts of computation and data rate. Scalable 360 video tiling enables having elastic VR computational tasks that can be scaled adaptively in computation and data rate based on the available user and system resources. We integrate scalable 360 video tiling in an edge-client wireless multi-connectivity architecture for joint elastic task computation offloading across multiple VR users called ElasticVR. To balance the trade-offs in communication, computation, energy consumption, and QoE that arise herein, we formulate a constrained QoE and energy optimization problem that integrates the multi-user/multi-connectivity action space with the elasticity of VR computational tasks. The ElasticVR framework introduces two multi-agent deep reinforcement learning solutions, namely CPPG and IPPG. CPPG adopts a centralized training and centralized execution approach to capture the coupling between users' communication and computational demands. This leads to globally coordinated decisions at the cost of increased computational overheads and limited scalability. To address the latter challenges, we also explore an alternative strategy denoted IPPG that adopts a centralized training with decentralized execution paradigm. IPPG leverages shared information and parameter sharing to learn robust policies; however, during execution, each user takes action independently based on its local state information only. The decentralized execution alleviates the communication and computation overhead of centralized decision-making and improves scalability. We show that the ElasticVR framework improves the PSNR by 43.21%, while reducing the response time and energy consumption by 42.35% and 56.83%, respectively, compared with a case where no elasticity is incorporated into VR computations.</summary>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T15:29:55Z</published>
    <arxiv:comment>Submitted to ACM TOMM</arxiv:comment>
    <arxiv:primary_category term="cs.IT"/>
    <author>
      <name>Babak Badnava</name>
    </author>
    <author>
      <name>Jacob Chakareski</name>
    </author>
    <author>
      <name>Morteza Hashemi</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12365v1</id>
    <title>Synthetic Swarm Mosquito Dataset for Acoustic Classification: A Proof of Concept</title>
    <updated>2025-12-13T15:23:12Z</updated>
    <link href="https://arxiv.org/abs/2512.12365v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12365v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Mosquito-borne diseases pose a serious global health threat, causing over 700,000 deaths annually. This work introduces a proof-of-concept Synthetic Swarm Mosquito Dataset for Acoustic Classification, created to simulate realistic multi-species and noisy swarm conditions. Unlike conventional datasets that require labor-intensive recording of individual mosquitoes, the synthetic approach enables scalable data generation while reducing human resource demands. Using log-mel spectrograms, we evaluated lightweight deep learning architectures for the classification of mosquito species. Experiments show that these models can effectively identify six major mosquito vectors and are suitable for deployment on embedded low-power devices. The study demonstrates the potential of synthetic swarm audio datasets to accelerate acoustic mosquito research and enable scalable real-time surveillance solutions.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T15:23:12Z</published>
    <arxiv:comment>Accepted at RIVF 2025</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Thai-Duy Dinh</name>
    </author>
    <author>
      <name>Minh-Luan Vo</name>
    </author>
    <author>
      <name>Cuong Tuan Nguyen</name>
    </author>
    <author>
      <name>Bich-Hien Vo</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12360v1</id>
    <title>VideoARM: Agentic Reasoning over Hierarchical Memory for Long-Form Video Understanding</title>
    <updated>2025-12-13T15:11:03Z</updated>
    <link href="https://arxiv.org/abs/2512.12360v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12360v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Long-form video understanding remains challenging due to the extended temporal structure and dense multimodal cues. Despite recent progress, many existing approaches still rely on hand-crafted reasoning pipelines or employ token-consuming video preprocessing to guide MLLMs in autonomous reasoning. To overcome these limitations, we introduce VideoARM, an Agentic Reasoning-over-hierarchical-Memory paradigm for long-form video understanding. Instead of static, exhaustive preprocessing, VideoARM performs adaptive, on-the-fly agentic reasoning and memory construction. Specifically, VideoARM performs an adaptive and continuous loop of observing, thinking, acting, and memorizing, where a controller autonomously invokes tools to interpret the video in a coarse-to-fine manner, thereby substantially reducing token consumption. In parallel, a hierarchical multimodal memory continuously captures and updates multi-level clues throughout the operation of the agent, providing precise contextual information to support the controller in decision-making. Experiments on prevalent benchmarks demonstrate that VideoARM outperforms the state-of-the-art method, DVD, while significantly reducing token consumption for long-form videos.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T15:11:03Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Yufei Yin</name>
    </author>
    <author>
      <name>Qianke Meng</name>
    </author>
    <author>
      <name>Minghao Chen</name>
    </author>
    <author>
      <name>Jiajun Ding</name>
    </author>
    <author>
      <name>Zhenwei Shao</name>
    </author>
    <author>
      <name>Zhou Yu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12358v1</id>
    <title>Towards a pretrained deep learning estimator of the Linfoot informational correlation</title>
    <updated>2025-12-13T15:07:56Z</updated>
    <link href="https://arxiv.org/abs/2512.12358v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12358v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We develop a supervised deep-learning approach to estimate mutual information between two continuous random variables. As labels, we use the Linfoot informational correlation, a transformation of mutual information that has many important properties. Our method is based on ground truth labels for Gaussian and Clayton copulas. We compare our method with estimators based on kernel density, k-nearest neighbours and neural estimators. We show generally lower bias and lower variance. As a proof of principle, future research could look into training the model with a more diverse set of examples from other copulas for which ground truth labels are available.</summary>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T15:07:56Z</published>
    <arxiv:comment>3 figures</arxiv:comment>
    <arxiv:primary_category term="stat.ML"/>
    <author>
      <name>Stéphanie M. van den Berg</name>
    </author>
    <author>
      <name>Ulrich Halekoh</name>
    </author>
    <author>
      <name>Sören Möller</name>
    </author>
    <author>
      <name>Andreas Kryger Jensen</name>
    </author>
    <author>
      <name>Jacob von Bornemann Hjelmborg</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12357v1</id>
    <title>TCLeaf-Net: a transformer-convolution framework with global-local attention for robust in-field lesion-level plant leaf disease detection</title>
    <updated>2025-12-13T15:03:48Z</updated>
    <link href="https://arxiv.org/abs/2512.12357v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12357v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Timely and accurate detection of foliar diseases is vital for safeguarding crop growth and reducing yield losses. Yet, in real-field conditions, cluttered backgrounds, domain shifts, and limited lesion-level datasets hinder robust modeling. To address these challenges, we release Daylily-Leaf, a paired lesion-level dataset comprising 1,746 RGB images and 7,839 lesions captured under both ideal and in-field conditions, and propose TCLeaf-Net, a transformer-convolution hybrid detector optimized for real-field use. TCLeaf-Net is designed to tackle three major challenges. To mitigate interference from complex backgrounds, the transformer-convolution module (TCM) couples global context with locality-preserving convolution to suppress non-leaf regions. To reduce information loss during downsampling, the raw-scale feature recalling and sampling (RSFRS) block combines bilinear resampling and convolution to preserve fine spatial detail. To handle variations in lesion scale and feature shifts, the deformable alignment block with FPN (DFPN) employs offset-based alignment and multi-receptive-field perception to strengthen multi-scale fusion. Experimental results show that on the in-field split of the Daylily-Leaf dataset, TCLeaf-Net improves mAP@50 by 5.4 percentage points over the baseline model, reaching 78.2\%, while reducing computation by 7.5 GFLOPs and GPU memory usage by 8.7\%. Moreover, the model outperforms recent YOLO and RT-DETR series in both precision and recall, and demonstrates strong performance on the PlantDoc, Tomato-Leaf, and Rice-Leaf datasets, validating its robustness and generalizability to other plant disease detection scenarios.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T15:03:48Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Zishen Song</name>
    </author>
    <author>
      <name>Yongjian Zhu</name>
    </author>
    <author>
      <name>Dong Wang</name>
    </author>
    <author>
      <name>Hongzhan Liu</name>
    </author>
    <author>
      <name>Lingyu Jiang</name>
    </author>
    <author>
      <name>Yongxing Duan</name>
    </author>
    <author>
      <name>Zehua Zhang</name>
    </author>
    <author>
      <name>Sihan Li</name>
    </author>
    <author>
      <name>Jiarui Li</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12341v1</id>
    <title>Uncertainty Quantification for Machine Learning: One Size Does Not Fit All</title>
    <updated>2025-12-13T14:15:04Z</updated>
    <link href="https://arxiv.org/abs/2512.12341v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12341v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Proper quantification of predictive uncertainty is essential for the use of machine learning in safety-critical applications. Various uncertainty measures have been proposed for this purpose, typically claiming superiority over other measures. In this paper, we argue that there is no single best measure. Instead, uncertainty quantification should be tailored to the specific application. To this end, we use a flexible family of uncertainty measures that distinguishes between total, aleatoric, and epistemic uncertainty of second-order distributions. These measures can be instantiated with specific loss functions, so-called proper scoring rules, to control their characteristics, and we show that different characteristics are useful for different tasks. In particular, we show that, for the task of selective prediction, the scoring rule should ideally match the task loss. On the other hand, for out-of-distribution detection, our results confirm that mutual information, a widely used measure of epistemic uncertainty, performs best. Furthermore, in an active learning setting, epistemic uncertainty based on zero-one loss is shown to consistently outperform other uncertainty measures.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T14:15:04Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Paul Hofman</name>
    </author>
    <author>
      <name>Yusuf Sale</name>
    </author>
    <author>
      <name>Eyke Hüllermeier</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12339v1</id>
    <title>Unified Control for Inference-Time Guidance of Denoising Diffusion Models</title>
    <updated>2025-12-13T14:12:10Z</updated>
    <link href="https://arxiv.org/abs/2512.12339v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12339v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Aligning diffusion model outputs with downstream objectives is essential for improving task-specific performance. Broadly, inference-time training-free approaches for aligning diffusion models can be categorized into two main strategies: sampling-based methods, which explore multiple candidate outputs and select those with higher reward signals, and gradient-guided methods, which use differentiable reward approximations to directly steer the generation process. In this work, we propose a universal algorithm, UniCoDe, which brings together the strengths of sampling and gradient-based guidance into a unified framework. UniCoDe integrates local gradient signals during sampling, thereby addressing the sampling inefficiency inherent in complex reward-based sampling approaches. By cohesively combining these two paradigms, UniCoDe enables more efficient sampling while offering better trade-offs between reward alignment and divergence from the diffusion unconditional prior. Empirical results demonstrate that UniCoDe remains competitive with state-of-the-art baselines across a range of tasks. The code is available at https://github.com/maurya-goyal10/UniCoDe</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T14:12:10Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Maurya Goyal</name>
    </author>
    <author>
      <name>Anuj Singh</name>
    </author>
    <author>
      <name>Hadi Jamali-Rad</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12337v1</id>
    <title>SCIR: A Self-Correcting Iterative Refinement Framework for Enhanced Information Extraction Based on Schema</title>
    <updated>2025-12-13T14:07:25Z</updated>
    <link href="https://arxiv.org/abs/2512.12337v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12337v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Although Large language Model (LLM)-powered information extraction (IE) systems have shown impressive capabilities, current fine-tuning paradigms face two major limitations: high training costs and difficulties in aligning with LLM preferences. To address these issues, we propose a novel universal IE paradigm, the Self-Correcting Iterative Refinement (SCIR) framework, along with a Multi-task Bilingual (Chinese-English) Self-Correcting (MBSC) dataset containing over 100,000 entries. The SCIR framework achieves plug-and-play compatibility with existing LLMs and IE systems through its Dual-Path Self-Correcting module and feedback-driven optimization, thereby significantly reducing training costs. Concurrently, the MBSC dataset tackles the challenge of preference alignment by indirectly distilling GPT-4's capabilities into IE result detection models. Experimental results demonstrate that SCIR outperforms state-of-the-art IE methods across three key tasks: named entity recognition, relation extraction, and event extraction, achieving a 5.27 percent average improvement in span-based Micro-F1 while reducing training costs by 87 percent compared to baseline approaches. These advancements not only enhance the flexibility and accuracy of IE systems but also pave the way for lightweight and efficient IE paradigms.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T14:07:25Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Yushen Fang</name>
    </author>
    <author>
      <name>Jianjun Li</name>
    </author>
    <author>
      <name>Mingqian Ding</name>
    </author>
    <author>
      <name>Chang Liu</name>
    </author>
    <author>
      <name>Xinchi Zou</name>
    </author>
    <author>
      <name>Wenqi Yang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12334v1</id>
    <title>Extending the application of dynamic Bayesian networks in calculating market risk: Standard and stressed expected shortfall</title>
    <updated>2025-12-13T13:55:28Z</updated>
    <link href="https://arxiv.org/abs/2512.12334v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12334v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>In the last five years, expected shortfall (ES) and stressed ES (SES) have become key required regulatory measures of market risk in the banking sector, especially following events such as the global financial crisis. Thus, finding ways to optimize their estimation is of great importance. We extend the application of dynamic Bayesian networks (DBNs) to the estimation of 10-day 97.5% ES and stressed ES, building on prior work applying DBNs to value at risk. Using the S&amp;P 500 index as a proxy for the equities trading desk of a US bank, we compare the performance of three DBN structure-learning algorithms with several traditional market risk models, using either the normal or the skewed Student's t return distributions. Backtesting shows that all models fail to produce statistically accurate ES and SES forecasts at the 2.5% level, reflecting the difficulty of modeling extreme tail behavior. For ES, the EGARCH(1,1) model (normal) produces the most accurate forecasts, while, for SES, the GARCH(1,1) model (normal) performs best. All distribution-dependent models deteriorate substantially when using the skewed Student's t distribution. The DBNs perform comparably to the historical simulation model, but their contribution to tail prediction is limited by the small weight assigned to their one-day-ahead forecasts within the return distribution. Future research should examine weighting schemes that enhance the influence of forward-looking DBN forecasts on tail risk estimation.</summary>
    <category term="q-fin.RM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.CP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.ST" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T13:55:28Z</published>
    <arxiv:primary_category term="q-fin.RM"/>
    <author>
      <name>Eden Gross</name>
    </author>
    <author>
      <name>Ryan Kruger</name>
    </author>
    <author>
      <name>Francois Toerien</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12332v1</id>
    <title>Dynamic Homophily with Imperfect Recall: Modeling Resilience in Adversarial Networks</title>
    <updated>2025-12-13T13:45:27Z</updated>
    <link href="https://arxiv.org/abs/2512.12332v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12332v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The purpose of this study is to investigate how homophily, memory constraints, and adversarial disruptions collectively shape the resilience and adaptability of complex networks. To achieve this, we develop a new framework that integrates explicit memory decay mechanisms into homophily-based models and systematically evaluate their performance across diverse graph structures and adversarial settings. Our methods involve extensive experimentation on synthetic datasets, where we vary decay functions, reconnection probabilities, and similarity measures, primarily comparing cosine similarity with traditional metrics such as Jaccard similarity and baseline edge weights. The results show that cosine similarity achieves up to a 30\% improvement in stability metrics in sparse, convex, and modular networks. Moreover, the refined value-of-recall metric demonstrates that strategic forgetting can bolster resilience by balancing network robustness and adaptability. The findings underscore the critical importance of aligning memory and similarity parameters with the structural and adversarial dynamics of the network. By quantifying the tangible benefits of incorporating memory constraints into homophily-based analyses, this study offers actionable insights for optimizing real-world applications, including social systems, collaborative platforms, and cybersecurity contexts.</summary>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T13:45:27Z</published>
    <arxiv:primary_category term="cs.SI"/>
    <author>
      <name>Saad Alqithami</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12326v1</id>
    <title>The Role of AI in Modern Penetration Testing</title>
    <updated>2025-12-13T13:34:31Z</updated>
    <link href="https://arxiv.org/abs/2512.12326v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12326v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Penetration testing is a cornerstone of cybersecurity, traditionally driven by manual, time-intensive processes. As systems grow in complexity, there is a pressing need for more scalable and efficient testing methodologies. This systematic literature review examines how Artificial Intelligence (AI) is reshaping penetration testing, analyzing 58 peer-reviewed studies from major academic databases. Our findings reveal that while AI-assisted pentesting is still in its early stages, notable progress is underway, particularly through Reinforcement Learning (RL), which was the focus of 77% of the reviewed works. Most research centers on the discovery and exploitation phases of pentesting, where AI shows the greatest promise in automating repetitive tasks, optimizing attack strategies, and improving vulnerability identification. Real-world applications remain limited but encouraging, including the European Space Agency's PenBox and various open-source tools. These demonstrate AI's potential to streamline attack path analysis, analyze complex network topology, and reduce manual workload. However, challenges persist: current models often lack flexibility and are underdeveloped for the reconnaissance and post-exploitation phases of pentesting. Applications involving Large Language Models (LLMs) remain relatively under-researched, pointing to a promising direction for future exploration. This paper offers a critical overview of AI's current and potential role in penetration testing, providing valuable insights for researchers, practitioners, and organizations aiming to enhance security assessments through advanced automation or looking for gaps in existing research.</summary>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T13:34:31Z</published>
    <arxiv:primary_category term="cs.SE"/>
    <author>
      <name>J. Alexander Curtis</name>
    </author>
    <author>
      <name>Nasir U. Eisty</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12325v1</id>
    <title>Eventually LIL Regret: Almost Sure $\ln\ln T$ Regret for a sub-Gaussian Mixture on Unbounded Data</title>
    <updated>2025-12-13T13:34:03Z</updated>
    <link href="https://arxiv.org/abs/2512.12325v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12325v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We prove that a classic sub-Gaussian mixture proposed by Robbins in a stochastic setting actually satisfies a path-wise (deterministic) regret bound. For every path in a natural ``Ville event'' $E_α$, this regret till time $T$ is bounded by $\ln^2(1/α)/V_T + \ln (1/α) + \ln \ln V_T$ up to universal constants, where $V_T$ is a nonnegative, nondecreasing, cumulative variance process. (The bound reduces to $\ln(1/α) + \ln \ln V_T$ if $V_T \geq \ln(1/α)$.) If the data were stochastic, then one can show that $E_α$ has probability at least $1-α$ under a wide class of distributions (eg: sub-Gaussian, symmetric, variance-bounded, etc.). In fact, we show that on the Ville event $E_0$ of probability one, the regret on every path in $E_0$ is eventually bounded by $\ln \ln V_T$ (up to constants). We explain how this work helps bridge the world of adversarial online learning (which usually deals with regret bounds for bounded data), with game-theoretic statistics (which can handle unbounded data, albeit using stochastic assumptions). In short, conditional regret bounds serve as a bridge between stochastic and adversarial betting.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T13:34:03Z</published>
    <arxiv:comment>24 pages</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Shubhada Agrawal</name>
    </author>
    <author>
      <name>Aaditya Ramdas</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12324v1</id>
    <title>UniMark: Artificial Intelligence Generated Content Identification Toolkit</title>
    <updated>2025-12-13T13:30:48Z</updated>
    <link href="https://arxiv.org/abs/2512.12324v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12324v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The rapid proliferation of Artificial Intelligence Generated Content has precipitated a crisis of trust and urgent regulatory demands. However, existing identification tools suffer from fragmentation and a lack of support for visible compliance marking. To address these gaps, we introduce the \textbf{UniMark}, an open-source, unified framework for multimodal content governance. Our system features a modular unified engine that abstracts complexities across text, image, audio, and video modalities. Crucially, we propose a novel dual-operation strategy, natively supporting both \emph{Hidden Watermarking} for copyright protection and \emph{Visible Marking} for regulatory compliance. Furthermore, we establish a standardized evaluation framework with three specialized benchmarks (Image/Video/Audio-Bench) to ensure rigorous performance assessment. This toolkit bridges the gap between advanced algorithms and engineering implementation, fostering a more transparent and secure digital ecosystem.</summary>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T13:30:48Z</published>
    <arxiv:comment>5 Pages</arxiv:comment>
    <arxiv:primary_category term="cs.CR"/>
    <author>
      <name>Meilin Li</name>
    </author>
    <author>
      <name>Ji He</name>
    </author>
    <author>
      <name>Jia Xu</name>
    </author>
    <author>
      <name>Shanzhe Lei</name>
    </author>
    <author>
      <name>Yan Teng</name>
    </author>
    <author>
      <name>Yingchun Wang</name>
    </author>
    <author>
      <name>Xuhong Wang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12314v1</id>
    <title>Evaluating Asynchronous Semantics in Trace-Discovered Resilience Models: A Case Study on the OpenTelemetry Demo</title>
    <updated>2025-12-13T13:08:05Z</updated>
    <link href="https://arxiv.org/abs/2512.12314v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12314v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>While distributed tracing and chaos engineering are becoming standard for microservices, resilience models remain largely manual and bespoke. We revisit a trace-discovered connectivity model that derives a service dependency graph from traces and uses Monte Carlo simulation to estimate endpoint availability under fail-stop service failures. Compared to earlier work, we (i) derive the graph directly from raw OpenTelemetry traces, (ii) attach endpoint-specific success predicates, and (iii) add a simple asynchronous semantics that treats Kafka edges as non-blocking for immediate HTTP success. We apply this model to the OpenTelemetry Demo ("Astronomy Shop") using a GitHub Actions workflow that discovers the graph, runs simulations, and executes chaos experiments that randomly kill microservices in a Docker Compose deployment. Across the studied failure fractions, the model reproduces the overall availability degradation curve, while asynchronous semantics for Kafka edges change predicted availabilities by at most about 10^(-5) (0.001 percentage points). This null result suggests that for immediate HTTP availability in this case study, explicitly modeling asynchronous dependencies is not warranted, and a simpler connectivity-only model is sufficient.</summary>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T13:08:05Z</published>
    <arxiv:primary_category term="cs.SE"/>
    <author>
      <name>Anatoly A. Krasnovsky</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12313v1</id>
    <title>Taint-Based Code Slicing for LLMs-based Malicious NPM Package Detection</title>
    <updated>2025-12-13T12:56:03Z</updated>
    <link href="https://arxiv.org/abs/2512.12313v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12313v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The increasing sophistication of malware attacks in the npm ecosystem, characterized by obfuscation and complex logic, necessitates advanced detection methods. Recently, researchers have turned their attention from traditional detection approaches to Large Language Models (LLMs) due to their strong capabilities in semantic code understanding. However, while LLMs offer superior semantic reasoning for code analysis, their practical application is constrained by limited context windows and high computational cost. This paper addresses this challenge by introducing a novel framework that leverages code slicing techniques for an LLM-based malicious package detection task. We propose a specialized taintbased slicing technique for npm packages, augmented by a heuristic backtracking mechanism to accurately capture malicious data flows across asynchronous, event-driven patterns (e.g., callbacks and Promises) that elude traditional analysis. An evaluation on a dataset of more than 5000 malicious and benign npm packages demonstrates that our approach isolates security-relevant code, reducing input volume by over 99% while preserving critical behavioral semantics. Using the DeepSeek-Coder-6.7B model as the classification engine, our approach achieves a detection accuracy of 87.04%, substantially outperforming a naive token-splitting baseline (75.41%) and a traditional static-analysis-based approach. These results indicate that semantically optimized input representation via code slicing not only mitigates the LLM context-window bottleneck but also significantly enhances reasoning precision for security tasks, providing an efficient and effective defense against evolving malicious open-source packages.</summary>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T12:56:03Z</published>
    <arxiv:comment>17 pages, 4 figures, 9 tables</arxiv:comment>
    <arxiv:primary_category term="cs.CR"/>
    <author>
      <name>Dang-Khoa Nguyen</name>
    </author>
    <author>
      <name>Gia-Thang Ho</name>
    </author>
    <author>
      <name>Quang-Minh Pham</name>
    </author>
    <author>
      <name>Tuyet A. Dang-Thi</name>
    </author>
    <author>
      <name>Minh-Khanh Vu</name>
    </author>
    <author>
      <name>Thanh-Cong Nguyen</name>
    </author>
    <author>
      <name>Phat T. Tran-Truong</name>
    </author>
    <author>
      <name>Duc-Ly Vu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12309v1</id>
    <title>WeDetect: Fast Open-Vocabulary Object Detection as Retrieval</title>
    <updated>2025-12-13T12:40:28Z</updated>
    <link href="https://arxiv.org/abs/2512.12309v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12309v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Open-vocabulary object detection aims to detect arbitrary classes via text prompts. Methods without cross-modal fusion layers (non-fusion) offer faster inference by treating recognition as a retrieval problem, \ie, matching regions to text queries in a shared embedding space. In this work, we fully explore this retrieval philosophy and demonstrate its unique advantages in efficiency and versatility through a model family named WeDetect: (1) State-of-the-art performance. WeDetect is a real-time detector with a dual-tower architecture. We show that, with well-curated data and full training, the non-fusion WeDetect surpasses other fusion models and establishes a strong open-vocabulary foundation. (2) Fast backtrack of historical data. WeDetect-Uni is a universal proposal generator based on WeDetect. We freeze the entire detector and only finetune an objectness prompt to retrieve generic object proposals across categories. Importantly, the proposal embeddings are class-specific and enable a new application, object retrieval, supporting retrieval objects in historical data. (3) Integration with LMMs for referring expression comprehension (REC). We further propose WeDetect-Ref, an LMM-based object classifier to handle complex referring expressions, which retrieves target objects from the proposal list extracted by WeDetect-Uni. It discards next-token prediction and classifies objects in a single forward pass. Together, the WeDetect family unifies detection, proposal generation, object retrieval, and REC under a coherent retrieval framework, achieving state-of-the-art performance across 15 benchmarks with high inference efficiency.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T12:40:28Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Shenghao Fu</name>
    </author>
    <author>
      <name>Yukun Su</name>
    </author>
    <author>
      <name>Fengyun Rao</name>
    </author>
    <author>
      <name>Jing Lyu</name>
    </author>
    <author>
      <name>Xiaohua Xie</name>
    </author>
    <author>
      <name>Wei-Shi Zheng</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12307v1</id>
    <title>MRD: Using Physically Based Differentiable Rendering to Probe Vision Models for 3D Scene Understanding</title>
    <updated>2025-12-13T12:26:57Z</updated>
    <link href="https://arxiv.org/abs/2512.12307v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12307v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>While deep learning methods have achieved impressive success in many vision benchmarks, it remains difficult to understand and explain the representations and decisions of these models. Though vision models are typically trained on 2D inputs, they are often assumed to develop an implicit representation of the underlying 3D scene (for example, showing tolerance to partial occlusion, or the ability to reason about relative depth). Here, we introduce MRD (metamers rendered differentiably), an approach that uses physically based differentiable rendering to probe vision models' implicit understanding of generative 3D scene properties, by finding 3D scene parameters that are physically different but produce the same model activation (i.e. are model metamers). Unlike previous pixel-based methods for evaluating model representations, these reconstruction results are always grounded in physical scene descriptions. This means we can, for example, probe a model's sensitivity to object shape while holding material and lighting constant. As a proof-of-principle, we assess multiple models in their ability to recover scene parameters of geometry (shape) and bidirectional reflectance distribution function (material). The results show high similarity in model activation between target and optimized scenes, with varying visual results. Qualitatively, these reconstructions help investigate the physical scene attributes to which models are sensitive or invariant. MRD holds promise for advancing our understanding of both computer and human vision by enabling analysis of how physical scene parameters drive changes in model responses.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T12:26:57Z</published>
    <arxiv:comment>18 pages, 6 figures. Supplementary material and code will be provided at the end of January</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Benjamin Beilharz</name>
    </author>
    <author>
      <name>Thomas S. A. Wallis</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12303v1</id>
    <title>OMUDA: Omni-level Masking for Unsupervised Domain Adaptation in Semantic Segmentation</title>
    <updated>2025-12-13T12:01:23Z</updated>
    <link href="https://arxiv.org/abs/2512.12303v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12303v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Unsupervised domain adaptation (UDA) enables semantic segmentation models to generalize from a labeled source domain to an unlabeled target domain. However, existing UDA methods still struggle to bridge the domain gap due to cross-domain contextual ambiguity, inconsistent feature representations, and class-wise pseudo-label noise. To address these challenges, we propose Omni-level Masking for Unsupervised Domain Adaptation (OMUDA), a unified framework that introduces hierarchical masking strategies across distinct representation levels. Specifically, OMUDA comprises: 1) a Context-Aware Masking (CAM) strategy that adaptively distinguishes foreground from background to balance global context and local details; 2) a Feature Distillation Masking (FDM) strategy that enhances robust and consistent feature learning through knowledge transfer from pre-trained models; and 3) a Class Decoupling Masking (CDM) strategy that mitigates the impact of noisy pseudo-labels by explicitly modeling class-wise uncertainty. This hierarchical masking paradigm effectively reduces the domain shift at the contextual, representational, and categorical levels, providing a unified solution beyond existing approaches. Extensive experiments on multiple challenging cross-domain semantic segmentation benchmarks validate the effectiveness of OMUDA. Notably, on the SYNTHIA-&gt;Cityscapes and GTA5-&gt;Cityscapes tasks, OMUDA can be seamlessly integrated into existing UDA methods and consistently achieving state-of-the-art results with an average improvement of 7%.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T12:01:23Z</published>
    <arxiv:comment>Submitted to TMM</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Yang Ou</name>
    </author>
    <author>
      <name>Xiongwei Zhao</name>
    </author>
    <author>
      <name>Xinye Yang</name>
    </author>
    <author>
      <name>Yihan Wang</name>
    </author>
    <author>
      <name>Yicheng Di</name>
    </author>
    <author>
      <name>Rong Yuan</name>
    </author>
    <author>
      <name>Xieyuanli Chen</name>
    </author>
    <author>
      <name>Xu Zhu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12302v1</id>
    <title>From Human Intention to Action Prediction: A Comprehensive Benchmark for Intention-driven End-to-End Autonomous Driving</title>
    <updated>2025-12-13T11:59:51Z</updated>
    <link href="https://arxiv.org/abs/2512.12302v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12302v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Current end-to-end autonomous driving systems operate at a level of intelligence akin to following simple steering commands. However, achieving genuinely intelligent autonomy requires a paradigm shift: moving from merely executing low-level instructions to understanding and fulfilling high-level, abstract human intentions. This leap from a command-follower to an intention-fulfiller, as illustrated in our conceptual framework, is hindered by a fundamental challenge: the absence of a standardized benchmark to measure and drive progress on this complex task. To address this critical gap, we introduce Intention-Drive, the first comprehensive benchmark designed to evaluate the ability to translate high-level human intent into safe and precise driving actions. Intention-Drive features two core contributions: (1) a new dataset of complex scenarios paired with corresponding natural language intentions, and (2) a novel evaluation protocol centered on the Intent Success Rate (ISR), which assesses the semantic fulfillment of the human's goal beyond simple geometric accuracy. Through an extensive evaluation of a spectrum of baseline models on Intention-Drive, we reveal a significant performance deficit, showing that the baseline model struggle to achieve the comprehensive scene and intention understanding required for this advanced task.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T11:59:51Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Huan Zheng</name>
    </author>
    <author>
      <name>Yucheng Zhou</name>
    </author>
    <author>
      <name>Tianyi Yan</name>
    </author>
    <author>
      <name>Jiayi Su</name>
    </author>
    <author>
      <name>Hongjun Chen</name>
    </author>
    <author>
      <name>Dubing Chen</name>
    </author>
    <author>
      <name>Wencheng Han</name>
    </author>
    <author>
      <name>Runzhou Tao</name>
    </author>
    <author>
      <name>Zhongying Qiu</name>
    </author>
    <author>
      <name>Jianfei Yang</name>
    </author>
    <author>
      <name>Jianbing Shen</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12301v1</id>
    <title>TwinFormer: A Dual-Level Transformer for Long-Sequence Time-Series Forecasting</title>
    <updated>2025-12-13T11:50:18Z</updated>
    <link href="https://arxiv.org/abs/2512.12301v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12301v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>TwinFormer is a hierarchical Transformer for long-sequence time-series forecasting. It divides the input into non-overlapping temporal patches and processes them in two stages: (1) a Local Informer with top-$k$ Sparse Attention models intra-patch dynamics, followed by mean pooling; (2) a Global Informer captures long-range inter-patch dependencies using the same top-$k$ attention. A lightweight GRU aggregates the globally contextualized patch tokens for direct multi-horizon prediction. The resulting architecture achieves linear $O(kLd)$ time and memory complexity. On eight real-world benchmarking datasets from six different domains, including weather, stock price, temperature, power consumption, electricity, and disease, and forecasting horizons $96-720$, TwinFormer secures $27$ positions in the top two out of $34$. Out of the $27$, it achieves the best performance on MAE and RMSE at $17$ places and $10$ at the second-best place on MAE and RMSE. This consistently outperforms PatchTST, iTransformer, FEDformer, Informer, and vanilla Transformers. Ablations confirm the superiority of top-$k$ Sparse Attention over ProbSparse and the effectiveness of GRU-based aggregation. Code is available at this repository: https://github.com/Mahimakumavat1205/TwinFormer.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T11:50:18Z</published>
    <arxiv:comment>14 pages, 4 figures</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Mahima Kumavat</name>
    </author>
    <author>
      <name>Aditya Maheshwari</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12299v1</id>
    <title>A Conflict-Aware Resource Management Framework for the Computing Continuum</title>
    <updated>2025-12-13T11:44:23Z</updated>
    <link href="https://arxiv.org/abs/2512.12299v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12299v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The increasing device heterogeneity and decentralization requirements in the computing continuum (i.e., spanning edge, fog, and cloud) introduce new challenges in resource orchestration. In such environments, agents are often responsible for optimizing resource usage across deployed services. However, agent decisions can lead to persistent conflict loops, inefficient resource utilization, and degraded service performance. To overcome such challenges, we propose a novel framework for adaptive conflict resolution in resource-oriented orchestration using a Deep Reinforcement Learning (DRL) approach. The framework enables handling resource conflicts across deployments and integrates a DRL model trained to mediate such conflicts based on real-time performance feedback and historical state information. The framework has been prototyped and validated on a Kubernetes-based testbed, illustrating its methodological feasibility and architectural resilience. Preliminary results show that the framework achieves efficient resource reallocation and adaptive learning in dynamic scenarios, thus providing a scalable and resilient solution for conflict-aware orchestration in the computing continuum.</summary>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T11:44:23Z</published>
    <arxiv:primary_category term="cs.DC"/>
    <author>
      <name>Vlad Popescu-Vifor</name>
    </author>
    <author>
      <name>Ilir Murturi</name>
    </author>
    <author>
      <name>Praveen Kumar Donta</name>
    </author>
    <author>
      <name>Schahram Dustdar</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12297v1</id>
    <title>F5-TTS-RO: Extending F5-TTS to Romanian TTS via Lightweight Input Adaptation</title>
    <updated>2025-12-13T11:41:54Z</updated>
    <link href="https://arxiv.org/abs/2512.12297v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12297v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>This work introduces a lightweight input-level adapter for the F5-TTS model that enables Romanian Language support. To preserve the existing capabilities of the model (voice cloning, English and Chinese support), we keep the original weights frozen, append a sub-network to the model and train it as an extension for the textual embedding matrix of the text encoder. For simplicity, we rely on ConvNeXt module implemented in F5-TTS to also model the co-dependencies between the new character-level embeddings. The module serves as a ``soft`` letter-to-sound layer, converting Romanian text into a continuous representation that the F5-TTS model uses to produce naturally sounding Romanian utterances. We evaluate the model with a pool of 20 human listeners across three tasks: (a) audio similarity between reference and generated speech, (b) pronunciation and naturalness and (c) Romanian-English code-switching. The results indicate that our approach maintains voice cloning capabilities and enables, to a certain extent, code-switching within the same utterance; however, residual English accent characteristics remain. We open-source our code and provide example audio samples at https://github.com/racai-ro/Ro-F5TTS.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T11:41:54Z</published>
    <arxiv:comment>Accepted at The 20th International Conference on Linguistic Resources and Tools for Natural Language Processing</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Radu-Gabriel Chivereanu</name>
    </author>
    <author>
      <name>Tiberiu Boros</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12296v1</id>
    <title>GrowTAS: Progressive Expansion from Small to Large Subnets for Efficient ViT Architecture Search</title>
    <updated>2025-12-13T11:40:21Z</updated>
    <link href="https://arxiv.org/abs/2512.12296v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12296v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Transformer architecture search (TAS) aims to automatically discover efficient vision transformers (ViTs), reducing the need for manual design. Existing TAS methods typically train an over-parameterized network (i.e., a supernet) that encompasses all candidate architectures (i.e., subnets). However, all subnets share the same set of weights, which leads to interference that degrades the smaller subnets severely. We have found that well-trained small subnets can serve as a good foundation for training larger ones. Motivated by this, we propose a progressive training framework, dubbed GrowTAS, that begins with training small subnets and incorporate larger ones gradually. This enables reducing the interference and stabilizing a training process. We also introduce GrowTAS+ that fine-tunes a subset of weights only to further enhance the performance of large subnets. Extensive experiments on ImageNet and several transfer learning benchmarks, including CIFAR-10/100, Flowers, CARS, and INAT-19, demonstrate the effectiveness of our approach over current TAS methods</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T11:40:21Z</published>
    <arxiv:comment>Accepted to WACV 2026</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Hyunju Lee</name>
    </author>
    <author>
      <name>Youngmin Oh</name>
    </author>
    <author>
      <name>Jeimin Jeon</name>
    </author>
    <author>
      <name>Donghyeon Baek</name>
    </author>
    <author>
      <name>Bumsub Ham</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12295v1</id>
    <title>Near-Zero-Overhead Freshness for Recommendation Systems via Inference-Side Model Updates</title>
    <updated>2025-12-13T11:38:05Z</updated>
    <link href="https://arxiv.org/abs/2512.12295v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12295v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Deep Learning Recommendation Models (DLRMs) underpin personalized services but face a critical freshness-accuracy tradeoff due to massive parameter synchronization overheads. Production DLRMs deploy decoupled training/inference clusters, where synchronizing petabyte-scale embedding tables (EMTs) causes multi-minute staleness, degrading recommendation quality and revenue. We observe that (1) inference nodes exhibit sustained CPU underutilization (peak &lt;= 20%), and (2) EMT gradients possess intrinsic low-rank structure, enabling compact update representation. We present LiveUpdate, a system that eliminates inter-cluster synchronization by colocating Low-Rank Adaptation (LoRA) trainers within inference nodes. LiveUpdate addresses two core challenges: (1) dynamic rank adaptation via singular value monitoring to constrain memory overhead (&lt;2% of EMTs), and (2) NUMA-aware resource scheduling with hardware-enforced QoS to eliminate update inference contention (P99 latency impact &lt;20ms). Evaluations show LiveUpdate reduces update costs by 2x versus delta-update baselines while achieving higher accuracy within 1-hour windows. By transforming idle inference resources into freshness engines, LiveUpdate delivers online model updates while outperforming state-of-the-art delta-update methods by 0.04% to 0.24% in accuracy.</summary>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T11:38:05Z</published>
    <arxiv:comment>13 Pages, 19 figures</arxiv:comment>
    <arxiv:primary_category term="cs.DC"/>
    <author>
      <name>Wenjun Yu</name>
    </author>
    <author>
      <name>Sitian Chen</name>
    </author>
    <author>
      <name>Cheng Chen</name>
    </author>
    <author>
      <name>Amelie Chi Zhou</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12289v1</id>
    <title>Robust Outlier Detection and Low-Latency Concept Drift Adaptation for Data Stream Regression: A Dual-Channel Architecture</title>
    <updated>2025-12-13T11:17:47Z</updated>
    <link href="https://arxiv.org/abs/2512.12289v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12289v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Outlier detection and concept drift detection represent two challenges in data analysis. Most studies address these issues separately. However, joint detection mechanisms in regression remain underexplored, where the continuous nature of output spaces makes distinguishing drifts from outliers inherently challenging. To address this, we propose a novel robust regression framework for joint outlier and concept drift detection. Specifically, we introduce a dual-channel decision process that orchestrates prediction residuals into two coupled logic flows: a rapid response channel for filtering point outliers and a deep analysis channel for diagnosing drifts. We further develop the Exponentially Weighted Moving Absolute Deviation with Distinguishable Types (EWMAD-DT) detector to autonomously differentiate between abrupt and incremental drifts via dynamic thresholding. Comprehensive experiments on both synthetic and real-world datasets demonstrate that our unified framework, enhanced by EWMAD-DT, exhibits superior detection performance even when point outliers and concept drifts coexist.</summary>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-13T11:17:47Z</published>
    <arxiv:primary_category term="stat.ME"/>
    <author>
      <name>Bingbing Wang</name>
    </author>
    <author>
      <name>Shengyan Sun</name>
    </author>
    <author>
      <name>Jiaqi Wang</name>
    </author>
    <author>
      <name>Yu Tang</name>
    </author>
  </entry>
</feed>
