<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/" xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns="http://www.w3.org/2005/Atom">
  <id>https://arxiv.org/api/41L03a4LGubvYaYXRxW9OkWYviQ</id>
  <title>arXiv Query: search_query=cat:cs.AI OR cat:cs.LG OR cat:cs.CL OR cat:cs.CV OR cat:cs.NE OR cat:cs.SE OR cat:cs.CR OR cat:cs.DC&amp;id_list=&amp;start=0&amp;max_results=100</title>
  <updated>2025-12-16T13:50:15Z</updated>
  <link href="https://arxiv.org/api/query?search_query=cat:cs.AI+OR+(cat:cs.LG+OR+(cat:cs.CL+OR+(cat:cs.CV+OR+(cat:cs.NE+OR+(cat:cs.SE+OR+(cat:cs.CR+OR+cat:cs.DC))))))&amp;start=0&amp;max_results=100&amp;id_list=" type="application/atom+xml"/>
  <opensearch:itemsPerPage>100</opensearch:itemsPerPage>
  <opensearch:totalResults>566020</opensearch:totalResults>
  <opensearch:startIndex>0</opensearch:startIndex>
  <entry>
    <id>http://arxiv.org/abs/2512.13690v1</id>
    <title>DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders</title>
    <updated>2025-12-15T18:59:57Z</updated>
    <link href="https://arxiv.org/abs/2512.13690v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13690v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Video diffusion models have revolutionized generative video synthesis, but they are imprecise, slow, and can be opaque during generation -- keeping users in the dark for a prolonged period. In this work, we propose DiffusionBrowser, a model-agnostic, lightweight decoder framework that allows users to interactively generate previews at any point (timestep or transformer block) during the denoising process. Our model can generate multi-modal preview representations that include RGB and scene intrinsics at more than 4$\times$ real-time speed (less than 1 second for a 4-second video) that convey consistent appearance and motion to the final video. With the trained decoder, we show that it is possible to interactively guide the generation at intermediate noise steps via stochasticity reinjection and modal steering, unlocking a new control capability. Moreover, we systematically probe the model using the learned decoders, revealing how scene, object, and other details are composed and assembled during the otherwise black-box denoising process.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T18:59:57Z</published>
    <arxiv:comment>Project page: https://susunghong.github.io/DiffusionBrowser</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Susung Hong</name>
    </author>
    <author>
      <name>Chongjian Ge</name>
    </author>
    <author>
      <name>Zhifei Zhang</name>
    </author>
    <author>
      <name>Jui-Hsien Wang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13689v1</id>
    <title>LitePT: Lighter Yet Stronger Point Transformer</title>
    <updated>2025-12-15T18:59:57Z</updated>
    <link href="https://arxiv.org/abs/2512.13689v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13689v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Modern neural architectures for 3D point cloud processing contain both convolutional layers and attention blocks, but the best way to assemble them remains unclear. We analyse the role of different computational blocks in 3D point cloud networks and find an intuitive behaviour: convolution is adequate to extract low-level geometry at high-resolution in early layers, where attention is expensive without bringing any benefits; attention captures high-level semantics and context in low-resolution, deep layers more efficiently. Guided by this design principle, we propose a new, improved 3D point cloud backbone that employs convolutions in early stages and switches to attention for deeper layers. To avoid the loss of spatial layout information when discarding redundant convolution layers, we introduce a novel, training-free 3D positional encoding, PointROPE. The resulting LitePT model has $3.6\times$ fewer parameters, runs $2\times$ faster, and uses $2\times$ less memory than the state-of-the-art Point Transformer V3, but nonetheless matches or even outperforms it on a range of tasks and datasets. Code and models are available at: https://github.com/prs-eth/LitePT.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T18:59:57Z</published>
    <arxiv:comment>Project page: https://litept.github.io/</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Yuanwen Yue</name>
    </author>
    <author>
      <name>Damien Robert</name>
    </author>
    <author>
      <name>Jianyuan Wang</name>
    </author>
    <author>
      <name>Sunghwan Hong</name>
    </author>
    <author>
      <name>Jan Dirk Wegner</name>
    </author>
    <author>
      <name>Christian Rupprecht</name>
    </author>
    <author>
      <name>Konrad Schindler</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13687v1</id>
    <title>Towards Scalable Pre-training of Visual Tokenizers for Generation</title>
    <updated>2025-12-15T18:59:54Z</updated>
    <link href="https://arxiv.org/abs/2512.13687v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13687v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The quality of the latent space in visual tokenizers (e.g., VAEs) is crucial for modern generative models. However, the standard reconstruction-based training paradigm produces a latent space that is biased towards low-level information, leading to a foundation flaw: better pixel-level accuracy does not lead to higher-quality generation. This implies that pouring extensive compute into visual tokenizer pre-training translates poorly to improved performance in generation. We identify this as the ``pre-training scaling problem`` and suggest a necessary shift: to be effective for generation, a latent space must concisely represent high-level semantics. We present VTP, a unified visual tokenizer pre-training framework, pioneering the joint optimization of image-text contrastive, self-supervised, and reconstruction losses. Our large-scale study reveals two principal findings: (1) understanding is a key driver of generation, and (2) much better scaling properties, where generative performance scales effectively with compute, parameters, and data allocated to the pretraining of the visual tokenizer. After large-scale pre-training, our tokenizer delivers a competitive profile (78.2 zero-shot accuracy and 0.36 rFID on ImageNet) and 4.1 times faster convergence on generation compared to advanced distillation methods. More importantly, it scales effectively: without modifying standard DiT training specs, solely investing more FLOPS in pretraining VTP achieves 65.8\% FID improvement in downstream generation, while conventional autoencoder stagnates very early at 1/10 FLOPS. Our pre-trained models are available at https://github.com/MiniMax-AI/VTP.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T18:59:54Z</published>
    <arxiv:comment>Our pre-trained models are available at https://github.com/MiniMax-AI/VTP</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Jingfeng Yao</name>
    </author>
    <author>
      <name>Yuda Song</name>
    </author>
    <author>
      <name>Yucong Zhou</name>
    </author>
    <author>
      <name>Xinggang Wang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13685v1</id>
    <title>Beyond surface form: A pipeline for semantic analysis in Alzheimer's Disease detection from spontaneous speech</title>
    <updated>2025-12-15T18:59:49Z</updated>
    <link href="https://arxiv.org/abs/2512.13685v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13685v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Alzheimer's Disease (AD) is a progressive neurodegenerative condition that adversely affects cognitive abilities. Language-related changes can be automatically identified through the analysis of outputs from linguistic assessment tasks, such as picture description. Language models show promise as a basis for screening tools for AD, but their limited interpretability poses a challenge in distinguishing true linguistic markers of cognitive decline from surface-level textual patterns. To address this issue, we examine how surface form variation affects classification performance, with the goal of assessing the ability of language models to represent underlying semantic indicators. We introduce a novel approach where texts surface forms are transformed by altering syntax and vocabulary while preserving semantic content. The transformations significantly modify the structure and lexical content, as indicated by low BLEU and chrF scores, yet retain the underlying semantics, as reflected in high semantic similarity scores, isolating the effect of semantic information, and finding models perform similarly to if they were using the original text, with only small deviations in macro-F1. We also investigate whether language from picture descriptions retains enough detail to reconstruct the original image using generative models. We found that image-based transformations add substantial noise reducing classification accuracy. Our methodology provides a novel way of looking at what features influence model predictions, and allows the removal of possible spurious correlations. We find that just using semantic information, language model based classifiers can still detect AD. This work shows that difficult to detect semantic impairment can be identified, addressing an overlooked feature of linguistic deterioration, and opening new pathways for early detection systems.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T18:59:49Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Dylan Phelps</name>
    </author>
    <author>
      <name>Rodrigo Wilkens</name>
    </author>
    <author>
      <name>Edward Gow-Smith</name>
    </author>
    <author>
      <name>Lilian Hubner</name>
    </author>
    <author>
      <name>Bárbara Malcorra</name>
    </author>
    <author>
      <name>César Rennó-Costa</name>
    </author>
    <author>
      <name>Marco Idiart</name>
    </author>
    <author>
      <name>Maria-Cruz Villa-Uriol</name>
    </author>
    <author>
      <name>Aline Villavicencio</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13684v1</id>
    <title>Recurrent Video Masked Autoencoders</title>
    <updated>2025-12-15T18:59:48Z</updated>
    <link href="https://arxiv.org/abs/2512.13684v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13684v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We present Recurrent Video Masked-Autoencoders (RVM): a novel video representation learning approach that uses a transformer-based recurrent neural network to aggregate dense image features over time, effectively capturing the spatio-temporal structure of natural video data. RVM learns via an asymmetric masked prediction task requiring only a standard pixel reconstruction objective. This design yields a highly efficient ``generalist'' encoder: RVM achieves competitive performance with state-of-the-art video models (e.g. VideoMAE, V-JEPA) on video-level tasks like action recognition and point/object tracking, while also performing favorably against image models (e.g. DINOv2) on tasks that test geometric and dense spatial understanding. Notably, RVM achieves strong performance in the small-model regime without requiring knowledge distillation, exhibiting up to 30x greater parameter efficiency than competing video masked autoencoders. Moreover, we demonstrate that RVM's recurrent nature allows for stable feature propagation over long temporal horizons with linear computational cost, overcoming some of the limitations of standard spatio-temporal attention-based architectures. Finally, we use qualitative visualizations to highlight that RVM learns rich representations of scene semantics, structure, and motion.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T18:59:48Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Daniel Zoran</name>
    </author>
    <author>
      <name>Nikhil Parthasarathy</name>
    </author>
    <author>
      <name>Yi Yang</name>
    </author>
    <author>
      <name>Drew A Hudson</name>
    </author>
    <author>
      <name>Joao Carreira</name>
    </author>
    <author>
      <name>Andrew Zisserman</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13683v1</id>
    <title>I-Scene: 3D Instance Models are Implicit Generalizable Spatial Learners</title>
    <updated>2025-12-15T18:59:13Z</updated>
    <link href="https://arxiv.org/abs/2512.13683v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13683v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Generalization remains the central challenge for interactive 3D scene generation. Existing learning-based approaches ground spatial understanding in limited scene dataset, restricting generalization to new layouts. We instead reprogram a pre-trained 3D instance generator to act as a scene level learner, replacing dataset-bounded supervision with model-centric spatial supervision. This reprogramming unlocks the generator transferable spatial knowledge, enabling generalization to unseen layouts and novel object compositions. Remarkably, spatial reasoning still emerges even when the training scenes are randomly composed objects. This demonstrates that the generator's transferable scene prior provides a rich learning signal for inferring proximity, support, and symmetry from purely geometric cues. Replacing widely used canonical space, we instantiate this insight with a view-centric formulation of the scene space, yielding a fully feed-forward, generalizable scene generator that learns spatial relations directly from the instance model. Quantitative and qualitative results show that a 3D instance generator is an implicit spatial learner and reasoner, pointing toward foundation models for interactive 3D scene understanding and generation. Project page: https://luling06.github.io/I-Scene-project/</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T18:59:13Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Lu Ling</name>
    </author>
    <author>
      <name>Yunhao Ge</name>
    </author>
    <author>
      <name>Yichen Sheng</name>
    </author>
    <author>
      <name>Aniket Bera</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13680v1</id>
    <title>LASER: Layer-wise Scale Alignment for Training-Free Streaming 4D Reconstruction</title>
    <updated>2025-12-15T18:59:04Z</updated>
    <link href="https://arxiv.org/abs/2512.13680v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13680v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Recent feed-forward reconstruction models like VGGT and $π^3$ achieve impressive reconstruction quality but cannot process streaming videos due to quadratic memory complexity, limiting their practical deployment. While existing streaming methods address this through learned memory mechanisms or causal attention, they require extensive retraining and may not fully leverage the strong geometric priors of state-of-the-art offline models. We propose LASER, a training-free framework that converts an offline reconstruction model into a streaming system by aligning predictions across consecutive temporal windows. We observe that simple similarity transformation ($\mathrm{Sim}(3)$) alignment fails due to layer depth misalignment: monocular scale ambiguity causes relative depth scales of different scene layers to vary inconsistently between windows. To address this, we introduce layer-wise scale alignment, which segments depth predictions into discrete layers, computes per-layer scale factors, and propagates them across both adjacent windows and timestamps. Extensive experiments show that LASER achieves state-of-the-art performance on camera pose estimation and point map reconstruction %quality with offline models while operating at 14 FPS with 6 GB peak memory on a RTX A6000 GPU, enabling practical deployment for kilometer-scale streaming videos. Project website: $\href{https://neu-vi.github.io/LASER/}{\texttt{https://neu-vi.github.io/LASER/}}$</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T18:59:04Z</published>
    <arxiv:comment>16 pages</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Tianye Ding</name>
    </author>
    <author>
      <name>Yiming Xie</name>
    </author>
    <author>
      <name>Yiqing Liang</name>
    </author>
    <author>
      <name>Moitreya Chatterjee</name>
    </author>
    <author>
      <name>Pedro Miraldo</name>
    </author>
    <author>
      <name>Huaizu Jiang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13678v1</id>
    <title>Feedforward 3D Editing via Text-Steerable Image-to-3D</title>
    <updated>2025-12-15T18:58:55Z</updated>
    <link href="https://arxiv.org/abs/2512.13678v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13678v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Recent progress in image-to-3D has opened up immense possibilities for design, AR/VR, and robotics. However, to use AI-generated 3D assets in real applications, a critical requirement is the capability to edit them easily. We present a feedforward method, Steer3D, to add text steerability to image-to-3D models, which enables editing of generated 3D assets with language. Our approach is inspired by ControlNet, which we adapt to image-to-3D generation to enable text steering directly in a forward pass. We build a scalable data engine for automatic data generation, and develop a two-stage training recipe based on flow-matching training and Direct Preference Optimization (DPO). Compared to competing methods, Steer3D more faithfully follows the language instruction and maintains better consistency with the original 3D asset, while being 2.4x to 28.5x faster. Steer3D demonstrates that it is possible to add a new modality (text) to steer the generation of pretrained image-to-3D generative models with 100k data. Project website: https://glab-caltech.github.io/steer3d/</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T18:58:55Z</published>
    <arxiv:comment>https://glab-caltech.github.io/steer3d/</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Ziqi Ma</name>
    </author>
    <author>
      <name>Hongqiao Chen</name>
    </author>
    <author>
      <name>Yisong Yue</name>
    </author>
    <author>
      <name>Georgia Gkioxari</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13677v1</id>
    <title>JoVA: Unified Multimodal Learning for Joint Video-Audio Generation</title>
    <updated>2025-12-15T18:58:18Z</updated>
    <link href="https://arxiv.org/abs/2512.13677v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13677v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>In this paper, we present JoVA, a unified framework for joint video-audio generation. Despite recent encouraging advances, existing methods face two critical limitations. First, most existing approaches can only generate ambient sounds and lack the capability to produce human speech synchronized with lip movements. Second, recent attempts at unified human video-audio generation typically rely on explicit fusion or modality-specific alignment modules, which introduce additional architecture design and weaken the model simplicity of the original transformers. To address these issues, JoVA employs joint self-attention across video and audio tokens within each transformer layer, enabling direct and efficient cross-modal interaction without the need for additional alignment modules. Furthermore, to enable high-quality lip-speech synchronization, we introduce a simple yet effective mouth-area loss based on facial keypoint detection, which enhances supervision on the critical mouth region during training without compromising architectural simplicity. Extensive experiments on benchmarks demonstrate that JoVA outperforms or is competitive with both unified and audio-driven state-of-the-art methods in lip-sync accuracy, speech quality, and overall video-audio generation fidelity. Our results establish JoVA as an elegant framework for high-quality multimodal generation.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T18:58:18Z</published>
    <arxiv:comment>Project page: \url{https://visual-ai.github.io/jova}</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Xiaohu Huang</name>
    </author>
    <author>
      <name>Hao Zhou</name>
    </author>
    <author>
      <name>Qiangpeng Yang</name>
    </author>
    <author>
      <name>Shilei Wen</name>
    </author>
    <author>
      <name>Kai Han</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13676v1</id>
    <title>Towards Effective Model Editing for LLM Personalization</title>
    <updated>2025-12-15T18:58:15Z</updated>
    <link href="https://arxiv.org/abs/2512.13676v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13676v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Personalization is becoming indispensable for LLMs to align with individual user preferences and needs. Yet current approaches are often computationally expensive, data-intensive, susceptible to catastrophic forgetting, and prone to performance degradation in multi-turn interactions or when handling implicit queries. To address these challenges, we conceptualize personalization as a model editing task and introduce Personalization Editing, a framework that applies localized edits guided by clustered preference representations. This design enables precise preference-aligned updates while preserving overall model capabilities. In addition, existing personalization benchmarks frequently rely on persona-based dialogs between LLMs rather than user-LLM interactions, or focus primarily on stylistic imitation while neglecting information-seeking tasks that require accurate recall of user-specific preferences. We introduce User Preference Question Answering (UPQA), a short-answer QA dataset constructed from in-situ user queries with varying levels of difficulty. Unlike prior benchmarks, UPQA directly evaluates a model's ability to recall and apply specific user preferences. Across experimental settings, Personalization Editing achieves higher editing accuracy and greater computational efficiency than fine-tuning, while outperforming prompting-based baselines in multi-turn conversations and implicit preference questions settings.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T18:58:15Z</published>
    <arxiv:comment>15 pages (including appendix), 7 figures. Code, data, results, and additional resources are available at: https://model-editing.github.io</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Baixiang Huang</name>
    </author>
    <author>
      <name>Limeng Cui</name>
    </author>
    <author>
      <name>Jiapeng Liu</name>
    </author>
    <author>
      <name>Haoran Wang</name>
    </author>
    <author>
      <name>Jiawei Xu</name>
    </author>
    <author>
      <name>Zhuiyue Tan</name>
    </author>
    <author>
      <name>Yutong Chen</name>
    </author>
    <author>
      <name>Chen Luo</name>
    </author>
    <author>
      <name>Yi Liu</name>
    </author>
    <author>
      <name>Kai Shu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13674v1</id>
    <title>Towards Interactive Intelligence for Digital Humans</title>
    <updated>2025-12-15T18:57:35Z</updated>
    <link href="https://arxiv.org/abs/2512.13674v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13674v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We introduce Interactive Intelligence, a novel paradigm of digital human that is capable of personality-aligned expression, adaptive interaction, and self-evolution. To realize this, we present Mio (Multimodal Interactive Omni-Avatar), an end-to-end framework composed of five specialized modules: Thinker, Talker, Face Animator, Body Animator, and Renderer. This unified architecture integrates cognitive reasoning with real-time multimodal embodiment to enable fluid, consistent interaction. Furthermore, we establish a new benchmark to rigorously evaluate the capabilities of interactive intelligence. Extensive experiments demonstrate that our framework achieves superior performance compared to state-of-the-art methods across all evaluated dimensions. Together, these contributions move digital humans beyond superficial imitation toward intelligent interaction.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T18:57:35Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Yiyi Cai</name>
    </author>
    <author>
      <name>Xuangeng Chu</name>
    </author>
    <author>
      <name>Xiwei Gao</name>
    </author>
    <author>
      <name>Sitong Gong</name>
    </author>
    <author>
      <name>Yifei Huang</name>
    </author>
    <author>
      <name>Caixin Kang</name>
    </author>
    <author>
      <name>Kunhang Li</name>
    </author>
    <author>
      <name>Haiyang Liu</name>
    </author>
    <author>
      <name>Ruicong Liu</name>
    </author>
    <author>
      <name>Yun Liu</name>
    </author>
    <author>
      <name>Dianwen Ng</name>
    </author>
    <author>
      <name>Zixiong Su</name>
    </author>
    <author>
      <name>Erwin Wu</name>
    </author>
    <author>
      <name>Yuhan Wu</name>
    </author>
    <author>
      <name>Dingkun Yan</name>
    </author>
    <author>
      <name>Tianyu Yan</name>
    </author>
    <author>
      <name>Chang Zeng</name>
    </author>
    <author>
      <name>Bo Zheng</name>
    </author>
    <author>
      <name>You Zhou</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13672v1</id>
    <title>Directional Textual Inversion for Personalized Text-to-Image Generation</title>
    <updated>2025-12-15T18:57:07Z</updated>
    <link href="https://arxiv.org/abs/2512.13672v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13672v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Textual Inversion (TI) is an efficient approach to text-to-image personalization but often fails on complex prompts. We trace these failures to embedding norm inflation: learned tokens drift to out-of-distribution magnitudes, degrading prompt conditioning in pre-norm Transformers. Empirically, we show semantics are primarily encoded by direction in CLIP token space, while inflated norms harm contextualization; theoretically, we analyze how large magnitudes attenuate positional information and hinder residual updates in pre-norm blocks. We propose Directional Textual Inversion (DTI), which fixes the embedding magnitude to an in-distribution scale and optimizes only direction on the unit hypersphere via Riemannian SGD. We cast direction learning as MAP with a von Mises-Fisher prior, yielding a constant-direction prior gradient that is simple and efficient to incorporate. Across personalization tasks, DTI improves text fidelity over TI and TI-variants while maintaining subject similarity. Crucially, DTI's hyperspherical parameterization enables smooth, semantically coherent interpolation between learned concepts (slerp), a capability that is absent in standard TI. Our findings suggest that direction-only optimization is a robust and scalable path for prompt-faithful personalization.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T18:57:07Z</published>
    <arxiv:comment>Project page: https://kunheek.github.io/dti</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Kunhee Kim</name>
    </author>
    <author>
      <name>NaHyeon Park</name>
    </author>
    <author>
      <name>Kibeom Hong</name>
    </author>
    <author>
      <name>Hyunjung Shim</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13671v1</id>
    <title>AgentIAD: Tool-Augmented Single-Agent for Industrial Anomaly Detection</title>
    <updated>2025-12-15T18:57:04Z</updated>
    <link href="https://arxiv.org/abs/2512.13671v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13671v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Industrial anomaly detection (IAD) is difficult due to the scarcity of normal reference samples and the subtle, localized nature of many defects. Single-pass vision-language models (VLMs) often overlook small abnormalities and lack explicit mechanisms to compare against canonical normal patterns. We propose AgentIAD, a tool-driven agentic framework that enables multi-stage visual inspection. The agent is equipped with a Perceptive Zoomer (PZ) for localized fine-grained analysis and a Comparative Retriever (CR) for querying normal exemplars when evidence is ambiguous. To teach these inspection behaviors, we construct structured perceptive and comparative trajectories from the MMAD dataset and train the model in two stages: supervised fine-tuning followed by reinforcement learning. A two-part reward design drives this process: a perception reward that supervises classification accuracy, spatial alignment, and type correctness, and a behavior reward that encourages efficient tool use. Together, these components enable the model to refine its judgment through step-wise observation, zooming, and verification. AgentIAD achieves a new state-of-the-art 97.62% classification accuracy on MMAD, surpassing prior MLLM-based approaches while producing transparent and interpretable inspection traces.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T18:57:04Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Junwen Miao</name>
    </author>
    <author>
      <name>Penghui Du</name>
    </author>
    <author>
      <name>Yi Liu</name>
    </author>
    <author>
      <name>Yu Wang</name>
    </author>
    <author>
      <name>Yan Wang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13668v1</id>
    <title>A Scientific Reasoning Model for Organic Synthesis Procedure Generation</title>
    <updated>2025-12-15T18:55:39Z</updated>
    <link href="https://arxiv.org/abs/2512.13668v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13668v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Solving computer-aided synthesis planning is essential for enabling fully automated, robot-assisted synthesis workflows and improving the efficiency of drug discovery. A key challenge, however, is bridging the gap between computational route design and practical laboratory execution, particularly the accurate prediction of viable experimental procedures for each synthesis step. In this work, we present QFANG, a scientific reasoning language model capable of generating precise, structured experimental procedures directly from reaction equations, with explicit chain-of-thought reasoning. To develop QFANG, we curated a high-quality dataset comprising 905,990 chemical reactions paired with structured action sequences, extracted and processed from patent literature using large language models. We introduce a Chemistry-Guided Reasoning (CGR) framework that produces chain-of-thought data grounded in chemical knowledge at scale. The model subsequently undergoes supervised fine-tuning to elicit complex chemistry reasoning. Finally, we apply Reinforcement Learning from Verifiable Rewards (RLVR) to further enhance procedural accuracy. Experimental results demonstrate that QFANG outperforms advanced general-purpose reasoning models and nearest-neighbor retrieval baselines, measured by traditional NLP similarity metrics and a chemically aware evaluator using an LLM-as-a-judge. Moreover, QFANG generalizes to certain out-of-domain reaction classes and adapts to variations in laboratory conditions and user-specific constraints. We believe that QFANG's ability to generate high-quality synthesis procedures represents an important step toward bridging the gap between computational synthesis planning and fully automated laboratory synthesis.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T18:55:39Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Guoqing Liu</name>
    </author>
    <author>
      <name>Junren Li</name>
    </author>
    <author>
      <name>Zihan Zhao</name>
    </author>
    <author>
      <name>Eray Inanc</name>
    </author>
    <author>
      <name>Krzysztof Maziarz</name>
    </author>
    <author>
      <name>Jose Garrido Torres</name>
    </author>
    <author>
      <name>Victor Garcia Satorras</name>
    </author>
    <author>
      <name>Shoko Ueda</name>
    </author>
    <author>
      <name>Christopher M. Bishop</name>
    </author>
    <author>
      <name>Marwin Segler</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13667v1</id>
    <title>A stylometric analysis of speaker attribution from speech transcripts</title>
    <updated>2025-12-15T18:55:25Z</updated>
    <link href="https://arxiv.org/abs/2512.13667v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13667v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Forensic scientists often need to identify an unknown speaker or writer in cases such as ransom calls, covert recordings, alleged suicide notes, or anonymous online communications, among many others. Speaker recognition in the speech domain usually examines phonetic or acoustic properties of a voice, and these methods can be accurate and robust under certain conditions. However, if a speaker disguises their voice or employs text-to-speech software, vocal properties may no longer be reliable, leaving only their linguistic content available for analysis. Authorship attribution methods traditionally use syntactic, semantic, and related linguistic information to identify writers of written text (authorship attribution). In this paper, we apply a content-based authorship approach to speech that has been transcribed into text, using what a speaker says to attribute speech to individuals (speaker attribution). We introduce a stylometric method, StyloSpeaker, which incorporates character, word, token, sentence, and style features from the stylometric literature on authorship, to assess whether two transcripts were produced by the same speaker. We evaluate this method on two types of transcript formatting: one approximating prescriptive written text with capitalization and punctuation and another normalized style that removes these conventions. The transcripts' conversation topics are also controlled to varying degrees. We find generally higher attribution performance on normalized transcripts, except under the strongest topic control condition, in which overall performance is highest. Finally, we compare this more explainable stylometric model to black-box neural approaches on the same data and investigate which stylistic features most effectively distinguish speakers.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T18:55:25Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Cristina Aggazzotti</name>
    </author>
    <author>
      <name>Elizabeth Allyn Smith</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13666v1</id>
    <title>SEDULity: A Proof-of-Learning Framework for Distributed and Secure Blockchains with Efficient Useful Work</title>
    <updated>2025-12-15T18:55:20Z</updated>
    <link href="https://arxiv.org/abs/2512.13666v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13666v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The security and decentralization of Proof-of-Work (PoW) have been well-tested in existing blockchain systems. However, its tremendous energy waste has raised concerns about sustainability. Proof-of-Useful-Work (PoUW) aims to redirect the meaningless computation to meaningful tasks such as solving machine learning (ML) problems, giving rise to the branch of Proof-of-Learning (PoL). While previous studies have proposed various PoLs, they all, to some degree, suffer from security, decentralization, or efficiency issues. In this paper, we propose a PoL framework that trains ML models efficiently while maintaining blockchain security in a fully distributed manner. We name the framework SEDULity, which stands for a Secure, Efficient, Distributed, and Useful Learning-based blockchain system. Specifically, we encode the template block into the training process and design a useful function that is difficult to solve but relatively easy to verify, as a substitute for the PoW puzzle. We show that our framework is distributed, secure, and efficiently trains ML models. We further demonstrate that the proposed PoL framework can be extended to other types of useful work and design an incentive mechanism to incentivize task verification. We show theoretically that a rational miner is incentivized to train fully honestly with well-designed system parameters. Finally, we present simulation results to demonstrate the performance of our framework and validate our analysis.</summary>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T18:55:20Z</published>
    <arxiv:primary_category term="cs.CR"/>
    <author>
      <name>Weihang Cao</name>
    </author>
    <author>
      <name>Mustafa Doger</name>
    </author>
    <author>
      <name>Sennur Ulukus</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13665v1</id>
    <title>Grab-3D: Detecting AI-Generated Videos from 3D Geometric Temporal Consistency</title>
    <updated>2025-12-15T18:54:30Z</updated>
    <link href="https://arxiv.org/abs/2512.13665v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13665v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Recent advances in diffusion-based generation techniques enable AI models to produce highly realistic videos, heightening the need for reliable detection mechanisms. However, existing detection methods provide only limited exploration of the 3D geometric patterns present in generated videos. In this paper, we use vanishing points as an explicit representation of 3D geometry patterns, revealing fundamental discrepancies in geometric consistency between real and AI-generated videos. We introduce Grab-3D, a geometry-aware transformer framework for detecting AI-generated videos based on 3D geometric temporal consistency. To enable reliable evaluation, we construct an AI-generated video dataset of static scenes, allowing stable 3D geometric feature extraction. We propose a geometry-aware transformer equipped with geometric positional encoding, temporal-geometric attention, and an EMA-based geometric classifier head to explicitly inject 3D geometric awareness into temporal modeling. Experiments demonstrate that Grab-3D significantly outperforms state-of-the-art detectors, achieving robust cross-domain generalization to unseen generators.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T18:54:30Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Wenhan Chen</name>
    </author>
    <author>
      <name>Sezer Karaoglu</name>
    </author>
    <author>
      <name>Theo Gevers</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13660v1</id>
    <title>RoboTracer: Mastering Spatial Trace with Reasoning in Vision-Language Models for Robotics</title>
    <updated>2025-12-15T18:52:43Z</updated>
    <link href="https://arxiv.org/abs/2512.13660v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13660v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Spatial tracing, as a fundamental embodied interaction ability for robots, is inherently challenging as it requires multi-step metric-grounded reasoning compounded with complex spatial referring and real-world metric measurement. However, existing methods struggle with this compositional task. To this end, we propose RoboTracer, a 3D-aware VLM that first achieves both 3D spatial referring and measuring via a universal spatial encoder and a regression-supervised decoder to enhance scale awareness during supervised fine-tuning (SFT). Moreover, RoboTracer advances multi-step metric-grounded reasoning via reinforcement fine-tuning (RFT) with metric-sensitive process rewards, supervising key intermediate perceptual cues to accurately generate spatial traces. To support SFT and RFT training, we introduce TraceSpatial, a large-scale dataset of 30M QA pairs, spanning outdoor/indoor/tabletop scenes and supporting complex reasoning processes (up to 9 steps). We further present TraceSpatial-Bench, a challenging benchmark filling the gap to evaluate spatial tracing. Experimental results show that RoboTracer surpasses baselines in spatial understanding, measuring, and referring, with an average success rate of 79.1%, and also achieves SOTA performance on TraceSpatial-Bench by a large margin, exceeding Gemini-2.5-Pro by 36% accuracy. Notably, RoboTracer can be integrated with various control policies to execute long-horizon, dynamic tasks across diverse robots (UR5, G1 humanoid) in cluttered real-world scenes.</summary>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T18:52:43Z</published>
    <arxiv:comment>Project page: https://zhoues.github.io/RoboTracer</arxiv:comment>
    <arxiv:primary_category term="cs.RO"/>
    <author>
      <name>Enshen Zhou</name>
    </author>
    <author>
      <name>Cheng Chi</name>
    </author>
    <author>
      <name>Yibo Li</name>
    </author>
    <author>
      <name>Jingkun An</name>
    </author>
    <author>
      <name>Jiayuan Zhang</name>
    </author>
    <author>
      <name>Shanyu Rong</name>
    </author>
    <author>
      <name>Yi Han</name>
    </author>
    <author>
      <name>Yuheng Ji</name>
    </author>
    <author>
      <name>Mengzhen Liu</name>
    </author>
    <author>
      <name>Pengwei Wang</name>
    </author>
    <author>
      <name>Zhongyuan Wang</name>
    </author>
    <author>
      <name>Lu Sheng</name>
    </author>
    <author>
      <name>Shanghang Zhang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13658v1</id>
    <title>Embedding-Based Rankings of Educational Resources based on Learning Outcome Alignment: Benchmarking, Expert Validation, and Learner Performance</title>
    <updated>2025-12-15T18:51:00Z</updated>
    <link href="https://arxiv.org/abs/2512.13658v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13658v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>As the online learning landscape evolves, the need for personalization is increasingly evident. Although educational resources are burgeoning, educators face challenges selecting materials that both align with intended learning outcomes and address diverse learner needs. Large Language Models (LLMs) are attracting growing interest for their potential to create learning resources that better support personalization, but verifying coverage of intended outcomes still requires human alignment review, which is costly and limits scalability. We propose a framework that supports the cost-effective automation of evaluating alignment between educational resources and intended learning outcomes. Using human-generated materials, we benchmarked LLM-based text-embedding models and found that the most accurate model (Voyage) achieved 79% accuracy in detecting alignment. We then applied the optimal model to LLM-generated resources and, via expert evaluation, confirmed that it reliably assessed correspondence to intended outcomes (83% accuracy). Finally, in a three-group experiment with 360 learners, higher alignment scores were positively related to greater learning performance, chi-squared(2, N = 360) = 15.39, p &lt; 0.001. These findings show that embedding-based alignment scores can facilitate scalable personalization by confirming alignment with learning outcomes, which allows teachers to focus on tailoring content to diverse learner needs.</summary>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T18:51:00Z</published>
    <arxiv:comment>Accepted for publication at the 16th International Conference on Learning Analytics &amp; Knowledge (LAK 2026)</arxiv:comment>
    <arxiv:primary_category term="cs.CY"/>
    <author>
      <name>Mohammadreza Molavi</name>
    </author>
    <author>
      <name>Mohammad Moein</name>
    </author>
    <author>
      <name>Mohammadreza Tavakoli</name>
    </author>
    <author>
      <name>Abdolali Faraji</name>
    </author>
    <author>
      <name>Stefan T. Mol</name>
    </author>
    <author>
      <name>Gábor Kismihók</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13655v1</id>
    <title>Comparative Analysis of LLM Abliteration Methods: A Cross-Architecture Evaluation</title>
    <updated>2025-12-15T18:48:42Z</updated>
    <link href="https://arxiv.org/abs/2512.13655v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13655v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Safety alignment mechanisms in large language models prevent responses to harmful queries through learned refusal behavior, yet these same mechanisms impede legitimate research applications including cognitive modeling, adversarial testing, and security analysis. While abliteration techniques enable surgical removal of refusal representations through directional orthogonalization, the relative effectiveness of available implementations remains uncharacterized. This study evaluates four abliteration tools (Heretic, DECCP, ErisForge, FailSpy) across sixteen instruction-tuned models (7B-14B parameters), reporting tool compatibility on all 16 models and quantitative metrics on subsets dictated by tool support. Single-pass methods demonstrated superior capability preservation on the benchmarked subset (avg GSM8K change across three models: ErisForge -0.28 pp; DECCP -0.13 pp), while Bayesian-optimized abliteration produced variable distribution shift (KL divergence: 0.043-1.646) with model-dependent capability impact. These findings provide researchers with evidence-based selection criteria for abliteration tool deployment across diverse model architectures. The principal finding indicates that mathematical reasoning capabilities exhibit the highest sensitivity to abliteration interventions, with GSM8K change ranging from +1.51 pp to -18.81 pp (-26.5% relative) depending on tool selection and model architecture.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T18:48:42Z</published>
    <arxiv:comment>25 pages, 6 figures, 8 tables</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Richard J. Young</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13654v1</id>
    <title>Large-Language Memorization During the Classification of United States Supreme Court Cases</title>
    <updated>2025-12-15T18:47:48Z</updated>
    <link href="https://arxiv.org/abs/2512.13654v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13654v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Large-language models (LLMs) have been shown to respond in a variety of ways for classification tasks outside of question-answering. LLM responses are sometimes called "hallucinations" since the output is not what is ex pected. Memorization strategies in LLMs are being studied in detail, with the goal of understanding how LLMs respond. We perform a deep dive into a classification task based on United States Supreme Court (SCOTUS) decisions. The SCOTUS corpus is an ideal classification task to study for LLM memory accuracy because it presents significant challenges due to extensive sentence length, complex legal terminology, non-standard structure, and domain-specific vocabulary. Experimentation is performed with the latest LLM fine tuning and retrieval-based approaches, such as parameter-efficient fine-tuning, auto-modeling, and others, on two traditional category-based SCOTUS classification tasks: one with 15 labeled topics and another with 279. We show that prompt-based models with memories, such as DeepSeek, can be more robust than previous BERT-based models on both tasks scoring about 2 points better than previous models not based on prompting.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T18:47:48Z</published>
    <arxiv:comment>7 pages, 1 figure, Appendix of Prompts</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>John E. Ortega</name>
    </author>
    <author>
      <name>Dhruv D. Joshi</name>
    </author>
    <author>
      <name>Matt P. Borkowski</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13644v1</id>
    <title>World Models Can Leverage Human Videos for Dexterous Manipulation</title>
    <updated>2025-12-15T18:37:12Z</updated>
    <link href="https://arxiv.org/abs/2512.13644v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13644v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Dexterous manipulation is challenging because it requires understanding how subtle hand motion influences the environment through contact with objects. We introduce DexWM, a Dexterous Manipulation World Model that predicts the next latent state of the environment conditioned on past states and dexterous actions. To overcome the scarcity of dexterous manipulation datasets, DexWM is trained on over 900 hours of human and non-dexterous robot videos. To enable fine-grained dexterity, we find that predicting visual features alone is insufficient; therefore, we introduce an auxiliary hand consistency loss that enforces accurate hand configurations. DexWM outperforms prior world models conditioned on text, navigation, and full-body actions, achieving more accurate predictions of future states. DexWM also demonstrates strong zero-shot generalization to unseen manipulation skills when deployed on a Franka Panda arm equipped with an Allegro gripper, outperforming Diffusion Policy by over 50% on average in grasping, placing, and reaching tasks.</summary>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T18:37:12Z</published>
    <arxiv:primary_category term="cs.RO"/>
    <author>
      <name>Raktim Gautam Goswami</name>
    </author>
    <author>
      <name>Amir Bar</name>
    </author>
    <author>
      <name>David Fan</name>
    </author>
    <author>
      <name>Tsung-Yen Yang</name>
    </author>
    <author>
      <name>Gaoyue Zhou</name>
    </author>
    <author>
      <name>Prashanth Krishnamurthy</name>
    </author>
    <author>
      <name>Michael Rabbat</name>
    </author>
    <author>
      <name>Farshad Khorrami</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13641v1</id>
    <title>From Code to Field: Evaluating the Robustness of Convolutional Neural Networks for Disease Diagnosis in Mango Leaves</title>
    <updated>2025-12-15T18:36:48Z</updated>
    <link href="https://arxiv.org/abs/2512.13641v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13641v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The validation and verification of artificial intelligence (AI) models through robustness assessment are essential to guarantee the reliable performance of intelligent systems facing real-world challenges, such as image corruptions including noise, blurring, and weather variations. Despite the global importance of mango (Mangifera indica L.), there is a lack of studies on the robustness of models for the diagnosis of disease in its leaves. This paper proposes a methodology to evaluate convolutional neural networks (CNNs) under adverse conditions. We adapted the MangoLeafDB dataset, generating MangoLeafDB-C with 19 types of artificial corruptions at five severity levels. We conducted a benchmark comparing five architectures: ResNet-50, ResNet-101, VGG-16, Xception, and LCNN (the latter being a lightweight architecture designed specifically for mango leaf diagnosis). The metrics include the F1 score, the corruption error (CE) and the relative mean corruption error (relative mCE). The results show that LCNN outperformed complex models in corruptions that can be present in real-world scenarios such as Defocus Blur, Motion Blur, while also achieving the lowest mCE. Modern architectures (e.g., ResNet-101) exhibited significant performance degradation in corrupted scenarios, despite their high accuracy under ideal conditions. These findings suggest that lightweight and specialized models may be more suitable for real-world applications in edge devices, where robustness and efficiency are critical. The study highlights the need to incorporate robustness assessments in the development of intelligent systems for agriculture, particularly in regions with technological limitations.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T18:36:48Z</published>
    <arxiv:comment>This work was presented at the BRACIS 2025 conference in Fortaleza</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Gabriel Vitorino de Andrade</name>
    </author>
    <author>
      <name>Saulo Roberto dos Santos</name>
    </author>
    <author>
      <name>Itallo Patrick Castro Alves da Silva</name>
    </author>
    <author>
      <name>Emanuel Adler Medeiros Pereira</name>
    </author>
    <author>
      <name>Erick de Andrade Barboza</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13639v1</id>
    <title>Charge: A Comprehensive Novel View Synthesis Benchmark and Dataset to Bind Them All</title>
    <updated>2025-12-15T18:33:08Z</updated>
    <link href="https://arxiv.org/abs/2512.13639v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13639v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>This paper presents a new dataset for Novel View Synthesis, generated from a high-quality, animated film with stunning realism and intricate detail. Our dataset captures a variety of dynamic scenes, complete with detailed textures, lighting, and motion, making it ideal for training and evaluating cutting-edge 4D scene reconstruction and novel view generation models. In addition to high-fidelity RGB images, we provide multiple complementary modalities, including depth, surface normals, object segmentation and optical flow, enabling a deeper understanding of scene geometry and motion. The dataset is organised into three distinct benchmarking scenarios: a dense multi-view camera setup, a sparse camera arrangement, and monocular video sequences, enabling a wide range of experimentation and comparison across varying levels of data sparsity. With its combination of visual richness, high-quality annotations, and diverse experimental setups, this dataset offers a unique resource for pushing the boundaries of view synthesis and 3D vision.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T18:33:08Z</published>
    <arxiv:comment>Project page: https://charge-benchmark.github.io/</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Michal Nazarczuk</name>
    </author>
    <author>
      <name>Thomas Tanay</name>
    </author>
    <author>
      <name>Arthur Moreau</name>
    </author>
    <author>
      <name>Zhensong Zhang</name>
    </author>
    <author>
      <name>Eduardo Pérez-Pellitero</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13638v1</id>
    <title>Design in Tiles: Automating GEMM Deployment on Tile-Based Many-PE Accelerators</title>
    <updated>2025-12-15T18:33:04Z</updated>
    <link href="https://arxiv.org/abs/2512.13638v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13638v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Tile-based many-Processing Element (PE) accelerators can achieve competitive performance on General Matrix Multiplication (GEMM), but they are extremely hard to program, as their optimal software mapping is deeply coupled with hardware design which is unwieldy to manual deployment. We propose "Design in Tiles (DiT)", an automated framework connecting a deployment toolchain with a configurable executable model for these accelerators. For evaluation, we apply our framework to GEMM targeting a large acceleration configuration (e.g., 32x32 tiles, 1979 TFLOPS@FP8, 4 TB/s Bandwidth) comparable to an NVIDIA GH200. We achieve higher PE utilization than GH200 with its expert-tuned GEMM libraries, achieving 1.2-2.0x speedup across diverse matrix shapes.</summary>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T18:33:04Z</published>
    <arxiv:primary_category term="cs.DC"/>
    <author>
      <name>Aofeng Shen</name>
    </author>
    <author>
      <name>Chi Zhang</name>
    </author>
    <author>
      <name>Yakup Budanaz</name>
    </author>
    <author>
      <name>Alexandru Calotoiu</name>
    </author>
    <author>
      <name>Torsten Hoefler</name>
    </author>
    <author>
      <name>Luca Benini</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13636v1</id>
    <title>MindDrive: A Vision-Language-Action Model for Autonomous Driving via Online Reinforcement Learning</title>
    <updated>2025-12-15T18:31:32Z</updated>
    <link href="https://arxiv.org/abs/2512.13636v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13636v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Current Vision-Language-Action (VLA) paradigms in autonomous driving primarily rely on Imitation Learning (IL), which introduces inherent challenges such as distribution shift and causal confusion. Online Reinforcement Learning offers a promising pathway to address these issues through trial-and-error learning. However, applying online reinforcement learning to VLA models in autonomous driving is hindered by inefficient exploration in continuous action spaces. To overcome this limitation, we propose MindDrive, a VLA framework comprising a large language model (LLM) with two distinct sets of LoRA parameters. The one LLM serves as a Decision Expert for scenario reasoning and driving decision-making, while the other acts as an Action Expert that dynamically maps linguistic decisions into feasible trajectories. By feeding trajectory-level rewards back into the reasoning space, MindDrive enables trial-and-error learning over a finite set of discrete linguistic driving decisions, instead of operating directly in a continuous action space. This approach effectively balances optimal decision-making in complex scenarios, human-like driving behavior, and efficient exploration in online reinforcement learning. MindDrive achieves strong closed-loop performance on the challenging Bench2Drive benchmark, with a Driving Score (DS) of 78.04 and a Success Rate (SR) of 55.09%. To the best of our knowledge, this is the first work to demonstrate the effectiveness of online reinforcement learning for the VLA model in autonomous driving.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T18:31:32Z</published>
    <arxiv:comment>16 pages, 12 figures, 6 tables; Project Page: https://xiaomi-mlab.github.io/MindDrive/</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Haoyu Fu</name>
    </author>
    <author>
      <name>Diankun Zhang</name>
    </author>
    <author>
      <name>Zongchuang Zhao</name>
    </author>
    <author>
      <name>Jianfeng Cui</name>
    </author>
    <author>
      <name>Hongwei Xie</name>
    </author>
    <author>
      <name>Bing Wang</name>
    </author>
    <author>
      <name>Guang Chen</name>
    </author>
    <author>
      <name>Dingkang Liang</name>
    </author>
    <author>
      <name>Xiang Bai</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13635v1</id>
    <title>SCR2-ST: Combine Single Cell with Spatial Transcriptomics for Efficient Active Sampling via Reinforcement Learning</title>
    <updated>2025-12-15T18:30:40Z</updated>
    <link href="https://arxiv.org/abs/2512.13635v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13635v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Spatial transcriptomics (ST) is an emerging technology that enables researchers to investigate the molecular relationships underlying tissue morphology. However, acquiring ST data remains prohibitively expensive, and traditional fixed-grid sampling strategies lead to redundant measurements of morphologically similar or biologically uninformative regions, thus resulting in scarce data that constrain current methods. The well-established single-cell sequencing field, however, could provide rich biological data as an effective auxiliary source to mitigate this limitation. To bridge these gaps, we introduce SCR2-ST, a unified framework that leverages single-cell prior knowledge to guide efficient data acquisition and accurate expression prediction. SCR2-ST integrates a single-cell guided reinforcement learning-based (SCRL) active sampling and a hybrid regression-retrieval prediction network SCR2Net. SCRL combines single-cell foundation model embeddings with spatial density information to construct biologically grounded reward signals, enabling selective acquisition of informative tissue regions under constrained sequencing budgets. SCR2Net then leverages the actively sampled data through a hybrid architecture combining regression-based modeling with retrieval-augmented inference, where a majority cell-type filtering mechanism suppresses noisy matches and retrieved expression profiles serve as soft labels for auxiliary supervision. We evaluated SCR2-ST on three public ST datasets, demonstrating SOTA performance in both sampling efficiency and prediction accuracy, particularly under low-budget scenarios. Code is publicly available at: https://github.com/hrlblab/SCR2ST</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T18:30:40Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Junchao Zhu</name>
    </author>
    <author>
      <name>Ruining Deng</name>
    </author>
    <author>
      <name>Junlin Guo</name>
    </author>
    <author>
      <name>Tianyuan Yao</name>
    </author>
    <author>
      <name>Chongyu Qu</name>
    </author>
    <author>
      <name>Juming Xiong</name>
    </author>
    <author>
      <name>Siqi Lu</name>
    </author>
    <author>
      <name>Zhengyi Lu</name>
    </author>
    <author>
      <name>Yanfan Zhu</name>
    </author>
    <author>
      <name>Marilyn Lionts</name>
    </author>
    <author>
      <name>Yuechen Yang</name>
    </author>
    <author>
      <name>Yalin Zheng</name>
    </author>
    <author>
      <name>Yu Wang</name>
    </author>
    <author>
      <name>Shilin Zhao</name>
    </author>
    <author>
      <name>Haichun Yang</name>
    </author>
    <author>
      <name>Yuankai Huo</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13634v1</id>
    <title>Universality of high-dimensional scaling limits of stochastic gradient descent</title>
    <updated>2025-12-15T18:30:26Z</updated>
    <link href="https://arxiv.org/abs/2512.13634v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13634v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We consider statistical tasks in high dimensions whose loss depends on the data only through its projection into a fixed-dimensional subspace spanned by the parameter vectors and certain ground truth vectors. This includes classifying mixture distributions with cross-entropy loss with one and two-layer networks, and learning single and multi-index models with one and two-layer networks. When the data is drawn from an isotropic Gaussian mixture distribution, it is known that the evolution of a finite family of summary statistics under stochastic gradient descent converges to an autonomous ordinary differential equation (ODE), as the dimension and sample size go to $\infty$ and the step size goes to $0$ commensurately. Our main result is that these ODE limits are universal in that this convergence occurs even when the data is drawn from mixtures of product measures provided the first two moments match the corresponding Gaussian distribution and the initialization and ground truth vectors are sufficiently coordinate-delocalized. We complement this by proving two corresponding non-universality results. We provide a simple example where the ODE limits are non-universal if the initialization is coordinate aligned. We also show that the stochastic differential equation limits arising as fluctuations of the summary statistics around their ODE's fixed points are not universal.</summary>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T18:30:26Z</published>
    <arxiv:comment>30 pages</arxiv:comment>
    <arxiv:primary_category term="stat.ML"/>
    <author>
      <name>Reza Gheissari</name>
    </author>
    <author>
      <name>Aukosh Jagannath</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13632v1</id>
    <title>StutterFuse: Mitigating Modality Collapse in Stuttering Detection with Jaccard-Weighted Metric Learning and Gated Fusion</title>
    <updated>2025-12-15T18:28:39Z</updated>
    <link href="https://arxiv.org/abs/2512.13632v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13632v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Stuttering detection breaks down when disfluencies overlap. Existing parametric models struggle to distinguish complex, simultaneous disfluencies (e.g., a 'block' with a 'prolongation') due to the scarcity of these specific combinations in training data. While Retrieval-Augmented Generation (RAG) has revolutionized NLP by grounding models in external knowledge, this paradigm remains unexplored in pathological speech processing. To bridge this gap, we introduce StutterFuse, the first Retrieval-Augmented Classifier (RAC) for multi-label stuttering detection. By conditioning a Conformer encoder on a non-parametric memory bank of clinical examples, we allow the model to classify by reference rather than memorization. We further identify and solve "Modality Collapse", an "Echo Chamber" effect where naive retrieval boosts recall but degrades precision. We mitigate this using: (1) SetCon, a Jaccard-Weighted Metric Learning objective that optimizes for multi-label set similarity, and (2) a Gated Mixture-of-Experts fusion strategy that dynamically arbitrates between acoustic evidence and retrieved context. On the SEP-28k dataset, StutterFuse achieves a weighted F1-score of 0.65, outperforming strong baselines and demonstrating remarkable zero-shot cross-lingual generalization.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T18:28:39Z</published>
    <arxiv:comment>13 pages, 10 figures</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Guransh Singh</name>
    </author>
    <author>
      <name>Md Shah Fahad</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13628v1</id>
    <title>Certified-Everlasting Quantum NIZK Proofs</title>
    <updated>2025-12-15T18:23:48Z</updated>
    <link href="https://arxiv.org/abs/2512.13628v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13628v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We study non-interactive zero-knowledge proofs (NIZKs) for NP satisfying: 1) statistical soundness, 2) computational zero-knowledge and 3) certified-everlasting zero-knowledge (CE-ZK). The CE-ZK property allows a verifier of a quantum proof to revoke the proof in a way that can be checked (certified) by the prover. Conditioned on successful certification, the verifier's state can be efficiently simulated with only the statement, in a statistically indistinguishable way. Our contributions regarding these certified-everlasting NIZKs (CE-NIZKs) are as follows:
  - We identify a barrier to obtaining CE-NIZKs in the CRS model via generalizations of known interactive proofs that satisfy CE-ZK.
  - We circumvent this by constructing CE-NIZK from black-box use of NIZK for NP satisfying certain properties, along with OWFs. As a result, we obtain CE-NIZKs for NP in the CRS model, based on polynomial hardness of the learning with errors (LWE) assumption.
  - In addition, we observe that the aforementioned barrier does not apply to the shared EPR model. Consequently, we present a CE-NIZK for NP in this model based on any statistical binding hidden-bits generator, which can be based on LWE. The only quantum computation in this protocol involves single-qubit measurements of the shared EPR pairs.</summary>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T18:23:48Z</published>
    <arxiv:primary_category term="quant-ph"/>
    <author>
      <name>Nikhil Pappu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13618v1</id>
    <title>Temporal Tokenization Strategies for Event Sequence Modeling with Large Language Models</title>
    <updated>2025-12-15T18:10:51Z</updated>
    <link href="https://arxiv.org/abs/2512.13618v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13618v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Representing continuous time is a critical and under-explored challenge in modeling temporal event sequences with large language models (LLMs). Various strategies like byte-level representations or calendar tokens have been proposed. However, the optimal approach remains unclear, especially given the diverse statistical distributions of real-world event data, which range from smooth log-normal to discrete, spiky patterns. This paper presents the first empirical study of temporal tokenization for event sequences, comparing distinct encoding strategies: naive numeric strings, high-precision byte-level representations, human-semantic calendar tokens, classic uniform binning, and adaptive residual scalar quantization. We evaluate these strategies by fine-tuning LLMs on real-world datasets that exemplify these diverse distributions. Our analysis reveals that no single strategy is universally superior; instead, prediction performance depends heavily on aligning the tokenizer with the data's statistical properties, with log-based strategies excelling on skewed distributions and human-centric formats proving robust for mixed modalities.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T18:10:51Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Zefang Liu</name>
    </author>
    <author>
      <name>Nam Nguyen</name>
    </author>
    <author>
      <name>Yinzhu Quan</name>
    </author>
    <author>
      <name>Austin Zhang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13617v1</id>
    <title>LightTopoGAT: Enhancing Graph Attention Networks with Topological Features for Efficient Graph Classification</title>
    <updated>2025-12-15T18:09:59Z</updated>
    <link href="https://arxiv.org/abs/2512.13617v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13617v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Graph Neural Networks have demonstrated significant success in graph classification tasks, yet they often require substantial computational resources and struggle to capture global graph properties effectively. We introduce LightTopoGAT, a lightweight graph attention network that enhances node features through topological augmentation by incorporating node degree and local clustering coefficient to improve graph representation learning. The proposed approach maintains parameter efficiency through streamlined attention mechanisms while integrating structural information that is typically overlooked by local message passing schemes. Through comprehensive experiments on three benchmark datasets, MUTAG, ENZYMES, and PROTEINS, we show that LightTopoGAT achieves superior performance compared to established baselines including GCN, GraphSAGE, and standard GAT, with a 6.6 percent improvement in accuracy on MUTAG and a 2.2 percent improvement on PROTEINS. Ablation studies further confirm that these performance gains arise directly from the inclusion of topological features, demonstrating a simple yet effective strategy for enhancing graph neural network performance without increasing architectural complexity.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T18:09:59Z</published>
    <arxiv:comment>9 pages</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Ankit Sharma</name>
    </author>
    <author>
      <name>Sayan Roy Gupta</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13613v1</id>
    <title>QoeSiGN: Towards Qualified Collaborative eSignatures</title>
    <updated>2025-12-15T18:07:17Z</updated>
    <link href="https://arxiv.org/abs/2512.13613v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13613v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>eSignatures ensure data's authenticity, non-repudiation, and integrity. EU's eIDAS regulation specifies, e.g., advanced and qualified (QES) eSignatures. While eSignatures' concrete legal effects depend on the individual case, QESs constitute the highest level of technical protection and authenticity under eIDAS. QESs are based on a qualified certificate issued by a qualified trust service provider (QTSP). Despite legal requirements, technically, a QTSP represents a single point of failure. Contrary, privacy-preserving collaborative computations (P2C2s) have become increasingly practical in recent years; yet lacking an extensive investigation on potential integrations in the QES landscape.
  We perform a threat analysis on the QES-creation process of Austria's national eID, using STRIDE and a DREAD-like model to extract requirement challenges (RCs) primarily related to: (1) Distributed Service Robustness, (2) Agile Crypto Deployment, and (3) Active User Involvement. To address these RCs, we present QoeSiGN, utilizing novel P2C2 technologies. While currently no P2C2 addresses all RCs, legal aspects, and practical efficiency simultaneously, QoeSiGN gives instantiation possibilities for different needs. For instance, "Multi-Party HSMs" for distributed hardware-secured computations; or secure multi-party computation (software) for highest crypto agility and user involvement, where the user participates in the QES computation.
  Deployment-wise, QTSPs would need to adapt the signing process and setup trusted communication channels. Legal-wise, QoeSiGN's implementation appears permissible, needing further analysis for realization. Technically, QoeSiGN addresses some regulation requirements better than the current solution, such as "sole control" or crypto agility. Our identified threats and extracted requirements can be transferred to the general QES ecosystem.</summary>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T18:07:17Z</published>
    <arxiv:primary_category term="cs.CR"/>
    <author>
      <name>Karl W. Koch</name>
    </author>
    <author>
      <name>Stephan Krenn</name>
    </author>
    <author>
      <name>Alexandra Hofer</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13609v1</id>
    <title>Do-Undo: Generating and Reversing Physical Actions in Vision-Language Models</title>
    <updated>2025-12-15T18:03:42Z</updated>
    <link href="https://arxiv.org/abs/2512.13609v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13609v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We introduce the Do-Undo task and benchmark to address a critical gap in vision-language models: understanding and generating physically plausible scene transformations driven by real-world actions. Unlike prior work focused on object-level edits, Do-Undo requires models to simulate the outcome of a physical action and then accurately reverse it, reflecting true cause-and-effect in the visual world. We curate a large-scale dataset of reversible actions from real-world videos and design a training strategy enforcing consistency for robust action grounding. Our experiments reveal that current models struggle with physical reversibility, underscoring the importance of this task for embodied AI, robotics, and physics-aware generative modeling. Do-Undo establishes an intuitive testbed for evaluating and advancing physical reasoning in multimodal systems.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T18:03:42Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Shweta Mahajan</name>
    </author>
    <author>
      <name>Shreya Kadambi</name>
    </author>
    <author>
      <name>Hoang Le</name>
    </author>
    <author>
      <name>Munawar Hayat</name>
    </author>
    <author>
      <name>Fatih Porikli</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13608v1</id>
    <title>DBT-DINO: Towards Foundation model based analysis of Digital Breast Tomosynthesis</title>
    <updated>2025-12-15T18:03:09Z</updated>
    <link href="https://arxiv.org/abs/2512.13608v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13608v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Foundation models have shown promise in medical imaging but remain underexplored for three-dimensional imaging modalities. No foundation model currently exists for Digital Breast Tomosynthesis (DBT), despite its use for breast cancer screening.
  To develop and evaluate a foundation model for DBT (DBT-DINO) across multiple clinical tasks and assess the impact of domain-specific pre-training.
  Self-supervised pre-training was performed using the DINOv2 methodology on over 25 million 2D slices from 487,975 DBT volumes from 27,990 patients. Three downstream tasks were evaluated: (1) breast density classification using 5,000 screening exams; (2) 5-year risk of developing breast cancer using 106,417 screening exams; and (3) lesion detection using 393 annotated volumes.
  For breast density classification, DBT-DINO achieved an accuracy of 0.79 (95\% CI: 0.76--0.81), outperforming both the MetaAI DINOv2 baseline (0.73, 95\% CI: 0.70--0.76, p&lt;.001) and DenseNet-121 (0.74, 95\% CI: 0.71--0.76, p&lt;.001). For 5-year breast cancer risk prediction, DBT-DINO achieved an AUROC of 0.78 (95\% CI: 0.76--0.80) compared to DINOv2's 0.76 (95\% CI: 0.74--0.78, p=.57). For lesion detection, DINOv2 achieved a higher average sensitivity of 0.67 (95\% CI: 0.60--0.74) compared to DBT-DINO with 0.62 (95\% CI: 0.53--0.71, p=.60). DBT-DINO demonstrated better performance on cancerous lesions specifically with a detection rate of 78.8\% compared to Dinov2's 77.3\%.
  Using a dataset of unprecedented size, we developed DBT-DINO, the first foundation model for DBT. DBT-DINO demonstrated strong performance on breast density classification and cancer risk prediction. However, domain-specific pre-training showed variable benefits on the detection task, with ImageNet baseline outperforming DBT-DINO on general lesion detection, indicating that localized detection tasks require further methodological development.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T18:03:09Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Felix J. Dorfner</name>
    </author>
    <author>
      <name>Manon A. Dorster</name>
    </author>
    <author>
      <name>Ryan Connolly</name>
    </author>
    <author>
      <name>Oscar Gentilhomme</name>
    </author>
    <author>
      <name>Edward Gibbs</name>
    </author>
    <author>
      <name>Steven Graham</name>
    </author>
    <author>
      <name>Seth Wander</name>
    </author>
    <author>
      <name>Thomas Schultz</name>
    </author>
    <author>
      <name>Manisha Bahl</name>
    </author>
    <author>
      <name>Dania Daye</name>
    </author>
    <author>
      <name>Albert E. Kim</name>
    </author>
    <author>
      <name>Christopher P. Bridge</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13607v1</id>
    <title>Nemotron-Cascade: Scaling Cascaded Reinforcement Learning for General-Purpose Reasoning Models</title>
    <updated>2025-12-15T18:02:35Z</updated>
    <link href="https://arxiv.org/abs/2512.13607v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13607v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Building general-purpose reasoning models with reinforcement learning (RL) entails substantial cross-domain heterogeneity, including large variation in inference-time response lengths and verification latency. Such variability complicates the RL infrastructure, slows training, and makes training curriculum (e.g., response length extension) and hyperparameter selection challenging. In this work, we propose cascaded domain-wise reinforcement learning (Cascade RL) to develop general-purpose reasoning models, Nemotron-Cascade, capable of operating in both instruct and deep thinking modes. Departing from conventional approaches that blend heterogeneous prompts from different domains, Cascade RL orchestrates sequential, domain-wise RL, reducing engineering complexity and delivering state-of-the-art performance across a wide range of benchmarks. Notably, RLHF for alignment, when used as a pre-step, boosts the model's reasoning ability far beyond mere preference optimization, and subsequent domain-wise RLVR stages rarely degrade the benchmark performance attained in earlier domains and may even improve it (see an illustration in Figure 1). Our 14B model, after RL, outperforms its SFT teacher, DeepSeek-R1-0528, on LiveCodeBench v5/v6/Pro and achieves silver-medal performance in the 2025 International Olympiad in Informatics (IOI). We transparently share our training and data recipes.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T18:02:35Z</published>
    <arxiv:comment>We publicly release the Nemotron-Cascade models and the full collection of training data at: https://huggingface.co/collections/nvidia/nemotron-cascade</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Boxin Wang</name>
    </author>
    <author>
      <name>Chankyu Lee</name>
    </author>
    <author>
      <name>Nayeon Lee</name>
    </author>
    <author>
      <name>Sheng-Chieh Lin</name>
    </author>
    <author>
      <name>Wenliang Dai</name>
    </author>
    <author>
      <name>Yang Chen</name>
    </author>
    <author>
      <name>Yangyi Chen</name>
    </author>
    <author>
      <name>Zhuolin Yang</name>
    </author>
    <author>
      <name>Zihan Liu</name>
    </author>
    <author>
      <name>Mohammad Shoeybi</name>
    </author>
    <author>
      <name>Bryan Catanzaro</name>
    </author>
    <author>
      <name>Wei Ping</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13604v1</id>
    <title>LongVie 2: Multimodal Controllable Ultra-Long Video World Model</title>
    <updated>2025-12-15T17:59:58Z</updated>
    <link href="https://arxiv.org/abs/2512.13604v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13604v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Building video world models upon pretrained video generation systems represents an important yet challenging step toward general spatiotemporal intelligence. A world model should possess three essential properties: controllability, long-term visual quality, and temporal consistency. To this end, we take a progressive approach-first enhancing controllability and then extending toward long-term, high-quality generation. We present LongVie 2, an end-to-end autoregressive framework trained in three stages: (1) Multi-modal guidance, which integrates dense and sparse control signals to provide implicit world-level supervision and improve controllability; (2) Degradation-aware training on the input frame, bridging the gap between training and long-term inference to maintain high visual quality; and (3) History-context guidance, which aligns contextual information across adjacent clips to ensure temporal consistency. We further introduce LongVGenBench, a comprehensive benchmark comprising 100 high-resolution one-minute videos covering diverse real-world and synthetic environments. Extensive experiments demonstrate that LongVie 2 achieves state-of-the-art performance in long-range controllability, temporal coherence, and visual fidelity, and supports continuous video generation lasting up to five minutes, marking a significant step toward unified video world modeling.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T17:59:58Z</published>
    <arxiv:comment>Project Page: https://vchitect.github.io/LongVie2-project/</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Jianxiong Gao</name>
    </author>
    <author>
      <name>Zhaoxi Chen</name>
    </author>
    <author>
      <name>Xian Liu</name>
    </author>
    <author>
      <name>Junhao Zhuang</name>
    </author>
    <author>
      <name>Chengming Xu</name>
    </author>
    <author>
      <name>Jianfeng Feng</name>
    </author>
    <author>
      <name>Yu Qiao</name>
    </author>
    <author>
      <name>Yanwei Fu</name>
    </author>
    <author>
      <name>Chenyang Si</name>
    </author>
    <author>
      <name>Ziwei Liu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13600v1</id>
    <title>DA-SSL: self-supervised domain adaptor to leverage foundational models in turbt histopathology slides</title>
    <updated>2025-12-15T17:53:18Z</updated>
    <link href="https://arxiv.org/abs/2512.13600v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13600v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Recent deep learning frameworks in histopathology, particularly multiple instance learning (MIL) combined with pathology foundational models (PFMs), have shown strong performance. However, PFMs exhibit limitations on certain cancer or specimen types due to domain shifts - these cancer types were rarely used for pretraining or specimens contain tissue-based artifacts rarely seen within the pretraining population. Such is the case for transurethral resection of bladder tumor (TURBT), which are essential for diagnosing muscle-invasive bladder cancer (MIBC), but contain fragmented tissue chips and electrocautery artifacts and were not widely used in publicly available PFMs. To address this, we propose a simple yet effective domain-adaptive self-supervised adaptor (DA-SSL) that realigns pretrained PFM features to the TURBT domain without fine-tuning the foundational model itself. We pilot this framework for predicting treatment response in TURBT, where histomorphological features are currently underutilized and identifying patients who will benefit from neoadjuvant chemotherapy (NAC) is challenging. In our multi-center study, DA-SSL achieved an AUC of 0.77+/-0.04 in five-fold cross-validation and an external test accuracy of 0.84, sensitivity of 0.71, and specificity of 0.91 using majority voting. Our results demonstrate that lightweight domain adaptation with self-supervision can effectively enhance PFM-based MIL pipelines for clinically challenging histopathology tasks. Code is Available at https://github.com/zhanghaoyue/DA_SSL_TURBT.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T17:53:18Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Haoyue Zhang</name>
    </author>
    <author>
      <name>Meera Chappidi</name>
    </author>
    <author>
      <name>Erolcan Sayar</name>
    </author>
    <author>
      <name>Helen Richards</name>
    </author>
    <author>
      <name>Zhijun Chen</name>
    </author>
    <author>
      <name>Lucas Liu</name>
    </author>
    <author>
      <name>Roxanne Wadia</name>
    </author>
    <author>
      <name>Peter A Humphrey</name>
    </author>
    <author>
      <name>Fady Ghali</name>
    </author>
    <author>
      <name>Alberto Contreras-Sanz</name>
    </author>
    <author>
      <name>Peter Black</name>
    </author>
    <author>
      <name>Jonathan Wright</name>
    </author>
    <author>
      <name>Stephanie Harmon</name>
    </author>
    <author>
      <name>Michael Haffner</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13598v1</id>
    <title>Textual Gradients are a Flawed Metaphor for Automatic Prompt Optimization</title>
    <updated>2025-12-15T17:52:16Z</updated>
    <link href="https://arxiv.org/abs/2512.13598v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13598v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>A well-engineered prompt can increase the performance of large language models; automatic prompt optimization techniques aim to increase performance without requiring human effort to tune the prompts. One leading class of prompt optimization techniques introduces the analogy of textual gradients. We investigate the behavior of these textual gradient methods through a series of experiments and case studies. While such methods often result in a performance improvement, our experiments suggest that the gradient analogy does not accurately explain their behavior. Our insights may inform the selection of prompt optimization strategies, and development of new approaches.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T17:52:16Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Daniel Melcer</name>
    </author>
    <author>
      <name>Qi Chen</name>
    </author>
    <author>
      <name>Wen-Hao Chiang</name>
    </author>
    <author>
      <name>Shweta Garg</name>
    </author>
    <author>
      <name>Pranav Garg</name>
    </author>
    <author>
      <name>Christian Bock</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13597v1</id>
    <title>Lighting in Motion: Spatiotemporal HDR Lighting Estimation</title>
    <updated>2025-12-15T17:49:22Z</updated>
    <link href="https://arxiv.org/abs/2512.13597v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13597v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We present Lighting in Motion (LiMo), a diffusion-based approach to spatiotemporal lighting estimation. LiMo targets both realistic high-frequency detail prediction and accurate illuminance estimation. To account for both, we propose generating a set of mirrored and diffuse spheres at different exposures, based on their 3D positions in the input. Making use of diffusion priors, we fine-tune powerful existing diffusion models on a large-scale customized dataset of indoor and outdoor scenes, paired with spatiotemporal light probes. For accurate spatial conditioning, we demonstrate that depth alone is insufficient and we introduce a new geometric condition to provide the relative position of the scene to the target 3D position. Finally, we combine diffuse and mirror predictions at different exposures into a single HDRI map leveraging differentiable rendering. We thoroughly evaluate our method and design choices to establish LiMo as state-of-the-art for both spatial control and prediction accuracy.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T17:49:22Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Christophe Bolduc</name>
    </author>
    <author>
      <name>Julien Philip</name>
    </author>
    <author>
      <name>Li Ma</name>
    </author>
    <author>
      <name>Mingming He</name>
    </author>
    <author>
      <name>Paul Debevec</name>
    </author>
    <author>
      <name>Jean-François Lalonde</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13593v1</id>
    <title>Scalable Formal Verification via Autoencoder Latent Space Abstraction</title>
    <updated>2025-12-15T17:48:07Z</updated>
    <link href="https://arxiv.org/abs/2512.13593v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13593v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Finite Abstraction methods provide a powerful formal framework for proving that systems satisfy their specifications. However, these techniques face scalability challenges for high-dimensional systems, as they rely on state-space discretization which grows exponentially with dimension. Learning-based approaches to dimensionality reduction, utilizing neural networks and autoencoders, have shown great potential to alleviate this problem. However, ensuring the correctness of the resulting verification results remains an open question. In this work, we provide a formal approach to reduce the dimensionality of systems via convex autoencoders and learn the dynamics in the latent space through a kernel-based method. We then construct a finite abstraction from the learned model in the latent space and guarantee that the abstraction contains the true behaviors of the original system. We show that the verification results in the latent space can be mapped back to the original system. Finally, we demonstrate the effectiveness of our approach on multiple systems, including a 26D system controlled by a neural network, showing significant scalability improvements without loss of rigor.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T17:48:07Z</published>
    <arxiv:comment>14 pages, 7 figures, under review</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Robert Reed</name>
    </author>
    <author>
      <name>Morteza Lahijanian</name>
    </author>
    <author>
      <name>Luca Laurenti</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13592v1</id>
    <title>Image Diffusion Preview with Consistency Solver</title>
    <updated>2025-12-15T17:47:49Z</updated>
    <link href="https://arxiv.org/abs/2512.13592v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13592v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The slow inference process of image diffusion models significantly degrades interactive user experiences. To address this, we introduce Diffusion Preview, a novel paradigm employing rapid, low-step sampling to generate preliminary outputs for user evaluation, deferring full-step refinement until the preview is deemed satisfactory. Existing acceleration methods, including training-free solvers and post-training distillation, struggle to deliver high-quality previews or ensure consistency between previews and final outputs. We propose ConsistencySolver derived from general linear multistep methods, a lightweight, trainable high-order solver optimized via Reinforcement Learning, that enhances preview quality and consistency. Experimental results demonstrate that ConsistencySolver significantly improves generation quality and consistency in low-step scenarios, making it ideal for efficient preview-and-refine workflows. Notably, it achieves FID scores on-par with Multistep DPM-Solver using 47% fewer steps, while outperforming distillation baselines. Furthermore, user studies indicate our approach reduces overall user interaction time by nearly 50% while maintaining generation quality. Code is available at https://github.com/G-U-N/consolver.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T17:47:49Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Fu-Yun Wang</name>
    </author>
    <author>
      <name>Hao Zhou</name>
    </author>
    <author>
      <name>Liangzhe Yuan</name>
    </author>
    <author>
      <name>Sanghyun Woo</name>
    </author>
    <author>
      <name>Boqing Gong</name>
    </author>
    <author>
      <name>Bohyung Han</name>
    </author>
    <author>
      <name>Ming-Hsuan Yang</name>
    </author>
    <author>
      <name>Han Zhang</name>
    </author>
    <author>
      <name>Yukun Zhu</name>
    </author>
    <author>
      <name>Ting Liu</name>
    </author>
    <author>
      <name>Long Zhao</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13591v1</id>
    <title>astroCAMP: A Community Benchmark and Co-Design Framework for Sustainable SKA-Scale Radio Imaging</title>
    <updated>2025-12-15T17:47:28Z</updated>
    <link href="https://arxiv.org/abs/2512.13591v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13591v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The Square Kilometre Array (SKA) project will operate one of the world's largest continuous scientific data systems, sustaining petascale imaging under strict power caps. Yet, current radio-interferometric pipelines utilize only a small fraction of hardware peak performance, typically 4-14%, due to memory and I/O bottlenecks, resulting in poor energy efficiency and high operational and carbon costs. Progress is further limited by the absence of standardised metrics and fidelity tolerances, preventing principled hardware-software co-design and rigorous exploration of quality-efficiency trade-offs. We introduce astroCAMP, a framework for guiding the co-design of next-generation imaging pipelines and sustainable HPC architectures that maximise scientific return within SKA's operational and environmental limits. astroCAMP provides: (1) a unified, extensible metric suite covering scientific fidelity, computational performance, sustainability, and lifecycle economics; (2) standardised SKA-representative datasets and reference outputs enabling reproducible benchmarking across CPUs, GPUs, and emerging accelerators; and (3) a multi-objective co-design formulation linking scientific-quality constraints to time-, energy-, carbon-to-solution, and total cost of ownership. We release datasets, benchmarking results, and a reproducibility kit, and evaluate co-design metrics for WSClean and IDG on an AMD EPYC 9334 processor and an NVIDIA H100 GPU. Further, we illustrate the use of astroCAMP for heterogeneous CPU-FPGA design-space exploration, and its potential to facilitate the identification of Pareto-optimal operating points for SKA-scale imaging deployments. Last, we make a call to the SKA community to define quantifiable fidelity metrics and thresholds to accelerate principled optimisation for SKA-scale imaging.</summary>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T17:47:28Z</published>
    <arxiv:comment>11 pages, 10 figures</arxiv:comment>
    <arxiv:primary_category term="cs.DC"/>
    <author>
      <name>Denisa-Andreea Constantinescu</name>
    </author>
    <author>
      <name>Rubén Rodríguez Álvarez</name>
    </author>
    <author>
      <name>Jacques Morin</name>
    </author>
    <author>
      <name>Etienne Orliac</name>
    </author>
    <author>
      <name>Mickaël Dardaillon</name>
    </author>
    <author>
      <name>Sunrise Wang</name>
    </author>
    <author>
      <name>Hugo Miomandre</name>
    </author>
    <author>
      <name>Miguel Peón-Quirós</name>
    </author>
    <author>
      <name>Jean-François Nezan</name>
    </author>
    <author>
      <name>David Atienza</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13586v1</id>
    <title>ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding</title>
    <updated>2025-12-15T17:41:19Z</updated>
    <link href="https://arxiv.org/abs/2512.13586v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13586v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18$\times$ speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33$\times$ average speedup.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T17:41:19Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Jia-Nan Li</name>
    </author>
    <author>
      <name>Jian Guan</name>
    </author>
    <author>
      <name>Wei Wu</name>
    </author>
    <author>
      <name>Chongxuan Li</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13583v1</id>
    <title>DP-CSGP: Differentially Private Stochastic Gradient Push with Compressed Communication</title>
    <updated>2025-12-15T17:37:02Z</updated>
    <link href="https://arxiv.org/abs/2512.13583v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13583v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>In this paper, we propose a Differentially Private Stochastic Gradient Push with Compressed communication (termed DP-CSGP) for decentralized learning over directed graphs. Different from existing works, the proposed algorithm is designed to maintain high model utility while ensuring both rigorous differential privacy (DP) guarantees and efficient communication. For general non-convex and smooth objective functions, we show that the proposed algorithm achieves a tight utility bound of $\mathcal{O}\left( \sqrt{d\log \left( \frac{1}δ \right)}/(\sqrt{n}Jε) \right)$ ($J$ and $d$ are the number of local samples and the dimension of decision variables, respectively) with $\left(ε, δ\right)$-DP guarantee for each node, matching that of decentralized counterparts with exact communication. Extensive experiments on benchmark tasks show that, under the same privacy budget, DP-CSGP achieves comparable model accuracy with significantly lower communication cost than existing decentralized counterparts with exact communication.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T17:37:02Z</published>
    <arxiv:comment>13 pages</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Zehan Zhu</name>
    </author>
    <author>
      <name>Heng Zhao</name>
    </author>
    <author>
      <name>Yan Huang</name>
    </author>
    <author>
      <name>Joey Tianyi Zhou</name>
    </author>
    <author>
      <name>Shouling Ji</name>
    </author>
    <author>
      <name>Jinming Xu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13576v1</id>
    <title>Reproducing and Dissecting Denoising Language Models for Speech Recognition</title>
    <updated>2025-12-15T17:33:22Z</updated>
    <link href="https://arxiv.org/abs/2512.13576v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13576v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Denoising language models (DLMs) have been proposed as a powerful alternative to traditional language models (LMs) for automatic speech recognition (ASR), motivated by their ability to use bidirectional context and adapt to a specific ASR model's error patterns. However, the complexity of the DLM training pipeline has hindered wider investigation. This paper presents the first independent, large-scale empirical study of DLMs. We build and release a complete, reproducible pipeline to systematically investigate the impact of key design choices. We evaluate dozens of configurations across multiple axes, including various data augmentation techniques (e.g., SpecAugment, dropout, mixup), different text-to-speech systems, and multiple decoding strategies. Our comparative analysis in a common subword vocabulary setting demonstrates that DLMs outperform traditional LMs, but only after a distinct compute tipping point. While LMs are more efficient at lower budgets, DLMs scale better with longer training, mirroring behaviors observed in diffusion language models. However, we observe smaller improvements than those reported in prior character-based work, which indicates that the DLM's performance is conditional on factors such as the vocabulary. Our analysis reveals that a key factor for improving performance is to condition the DLM on richer information from the ASR's hypothesis space, rather than just a single best guess. To this end, we introduce DLM-sum, a novel method for decoding from multiple ASR hypotheses, which consistently outperforms the previously proposed DSR decoding method. We believe our findings and public pipeline provide a crucial foundation for the community to better understand, improve, and build upon this promising class of models. The code is publicly available at https://github.com/rwth-i6/2025-denoising-lm/.</summary>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T17:33:22Z</published>
    <arxiv:comment>Equal contribution of first two authors</arxiv:comment>
    <arxiv:primary_category term="cs.NE"/>
    <author>
      <name>Dorian Koch</name>
      <arxiv:affiliation>Machine Learning and Human Language Technology, RWTH Aachen University</arxiv:affiliation>
    </author>
    <author>
      <name>Albert Zeyer</name>
      <arxiv:affiliation>Machine Learning and Human Language Technology, RWTH Aachen University</arxiv:affiliation>
      <arxiv:affiliation>AppTek</arxiv:affiliation>
    </author>
    <author>
      <name>Nick Rossenbach</name>
      <arxiv:affiliation>Machine Learning and Human Language Technology, RWTH Aachen University</arxiv:affiliation>
      <arxiv:affiliation>AppTek</arxiv:affiliation>
    </author>
    <author>
      <name>Ralf Schlüter</name>
      <arxiv:affiliation>Machine Learning and Human Language Technology, RWTH Aachen University</arxiv:affiliation>
      <arxiv:affiliation>AppTek</arxiv:affiliation>
    </author>
    <author>
      <name>Hermann Ney</name>
      <arxiv:affiliation>Machine Learning and Human Language Technology, RWTH Aachen University</arxiv:affiliation>
      <arxiv:affiliation>AppTek</arxiv:affiliation>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13573v1</id>
    <title>MMhops-R1: Multimodal Multi-hop Reasoning</title>
    <updated>2025-12-15T17:29:02Z</updated>
    <link href="https://arxiv.org/abs/2512.13573v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13573v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The ability to perform multi-modal multi-hop reasoning by iteratively integrating information across various modalities and external knowledge is critical for addressing complex real-world challenges. However, existing Multi-modal Large Language Models (MLLMs) are predominantly limited to single-step reasoning, as existing benchmarks lack the complexity needed to evaluate and drive multi-hop abilities. To bridge this gap, we introduce MMhops, a novel, large-scale benchmark designed to systematically evaluate and foster multi-modal multi-hop reasoning. MMhops dataset comprises two challenging task formats, Bridging and Comparison, which necessitate that models dynamically construct complex reasoning chains by integrating external knowledge. To tackle the challenges posed by MMhops, we propose MMhops-R1, a novel multi-modal Retrieval-Augmented Generation (mRAG) framework for dynamic reasoning. Our framework utilizes reinforcement learning to optimize the model for autonomously planning reasoning paths, formulating targeted queries, and synthesizing multi-level information. Comprehensive experiments demonstrate that MMhops-R1 significantly outperforms strong baselines on MMhops, highlighting that dynamic planning and multi-modal knowledge integration are crucial for complex reasoning. Moreover, MMhops-R1 demonstrates strong generalization to tasks requiring fixed-hop reasoning, underscoring the robustness of our dynamic planning approach. In conclusion, our work contributes a challenging new benchmark and a powerful baseline model, and we will release the associated code, data, and weights to catalyze future research in this critical area.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T17:29:02Z</published>
    <arxiv:comment>Acceped by AAAI 2026</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Tao Zhang</name>
    </author>
    <author>
      <name>Ziqi Zhang</name>
    </author>
    <author>
      <name>Zongyang Ma</name>
    </author>
    <author>
      <name>Yuxin Chen</name>
    </author>
    <author>
      <name>Bing Li</name>
    </author>
    <author>
      <name>Chunfeng Yuan</name>
    </author>
    <author>
      <name>Guangting Wang</name>
    </author>
    <author>
      <name>Fengyun Rao</name>
    </author>
    <author>
      <name>Ying Shan</name>
    </author>
    <author>
      <name>Weiming Hu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13568v1</id>
    <title>Superposition as Lossy Compression: Measure with Sparse Autoencoders and Connect to Adversarial Vulnerability</title>
    <updated>2025-12-15T17:25:39Z</updated>
    <link href="https://arxiv.org/abs/2512.13568v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13568v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Neural networks achieve remarkable performance through superposition: encoding multiple features as overlapping directions in activation space rather than dedicating individual neurons to each feature. This challenges interpretability, yet we lack principled methods to measure superposition. We present an information-theoretic framework measuring a neural representation's effective degrees of freedom. We apply Shannon entropy to sparse autoencoder activations to compute the number of effective features as the minimum neurons needed for interference-free encoding. Equivalently, this measures how many "virtual neurons" the network simulates through superposition. When networks encode more effective features than actual neurons, they must accept interference as the price of compression. Our metric strongly correlates with ground truth in toy models, detects minimal superposition in algorithmic tasks, and reveals systematic reduction under dropout. Layer-wise patterns mirror intrinsic dimensionality studies on Pythia-70M. The metric also captures developmental dynamics, detecting sharp feature consolidation during grokking. Surprisingly, adversarial training can increase effective features while improving robustness, contradicting the hypothesis that superposition causes vulnerability. Instead, the effect depends on task complexity and network capacity: simple tasks with ample capacity allow feature expansion (abundance regime), while complex tasks or limited capacity force reduction (scarcity regime). By defining superposition as lossy compression, this work enables principled measurement of how neural networks organize information under computational constraints, connecting superposition to adversarial robustness.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T17:25:39Z</published>
    <arxiv:comment>Accepted to TMLR, view HTML here: https://leonardbereska.github.io/blog/2025/superposition/</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <arxiv:journal_ref>Transactions on Machine Learning Research, 2025</arxiv:journal_ref>
    <author>
      <name>Leonard Bereska</name>
    </author>
    <author>
      <name>Zoe Tzifa-Kratira</name>
    </author>
    <author>
      <name>Reza Samavi</name>
    </author>
    <author>
      <name>Efstratios Gavves</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13565v1</id>
    <title>A Nonparametric Statistics Approach to Feature Selection in Deep Neural Networks with Theoretical Guarantees</title>
    <updated>2025-12-15T17:22:49Z</updated>
    <link href="https://arxiv.org/abs/2512.13565v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13565v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>This paper tackles the problem of feature selection in a highly challenging setting: $\mathbb{E}(y | \boldsymbol{x}) = G(\boldsymbol{x}_{\mathcal{S}_0})$, where $\mathcal{S}_0$ is the set of relevant features and $G$ is an unknown, potentially nonlinear function subject to mild smoothness conditions. Our approach begins with feature selection in deep neural networks, then generalizes the results to H{ö}lder smooth functions by exploiting the strong approximation capabilities of neural networks. Unlike conventional optimization-based deep learning methods, we reformulate neural networks as index models and estimate $\mathcal{S}_0$ using the second-order Stein's formula. This gradient-descent-free strategy guarantees feature selection consistency with a sample size requirement of $n = Ω(p^2)$, where $p$ is the feature dimension. To handle high-dimensional scenarios, we further introduce a screening-and-selection mechanism that achieves nonlinear selection consistency when $n = Ω(s \log p)$, with $s$ representing the sparsity level. Additionally, we refit a neural network on the selected features for prediction and establish performance guarantees under a relaxed sparsity assumption. Extensive simulations and real-data analyses demonstrate the strong performance of our method even in the presence of complex feature interactions.</summary>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T17:22:49Z</published>
    <arxiv:comment>64 pages</arxiv:comment>
    <arxiv:primary_category term="stat.ML"/>
    <author>
      <name>Junye Du</name>
    </author>
    <author>
      <name>Zhenghao Li</name>
    </author>
    <author>
      <name>Zhutong Gu</name>
    </author>
    <author>
      <name>Long Feng</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13564v1</id>
    <title>Memory in the Age of AI Agents</title>
    <updated>2025-12-15T17:22:34Z</updated>
    <link href="https://arxiv.org/abs/2512.13564v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13564v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As research on agent memory rapidly expands and attracts unprecedented attention, the field has also become increasingly fragmented. Existing works that fall under the umbrella of agent memory often differ substantially in their motivations, implementations, and evaluation protocols, while the proliferation of loosely defined memory terminologies has further obscured conceptual clarity. Traditional taxonomies such as long/short-term memory have proven insufficient to capture the diversity of contemporary agent memory systems. This work aims to provide an up-to-date landscape of current agent memory research. We begin by clearly delineating the scope of agent memory and distinguishing it from related concepts such as LLM memory, retrieval augmented generation (RAG), and context engineering. We then examine agent memory through the unified lenses of forms, functions, and dynamics. From the perspective of forms, we identify three dominant realizations of agent memory, namely token-level, parametric, and latent memory. From the perspective of functions, we propose a finer-grained taxonomy that distinguishes factual, experiential, and working memory. From the perspective of dynamics, we analyze how memory is formed, evolved, and retrieved over time. To support practical development, we compile a comprehensive summary of memory benchmarks and open-source frameworks. Beyond consolidation, we articulate a forward-looking perspective on emerging research frontiers, including memory automation, reinforcement learning integration, multimodal memory, multi-agent memory, and trustworthiness issues. We hope this survey serves not only as a reference for existing work, but also as a conceptual foundation for rethinking memory as a first-class primitive in the design of future agentic intelligence.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T17:22:34Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Yuyang Hu</name>
    </author>
    <author>
      <name>Shichun Liu</name>
    </author>
    <author>
      <name>Yanwei Yue</name>
    </author>
    <author>
      <name>Guibin Zhang</name>
    </author>
    <author>
      <name>Boyang Liu</name>
    </author>
    <author>
      <name>Fangyi Zhu</name>
    </author>
    <author>
      <name>Jiahang Lin</name>
    </author>
    <author>
      <name>Honglin Guo</name>
    </author>
    <author>
      <name>Shihan Dou</name>
    </author>
    <author>
      <name>Zhiheng Xi</name>
    </author>
    <author>
      <name>Senjie Jin</name>
    </author>
    <author>
      <name>Jiejun Tan</name>
    </author>
    <author>
      <name>Yanbin Yin</name>
    </author>
    <author>
      <name>Jiongnan Liu</name>
    </author>
    <author>
      <name>Zeyu Zhang</name>
    </author>
    <author>
      <name>Zhongxiang Sun</name>
    </author>
    <author>
      <name>Yutao Zhu</name>
    </author>
    <author>
      <name>Hao Sun</name>
    </author>
    <author>
      <name>Boci Peng</name>
    </author>
    <author>
      <name>Zhenrong Cheng</name>
    </author>
    <author>
      <name>Xuanbo Fan</name>
    </author>
    <author>
      <name>Jiaxin Guo</name>
    </author>
    <author>
      <name>Xinlei Yu</name>
    </author>
    <author>
      <name>Zhenhong Zhou</name>
    </author>
    <author>
      <name>Zewen Hu</name>
    </author>
    <author>
      <name>Jiahao Huo</name>
    </author>
    <author>
      <name>Junhao Wang</name>
    </author>
    <author>
      <name>Yuwei Niu</name>
    </author>
    <author>
      <name>Yu Wang</name>
    </author>
    <author>
      <name>Zhenfei Yin</name>
    </author>
    <author>
      <name>Xiaobin Hu</name>
    </author>
    <author>
      <name>Yue Liao</name>
    </author>
    <author>
      <name>Qiankun Li</name>
    </author>
    <author>
      <name>Kun Wang</name>
    </author>
    <author>
      <name>Wangchunshu Zhou</name>
    </author>
    <author>
      <name>Yixin Liu</name>
    </author>
    <author>
      <name>Dawei Cheng</name>
    </author>
    <author>
      <name>Qi Zhang</name>
    </author>
    <author>
      <name>Tao Gui</name>
    </author>
    <author>
      <name>Shirui Pan</name>
    </author>
    <author>
      <name>Yan Zhang</name>
    </author>
    <author>
      <name>Philip Torr</name>
    </author>
    <author>
      <name>Zhicheng Dou</name>
    </author>
    <author>
      <name>Ji-Rong Wen</name>
    </author>
    <author>
      <name>Xuanjing Huang</name>
    </author>
    <author>
      <name>Yu-Gang Jiang</name>
    </author>
    <author>
      <name>Shuicheng Yan</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13560v1</id>
    <title>3D Human-Human Interaction Anomaly Detection</title>
    <updated>2025-12-15T17:17:55Z</updated>
    <link href="https://arxiv.org/abs/2512.13560v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13560v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Human-centric anomaly detection (AD) has been primarily studied to specify anomalous behaviors in a single person. However, as humans by nature tend to act in a collaborative manner, behavioral anomalies can also arise from human-human interactions. Detecting such anomalies using existing single-person AD models is prone to low accuracy, as these approaches are typically not designed to capture the complex and asymmetric dynamics of interactions. In this paper, we introduce a novel task, Human-Human Interaction Anomaly Detection (H2IAD), which aims to identify anomalous interactive behaviors within collaborative 3D human actions. To address H2IAD, we then propose Interaction Anomaly Detection Network (IADNet), which is formalized with a Temporal Attention Sharing Module (TASM). Specifically, in designing TASM, we share the encoded motion embeddings across both people such that collaborative motion correlations can be effectively synchronized. Moreover, we notice that in addition to temporal dynamics, human interactions are also characterized by spatial configurations between two people. We thus introduce a Distance-Based Relational Encoding Module (DREM) to better reflect social cues in H2IAD. The normalizing flow is eventually employed for anomaly scoring. Extensive experiments on human-human motion benchmarks demonstrate that IADNet outperforms existing Human-centric AD baselines in H2IAD.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T17:17:55Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Shun Maeda</name>
    </author>
    <author>
      <name>Chunzhi Gu</name>
    </author>
    <author>
      <name>Koichiro Kamide</name>
    </author>
    <author>
      <name>Katsuya Hotta</name>
    </author>
    <author>
      <name>Shangce Gao</name>
    </author>
    <author>
      <name>Chao Zhang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13559v1</id>
    <title>Verifying Rumors via Stance-Aware Structural Modeling</title>
    <updated>2025-12-15T17:16:56Z</updated>
    <link href="https://arxiv.org/abs/2512.13559v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13559v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Verifying rumors on social media is critical for mitigating the spread of false information. The stances of conversation replies often provide important cues to determine a rumor's veracity. However, existing models struggle to jointly capture semantic content, stance information, and conversation strructure, especially under the sequence length constraints of transformer-based encoders. In this work, we propose a stance-aware structural modeling that encodes each post in a discourse with its stance signal and aggregates reply embedddings by stance category enabling a scalable and semantically enriched representation of the entire thread. To enhance structural awareness, we introduce stance distribution and hierarchical depth as covariates, capturing stance imbalance and the influence of reply depth. Extensive experiments on benchmark datasets demonstrate that our approach significantly outperforms prior methods in the ability to predict truthfulness of a rumor. We also demonstrate that our model is versatile for early detection and cross-platfrom generalization.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T17:16:56Z</published>
    <arxiv:comment>8 pages, 2 figures, published in The 24th IEEE/WIC International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT 2025), London, UK, 2025</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Gibson Nkhata</name>
    </author>
    <author>
      <name>Uttamasha Anjally Oyshi</name>
    </author>
    <author>
      <name>Quan Mai</name>
    </author>
    <author>
      <name>Susan Gauch</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13552v1</id>
    <title>PrahokBART: A Pre-trained Sequence-to-Sequence Model for Khmer Natural Language Generation</title>
    <updated>2025-12-15T17:11:31Z</updated>
    <link href="https://arxiv.org/abs/2512.13552v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13552v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>This work introduces {\it PrahokBART}, a compact pre-trained sequence-to-sequence model trained from scratch for Khmer using carefully curated Khmer and English corpora. We focus on improving the pre-training corpus quality and addressing the linguistic issues of Khmer, which are ignored in existing multilingual models, by incorporating linguistic components such as word segmentation and normalization. We evaluate PrahokBART on three generative tasks: machine translation, text summarization, and headline generation, where our results demonstrate that it outperforms mBART50, a strong multilingual pre-trained model. Additionally, our analysis provides insights into the impact of each linguistic module and evaluates how effectively our model handles space during text generation, which is crucial for the naturalness of texts in Khmer.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T17:11:31Z</published>
    <arxiv:comment>Published at COLING 2025, 14 pages</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Hour Kaing</name>
    </author>
    <author>
      <name>Raj Dabre</name>
    </author>
    <author>
      <name>Haiyue Song</name>
    </author>
    <author>
      <name>Van-Hien Tran</name>
    </author>
    <author>
      <name>Hideki Tanaka</name>
    </author>
    <author>
      <name>Masao Utiyama</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13534v1</id>
    <title>Pancakes: Consistent Multi-Protocol Image Segmentation Across Biomedical Domains</title>
    <updated>2025-12-15T17:00:21Z</updated>
    <link href="https://arxiv.org/abs/2512.13534v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13534v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>A single biomedical image can be meaningfully segmented in multiple ways, depending on the desired application. For instance, a brain MRI can be segmented according to tissue types, vascular territories, broad anatomical regions, fine-grained anatomy, or pathology, etc. Existing automatic segmentation models typically either (1) support only a single protocol, the one they were trained on, or (2) require labor-intensive manual prompting to specify the desired segmentation. We introduce Pancakes, a framework that, given a new image from a previously unseen domain, automatically generates multi-label segmentation maps for multiple plausible protocols, while maintaining semantic consistency across related images. Pancakes introduces a new problem formulation that is not currently attainable by existing foundation models. In a series of experiments on seven held-out datasets, we demonstrate that our model can significantly outperform existing foundation models in producing several plausible whole-image segmentations, that are semantically coherent across images.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T17:00:21Z</published>
    <arxiv:comment>Accepted at NeurIPS 2025. Code available at: https://github.com/mariannerakic/Pancakes</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Marianne Rakic</name>
    </author>
    <author>
      <name>Siyu Gai</name>
    </author>
    <author>
      <name>Etienne Chollet</name>
    </author>
    <author>
      <name>John V. Guttag</name>
    </author>
    <author>
      <name>Adrian V. Dalca</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13532v1</id>
    <title>Adaptive Sampling for Hydrodynamic Stability</title>
    <updated>2025-12-15T17:00:09Z</updated>
    <link href="https://arxiv.org/abs/2512.13532v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13532v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>An adaptive sampling approach for efficient detection of bifurcation boundaries in parametrized fluid flow problems is presented herein. The study extends the machine-learning approach of Silvester (Machine Learning for Hydrodynamic Stability, arXiv:2407.09572), where a classifier network was trained on preselected simulation data to identify bifurcated and nonbifurcated flow regimes. In contrast, the proposed methodology introduces adaptivity through a flow-based deep generative model that automatically refines the sampling of the parameter space. The strategy has two components: a classifier network maps the flow parameters to a bifurcation probability, and a probability density estimation technique (KRnet) for the generation of new samples at each adaptive step. The classifier output provides a probabilistic measure of flow stability, and the Shannon entropy of these predictions is employed as an uncertainty indicator. KRnet is trained to approximate a probability density function that concentrates sampling in regions of high entropy, thereby directing computational effort towards the evolving bifurcation boundary. This coupling between classification and generative modeling establishes a feedback-driven adaptive learning process analogous to error-indicator based refinement in contemporary partial differential equation solution strategies. Starting from a uniform parameter distribution, the new approach achieves accurate bifurcation boundary identification with significantly fewer Navier--Stokes simulations, providing a scalable foundation for high-dimensional stability analysis.</summary>
    <category term="physics.flu-dyn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T17:00:09Z</published>
    <arxiv:primary_category term="physics.flu-dyn"/>
    <author>
      <name>Anshima Singh</name>
    </author>
    <author>
      <name>David J. Silvester</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13530v1</id>
    <title>Actively Learning Joint Contours of Multiple Computer Experiments</title>
    <updated>2025-12-15T17:00:04Z</updated>
    <link href="https://arxiv.org/abs/2512.13530v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13530v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Contour location$\unicode{x2014}$the process of sequentially training a surrogate model to identify the design inputs that result in a pre-specified response value from a single computer experiment$\unicode{x2014}$is a well-studied active learning problem. Here, we tackle a related but distinct problem: identifying the input configuration that returns pre-specified values of multiple independent computer experiments simultaneously. Motivated by computer experiments of the rotational torques acting upon a vehicle in flight, we aim to identify stable flight conditions which result in zero torque forces. We propose a "joint contour location" (jCL) scheme that strikes a strategic balance between exploring the multiple response surfaces while exploiting learning of the intersecting contours. We employ both shallow and deep Gaussian process surrogates, but our jCL procedure is applicable to any surrogate that can provide posterior predictive distributions. Our jCL designs significantly outperform existing (single response) CL strategies, enabling us to efficiently locate the joint contour of our motivating computer experiments.</summary>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T17:00:04Z</published>
    <arxiv:primary_category term="stat.ME"/>
    <author>
      <name>Shih-Ni Prim</name>
    </author>
    <author>
      <name>Kevin R. Quinlan</name>
    </author>
    <author>
      <name>Paul Hawkins</name>
    </author>
    <author>
      <name>Jagadeesh Movva</name>
    </author>
    <author>
      <name>Annie S. Booth</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13529v1</id>
    <title>Enhancing lithological interpretation from petrophysical well log of IODP expedition 390/393 using machine learning</title>
    <updated>2025-12-15T16:59:13Z</updated>
    <link href="https://arxiv.org/abs/2512.13529v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13529v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Enhanced lithological interpretation from well logs plays a key role in geological resource exploration and mapping, as well as in geo-environmental modeling studies. Core and cutting information is useful for making sound interpretations of well logs; however, these are rarely collected at each depth due to high costs. Moreover, well log interpretation using traditional methods is constrained by poor borehole conditions. Traditional statistical methods are mostly linear, often failing to discriminate between lithology and rock facies, particularly when dealing with overlapping well log signals characterized by the structural and compositional variation of rock types. In this study, we develop multiple supervised and unsupervised machine learning algorithms to jointly analyze multivariate well log data from Integrated Ocean Drilling Program (IODP) expeditions 390 and 393 for enhanced lithological interpretations. Among the algorithms, Logistic Regression, Decision Trees, Gradient Boosting, Support Vector Machines (SVM), k-Nearest Neighbors (KNN), and Multi-Layer Perceptron (MLP) neural network models, the Decision Tree and Gradient Boosting models outperformed the others, achieving an accuracy of 0.9950 and an F1-score of 0.9951. While unsupervised machine learning (ML) provides the foundation for cluster information that inherently supports the classification algorithm, supervised ML is applied to devise a data-driven lithology clustering mechanism for IODP datasets. The joint ML-based method developed here has the potential to be further explored for analyzing other well log datasets from the world's oceans.</summary>
    <category term="physics.geo-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T16:59:13Z</published>
    <arxiv:comment>Accepted for presentation at the International Meeting for Applied Geoscience &amp; Energy (IMAGE) 2025</arxiv:comment>
    <arxiv:primary_category term="physics.geo-ph"/>
    <author>
      <name>Raj Sahu</name>
    </author>
    <author>
      <name>Saumen Maiti</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13526v1</id>
    <title>Async Control: Stress-testing Asynchronous Control Measures for LLM Agents</title>
    <updated>2025-12-15T16:56:28Z</updated>
    <link href="https://arxiv.org/abs/2512.13526v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13526v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>LLM-based software engineering agents are increasingly used in real-world development tasks, often with access to sensitive data or security-critical codebases. Such agents could intentionally sabotage these codebases if they were misaligned. We investigate asynchronous monitoring, in which a monitoring system reviews agent actions after the fact. Unlike synchronous monitoring, this approach does not impose runtime latency, while still attempting to disrupt attacks before irreversible harm occurs. We treat monitor development as an adversarial game between a blue team (who design monitors) and a red team (who create sabotaging agents). We attempt to set the game rules such that they upper bound the sabotage potential of an agent based on Claude 4.1 Opus. To ground this game in a realistic, high-stakes deployment scenario, we develop a suite of 5 diverse software engineering environments that simulate tasks that an agent might perform within an AI developer's internal infrastructure. Over the course of the game, we develop an ensemble monitor that achieves a 6% false negative rate at 1% false positive rate on a held out test environment. Then, we estimate risk of sabotage at deployment time by extrapolating from our monitor's false negative rate. We describe one simple model for this extrapolation, present a sensitivity analysis, and describe situations in which the model would be invalid. Code is available at: https://github.com/UKGovernmentBEIS/async-control.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T16:56:28Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Asa Cooper Stickland</name>
    </author>
    <author>
      <name>Jan Michelfeit</name>
    </author>
    <author>
      <name>Arathi Mani</name>
    </author>
    <author>
      <name>Charlie Griffin</name>
    </author>
    <author>
      <name>Ollie Matthews</name>
    </author>
    <author>
      <name>Tomek Korbak</name>
    </author>
    <author>
      <name>Rogan Inglis</name>
    </author>
    <author>
      <name>Oliver Makins</name>
    </author>
    <author>
      <name>Alan Cooney</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13525v1</id>
    <title>Janus: Disaggregating Attention and Experts for Scalable MoE Inference</title>
    <updated>2025-12-15T16:53:49Z</updated>
    <link href="https://arxiv.org/abs/2512.13525v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13525v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Large Mixture-of-Experts (MoE) model inference is challenging due to high resource demands and dynamic workloads. Existing solutions often deploy the entire model as a single monolithic unit, which applies a unified resource configuration to both attention and expert modules despite their different requirements, leading to limited scalability and resource inefficiency. In this paper, we propose Janus, a scalable MoE inference system that disaggregates attention and experts on separate GPU sub-clusters, enabling each module to be managed and scaled independently. Janus incorporates three key designs for efficient, disaggregated MoE inference. First, it proposes an adaptive two-phase communication scheme that exploits intra- and inter-node bandwidth hierarchies for low-latency data exchange. Second, motivated by the memory-bound nature of MoE modules, Janus introduces a lightweight scheduler and implements it as a GPU kernel to balance the number of activated experts across GPUs at minimal overhead, thereby reducing inference latency. Third, Janus performs fine-grained resource management to dynamically adjust expert placement and independently scale attention and MoE resources to improve overall efficiency. Evaluation shows Janus achieves up to 3.9 higher perGPU throughput than state-of-the-art systems while meeting per-token latency requirements.</summary>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T16:53:49Z</published>
    <arxiv:primary_category term="cs.DC"/>
    <author>
      <name>Zhexiang Zhang</name>
    </author>
    <author>
      <name>Ye Wang</name>
    </author>
    <author>
      <name>Xiangyu Wang</name>
    </author>
    <author>
      <name>Yumiao Zhao</name>
    </author>
    <author>
      <name>Jingzhe Jiang</name>
    </author>
    <author>
      <name>Qizhen Weng</name>
    </author>
    <author>
      <name>Shaohuai Shi</name>
    </author>
    <author>
      <name>Yin Chen</name>
    </author>
    <author>
      <name>Minchen Yu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13524v1</id>
    <title>How Low Can You Go? The Data-Light SE Challenge</title>
    <updated>2025-12-15T16:49:50Z</updated>
    <link href="https://arxiv.org/abs/2512.13524v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13524v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Much of software engineering (SE) research assumes that progress depends on massive datasets and CPU-intensive optimizers. Yet has this assumption been rigorously tested?
  The counter-evidence presented in this paper suggests otherwise: across dozens of optimization problems from recent SE literature, including software configuration and performance tuning, cloud and systems optimization, project and process-level decision modeling, behavioral analytics, financial risk modeling, project health prediction, reinforcement learning tasks, sales forecasting, and software testing, even with just a few dozen labels, very simple methods (e.g. diversity sampling, a minimal Bayesian learner, or random probes) achieve near 90% of the best reported results. Further, these simple methods perform just as well as more state-of-the-the-art optimizers like SMAC, TPE, DEHB etc. While some tasks would require better outcomes and more sampling, these results seen after a few dozen samples would suffice for many engineering needs (particularly when the goal is rapid and cost-efficient guidance rather than slow and exhaustive optimization).
  Our results highlight that some SE tasks may be better served by lightweight approaches that demand fewer labels and far less computation. We hence propose the data-light challenge: when will a handful of labels suffice for SE tasks? To enable a large-scale investigation of this issue, we contribute (1) a mathematical formalization of labeling, (2) lightweight baseline algorithms, and (3) results on public-domain data showing the conditions under which lightweight methods excel or fail.
  For the purposes of open science, our scripts and data are online at https://github.com/KKGanguly/NEO .</summary>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T16:49:50Z</published>
    <arxiv:primary_category term="cs.SE"/>
    <author>
      <name>Kishan Kumar Ganguly</name>
    </author>
    <author>
      <name>Tim Menzies</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13517v1</id>
    <title>A Deep Learning Model of Mental Rotation Informed by Interactive VR Experiments</title>
    <updated>2025-12-15T16:43:50Z</updated>
    <link href="https://arxiv.org/abs/2512.13517v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13517v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Mental rotation -- the ability to compare objects seen from different viewpoints -- is a fundamental example of mental simulation and spatial world modelling in humans. Here we propose a mechanistic model of human mental rotation, leveraging advances in deep, equivariant, and neuro-symbolic learning. Our model consists of three stacked components: (1) an equivariant neural encoder, taking images as input and producing 3D spatial representations of objects, (2) a neuro-symbolic object encoder, deriving symbolic descriptions of objects from these spatial representations, and (3) a neural decision agent, comparing these symbolic descriptions to prescribe rotation simulations in 3D latent space via a recurrent pathway. Our model design is guided by the abundant experimental literature on mental rotation, which we complemented with experiments in VR where participants could at times manipulate the objects to compare, providing us with additional insights into the cognitive process of mental rotation. Our model captures well the performance, response times and behavior of participants in our and others' experiments. The necessity of each model component is shown through systematic ablations. Our work adds to a recent collection of deep neural models of human spatial reasoning, further demonstrating the potency of integrating deep, equivariant, and symbolic representations to model the human mind.</summary>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T16:43:50Z</published>
    <arxiv:primary_category term="q-bio.NC"/>
    <author>
      <name>Raymond Khazoum</name>
    </author>
    <author>
      <name>Daniela Fernandes</name>
    </author>
    <author>
      <name>Aleksandr Krylov</name>
    </author>
    <author>
      <name>Qin Li</name>
    </author>
    <author>
      <name>Stephane Deny</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13515v1</id>
    <title>Fine-tuned LLM-based Code Migration Framework</title>
    <updated>2025-12-15T16:42:51Z</updated>
    <link href="https://arxiv.org/abs/2512.13515v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13515v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The study presents the outcomes of research and experimental validation in the domain of automated codebase migration, with a focus on addressing challenges in transitioning SQL-based systems. The proposed method for migration essentially appears as a framework that leverages the best aspects of traditional software engineering techniques and provides an iterative, scalable, precise and efficient solution for modern database transformations. The central piece of the approach is the integration of a fine-tuned Large Language Model to address critical issues in SQL code conversion, such as syntax mapping, resolving discrepancies between Oracle PL/SQL and PostgreSQL, and optimising database elements such as stored procedures, triggers, views, and overall database logic. Thus, the method involves a trade-off between fine-tuning and prompt engineering. Special attention is given to a fine-tuning approach, which enhances the adaptability and compatibility with migration requirements across the entire database. According to the achieved results, fine-tuning plays a very important role. The study employs targeted evaluation methodologies along with computational metrics to measure the success of iterative conversion cycles. Core innovations include automated SQL feature detection, semi-supervised error analysis and integration of Subject Matter Experts feedback within a systematic migration workflow. The methodology achieves significant reductions in Syntax Error Rates, enhances feature alignment throughout migration iterations, and leverages dataset sampling to ensure continual improvement. By embedding GAI into the migration process, the framework facilitates precise feature mapping, semi-automated error resolution, and data-driven optimisation loops, improving workflow efficiency.</summary>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T16:42:51Z</published>
    <arxiv:comment>16 pages, 27 figures, 7 references</arxiv:comment>
    <arxiv:primary_category term="cs.SE"/>
    <author>
      <name>Oleg Grynets</name>
    </author>
    <author>
      <name>Vasyl Lyashkevych</name>
    </author>
    <author>
      <name>Dmytro Baran</name>
    </author>
    <author>
      <name>Maksym Orliansky</name>
    </author>
    <author>
      <name>Taras Zelenyy</name>
    </author>
    <author>
      <name>Markiian Leshchyshyn</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13511v1</id>
    <title>TARA: Simple and Efficient Time Aware Retrieval Adaptation of MLLMs for Video Understanding</title>
    <updated>2025-12-15T16:38:59Z</updated>
    <link href="https://arxiv.org/abs/2512.13511v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13511v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Our objective is to build a general time-aware video-text embedding model for retrieval. To that end, we propose a simple and efficient recipe, dubbed TARA (Time Aware Retrieval Adaptation), to adapt Multimodal LLMs (MLLMs) to a time-aware video-text embedding model without using any video data at all. For evaluating time-awareness in retrieval, we propose a new benchmark with temporally opposite (chiral) actions as hard negatives and curated splits for chiral and non-chiral actions. We show that TARA outperforms all existing video-text models on this chiral benchmark while also achieving strong results on standard benchmarks. Furthermore, we discover additional benefits of TARA beyond time-awareness: (i) TARA embeddings are negation-aware as shown in NegBench benchmark that evaluates negation in video retrieval, (ii) TARA achieves state of the art performance on verb and adverb understanding in videos. Overall, TARA yields a strong, versatile, time-aware video-text embedding model with state of the art zero-shot performance.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T16:38:59Z</published>
    <arxiv:comment>18 Pages. Project page at http://bpiyush.github.io/tara-website</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Piyush Bagad</name>
    </author>
    <author>
      <name>Andrew Zisserman</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13510v1</id>
    <title>MedCEG: Reinforcing Verifiable Medical Reasoning with Critical Evidence Graph</title>
    <updated>2025-12-15T16:38:46Z</updated>
    <link href="https://arxiv.org/abs/2512.13510v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13510v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Large language models with reasoning capabilities have demonstrated impressive performance across a wide range of domains. In clinical applications, a transparent, step-by-step reasoning process provides physicians with strong evidence to support decision-making. While reinforcement learning has effectively enhanced reasoning performance in medical contexts, the clinical reliability of these reasoning processes remains limited because their accuracy and validity are often overlooked during training. To address this gap, we propose MedCEG, a framework that augments medical language models with clinically valid reasoning pathways by explicitly supervising the reasoning process through a Critical Evidence Graph (CEG). We curate a dataset of challenging clinical cases and algorithmically construct a CEG for each sample to represent a high-quality verifiable reasoning pathway. To guide the reasoning process, we introduce a Clinical Reasoning Procedure Reward, which evaluates Node Coverage, Structural Correctness, and Chain Completeness, thereby providing a holistic assessment of reasoning quality. Experimental results show that MedCEG surpasses existing methods in performance while producing clinically valid reasoning chains, representing a solid advancement in reliable medical AI reasoning. The code and models are available at https://github.com/LinjieMu/MedCEG.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T16:38:46Z</published>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Linjie Mu</name>
    </author>
    <author>
      <name>Yannian Gu</name>
    </author>
    <author>
      <name>Zhongzhen Huang</name>
    </author>
    <author>
      <name>Yakun Zhu</name>
    </author>
    <author>
      <name>Shaoting Zhang</name>
    </author>
    <author>
      <name>Xiaofan Zhang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13507v1</id>
    <title>Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model</title>
    <updated>2025-12-15T16:36:52Z</updated>
    <link href="https://arxiv.org/abs/2512.13507v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13507v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Recent strides in video generation have paved the way for unified audio-visual generation. In this work, we present Seedance 1.5 pro, a foundational model engineered specifically for native, joint audio-video generation. Leveraging a dual-branch Diffusion Transformer architecture, the model integrates a cross-modal joint module with a specialized multi-stage data pipeline, achieving exceptional audio-visual synchronization and superior generation quality. To ensure practical utility, we implement meticulous post-training optimizations, including Supervised Fine-Tuning (SFT) on high-quality datasets and Reinforcement Learning from Human Feedback (RLHF) with multi-dimensional reward models. Furthermore, we introduce an acceleration framework that boosts inference speed by over 10X. Seedance 1.5 pro distinguishes itself through precise multilingual and dialect lip-syncing, dynamic cinematic camera control, and enhanced narrative coherence, positioning it as a robust engine for professional-grade content creation. Seedance 1.5 pro is now accessible on Volcano Engine at https://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?type=GenVideo.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T16:36:52Z</published>
    <arxiv:comment>Seedance 1.5 pro Technical Report</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Siyan Chen</name>
    </author>
    <author>
      <name>Yanfei Chen</name>
    </author>
    <author>
      <name>Ying Chen</name>
    </author>
    <author>
      <name>Zhuo Chen</name>
    </author>
    <author>
      <name>Feng Cheng</name>
    </author>
    <author>
      <name>Xuyan Chi</name>
    </author>
    <author>
      <name>Jian Cong</name>
    </author>
    <author>
      <name>Qinpeng Cui</name>
    </author>
    <author>
      <name>Qide Dong</name>
    </author>
    <author>
      <name>Junliang Fan</name>
    </author>
    <author>
      <name>Jing Fang</name>
    </author>
    <author>
      <name>Zetao Fang</name>
    </author>
    <author>
      <name>Chengjian Feng</name>
    </author>
    <author>
      <name>Han Feng</name>
    </author>
    <author>
      <name>Mingyuan Gao</name>
    </author>
    <author>
      <name>Yu Gao</name>
    </author>
    <author>
      <name>Qiushan Guo</name>
    </author>
    <author>
      <name>Boyang Hao</name>
    </author>
    <author>
      <name>Qingkai Hao</name>
    </author>
    <author>
      <name>Bibo He</name>
    </author>
    <author>
      <name>Qian He</name>
    </author>
    <author>
      <name>Tuyen Hoang</name>
    </author>
    <author>
      <name>Ruoqing Hu</name>
    </author>
    <author>
      <name>Xi Hu</name>
    </author>
    <author>
      <name>Weilin Huang</name>
    </author>
    <author>
      <name>Zhaoyang Huang</name>
    </author>
    <author>
      <name>Zhongyi Huang</name>
    </author>
    <author>
      <name>Siqi Jiang</name>
    </author>
    <author>
      <name>Wei Jiang</name>
    </author>
    <author>
      <name>Yunpu Jiang</name>
    </author>
    <author>
      <name>Zhuo Jiang</name>
    </author>
    <author>
      <name>Ashley Kim</name>
    </author>
    <author>
      <name>Jianan Kong</name>
    </author>
    <author>
      <name>Zhichao Lai</name>
    </author>
    <author>
      <name>Shanshan Lao</name>
    </author>
    <author>
      <name>Ai Li</name>
    </author>
    <author>
      <name>Feiya Li</name>
    </author>
    <author>
      <name>Gen Li</name>
    </author>
    <author>
      <name>Huixia Li</name>
    </author>
    <author>
      <name>JiaShi Li</name>
    </author>
    <author>
      <name>Liang Li</name>
    </author>
    <author>
      <name>Ming Li</name>
    </author>
    <author>
      <name>Tao Li</name>
    </author>
    <author>
      <name>Xian Li</name>
    </author>
    <author>
      <name>Xiaojie Li</name>
    </author>
    <author>
      <name>Xiaoyang Li</name>
    </author>
    <author>
      <name>Xingxing Li</name>
    </author>
    <author>
      <name>Yameng Li</name>
    </author>
    <author>
      <name>Yifu Li</name>
    </author>
    <author>
      <name>Yiying Li</name>
    </author>
    <author>
      <name>Chao Liang</name>
    </author>
    <author>
      <name>Ying Liang</name>
    </author>
    <author>
      <name>Zhiqiang Liang</name>
    </author>
    <author>
      <name>Wang Liao</name>
    </author>
    <author>
      <name>Yalin Liao</name>
    </author>
    <author>
      <name>Heng Lin</name>
    </author>
    <author>
      <name>Kengyu Lin</name>
    </author>
    <author>
      <name>Shanchuan Lin</name>
    </author>
    <author>
      <name>Xi Lin</name>
    </author>
    <author>
      <name>Zhijie Lin</name>
    </author>
    <author>
      <name>Feng Ling</name>
    </author>
    <author>
      <name>Fangfang Liu</name>
    </author>
    <author>
      <name>Gaohong Liu</name>
    </author>
    <author>
      <name>Jiawei Liu</name>
    </author>
    <author>
      <name>Jie Liu</name>
    </author>
    <author>
      <name>Shouda Liu</name>
    </author>
    <author>
      <name>Shu Liu</name>
    </author>
    <author>
      <name>Sichao Liu</name>
    </author>
    <author>
      <name>Songwei Liu</name>
    </author>
    <author>
      <name>Xin Liu</name>
    </author>
    <author>
      <name>Xue Liu</name>
    </author>
    <author>
      <name>Yibo Liu</name>
    </author>
    <author>
      <name>Zikun Liu</name>
    </author>
    <author>
      <name>Zuxi Liu</name>
    </author>
    <author>
      <name>Junlin Lyu</name>
    </author>
    <author>
      <name>Lecheng Lyu</name>
    </author>
    <author>
      <name>Qian Lyu</name>
    </author>
    <author>
      <name>Han Mu</name>
    </author>
    <author>
      <name>Xiaonan Nie</name>
    </author>
    <author>
      <name>Jingzhe Ning</name>
    </author>
    <author>
      <name>Xitong Pan</name>
    </author>
    <author>
      <name>Yanghua Peng</name>
    </author>
    <author>
      <name>Lianke Qin</name>
    </author>
    <author>
      <name>Xueqiong Qu</name>
    </author>
    <author>
      <name>Yuxi Ren</name>
    </author>
    <author>
      <name>Yuchen Shen</name>
    </author>
    <author>
      <name>Guang Shi</name>
    </author>
    <author>
      <name>Lei Shi</name>
    </author>
    <author>
      <name>Yan Song</name>
    </author>
    <author>
      <name>Yinglong Song</name>
    </author>
    <author>
      <name>Fan Sun</name>
    </author>
    <author>
      <name>Li Sun</name>
    </author>
    <author>
      <name>Renfei Sun</name>
    </author>
    <author>
      <name>Zeyu Sun</name>
    </author>
    <author>
      <name>Wenjing Tang</name>
    </author>
    <author>
      <name>Zirui Tao</name>
    </author>
    <author>
      <name>Feng Wang</name>
    </author>
    <author>
      <name>Furui Wang</name>
    </author>
    <author>
      <name>Jinran Wang</name>
    </author>
    <author>
      <name>Junkai Wang</name>
    </author>
    <author>
      <name>Ke Wang</name>
    </author>
    <author>
      <name>Kexin Wang</name>
    </author>
    <author>
      <name>Qingyi Wang</name>
    </author>
    <author>
      <name>Rui Wang</name>
    </author>
    <author>
      <name>Sen Wang</name>
    </author>
    <author>
      <name>Shuai Wang</name>
    </author>
    <author>
      <name>Tingru Wang</name>
    </author>
    <author>
      <name>Weichen Wang</name>
    </author>
    <author>
      <name>Xin Wang</name>
    </author>
    <author>
      <name>Yanhui Wang</name>
    </author>
    <author>
      <name>Yue Wang</name>
    </author>
    <author>
      <name>Yuping Wang</name>
    </author>
    <author>
      <name>Yuxuan Wang</name>
    </author>
    <author>
      <name>Ziyu Wang</name>
    </author>
    <author>
      <name>Guoqiang Wei</name>
    </author>
    <author>
      <name>Wanru Wei</name>
    </author>
    <author>
      <name>Di Wu</name>
    </author>
    <author>
      <name>Guohong Wu</name>
    </author>
    <author>
      <name>Hanjie Wu</name>
    </author>
    <author>
      <name>Jian Wu</name>
    </author>
    <author>
      <name>Jie Wu</name>
    </author>
    <author>
      <name>Ruolan Wu</name>
    </author>
    <author>
      <name>Xinglong Wu</name>
    </author>
    <author>
      <name>Yonghui Wu</name>
    </author>
    <author>
      <name>Ruiqi Xia</name>
    </author>
    <author>
      <name>Liang Xiang</name>
    </author>
    <author>
      <name>Fei Xiao</name>
    </author>
    <author>
      <name>XueFeng Xiao</name>
    </author>
    <author>
      <name>Pan Xie</name>
    </author>
    <author>
      <name>Shuangyi Xie</name>
    </author>
    <author>
      <name>Shuang Xu</name>
    </author>
    <author>
      <name>Jinlan Xue</name>
    </author>
    <author>
      <name>Bangbang Yang</name>
    </author>
    <author>
      <name>Ceyuan Yang</name>
    </author>
    <author>
      <name>Jiaqi Yang</name>
    </author>
    <author>
      <name>Runkai Yang</name>
    </author>
    <author>
      <name>Tao Yang</name>
    </author>
    <author>
      <name>Yang Yang</name>
    </author>
    <author>
      <name>Yihang Yang</name>
    </author>
    <author>
      <name>ZhiXian Yang</name>
    </author>
    <author>
      <name>Ziyan Yang</name>
    </author>
    <author>
      <name>Yifan Yao</name>
    </author>
    <author>
      <name>Zilyu Ye</name>
    </author>
    <author>
      <name>Bowen Yu</name>
    </author>
    <author>
      <name>Chujie Yuan</name>
    </author>
    <author>
      <name>Linxiao Yuan</name>
    </author>
    <author>
      <name>Sichun Zeng</name>
    </author>
    <author>
      <name>Weihong Zeng</name>
    </author>
    <author>
      <name>Xuejiao Zeng</name>
    </author>
    <author>
      <name>Yan Zeng</name>
    </author>
    <author>
      <name>Chuntao Zhang</name>
    </author>
    <author>
      <name>Heng Zhang</name>
    </author>
    <author>
      <name>Jingjie Zhang</name>
    </author>
    <author>
      <name>Kuo Zhang</name>
    </author>
    <author>
      <name>Liang Zhang</name>
    </author>
    <author>
      <name>Liying Zhang</name>
    </author>
    <author>
      <name>Manlin Zhang</name>
    </author>
    <author>
      <name>Ting Zhang</name>
    </author>
    <author>
      <name>Weida Zhang</name>
    </author>
    <author>
      <name>Xiaohe Zhang</name>
    </author>
    <author>
      <name>Xinyan Zhang</name>
    </author>
    <author>
      <name>Yan Zhang</name>
    </author>
    <author>
      <name>Yuan Zhang</name>
    </author>
    <author>
      <name>Zixiang Zhang</name>
    </author>
    <author>
      <name>Fengxuan Zhao</name>
    </author>
    <author>
      <name>Huating Zhao</name>
    </author>
    <author>
      <name>Yang Zhao</name>
    </author>
    <author>
      <name>Hao Zheng</name>
    </author>
    <author>
      <name>Jianbin Zheng</name>
    </author>
    <author>
      <name>Xiaozheng Zheng</name>
    </author>
    <author>
      <name>Yangyang Zheng</name>
    </author>
    <author>
      <name>Yijie Zheng</name>
    </author>
    <author>
      <name>Jiexin Zhou</name>
    </author>
    <author>
      <name>Kuan Zhu</name>
    </author>
    <author>
      <name>Shenhan Zhu</name>
    </author>
    <author>
      <name>Wenjia Zhu</name>
    </author>
    <author>
      <name>Benhui Zou</name>
    </author>
    <author>
      <name>Feilong Zuo</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13506v1</id>
    <title>Learning under Distributional Drift: Reproducibility as an Intrinsic Statistical Resource</title>
    <updated>2025-12-15T16:34:47Z</updated>
    <link href="https://arxiv.org/abs/2512.13506v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13506v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Statistical learning under distributional drift remains insufficiently characterized: when each observation alters the data-generating law, classical generalization bounds can collapse. We introduce a new statistical primitive, the reproducibility budget $C_T$, which quantifies a system's finite capacity for statistical reproducibility - the extent to which its sampling process can remain governed by a consistent underlying distribution in the presence of both exogenous change and endogenous feedback. Formally, $C_T$ is defined as the cumulative Fisher-Rao path length of the coupled learner-environment evolution, measuring the total distributional motion accumulated during learning. From this construct we derive a drift-feedback generalization bound of order $O(T^{-1/2} + C_T/T)$, and we prove a matching minimax lower bound showing that this rate is minimax-optimal. Consequently, the results establish a reproducibility speed limit: no algorithm can achieve smaller worst-case generalization error than that imposed by the average Fisher-Rao drift rate $C_T/T$ of the data-generating process. The framework situates exogenous drift, adaptive data analysis, and performative prediction within a common geometric structure, with $C_T$ emerging as the intrinsic quantity measuring distributional motion across these settings.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T16:34:47Z</published>
    <arxiv:comment>37 pages, 4 figures</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Sofiya Zaichyk</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13505v1</id>
    <title>Defending the Hierarchical Result Models of Precedential Constraint</title>
    <updated>2025-12-15T16:33:33Z</updated>
    <link href="https://arxiv.org/abs/2512.13505v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13505v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>In recent years, hierarchical case-based-reasoning models of precedential constraint have been proposed. In various papers, Trevor Bench-Capon criticised these models on the grounds that they would give incorrect outcomes in some cases. In particular, the models would not account for the possibility that intermediate factors are established with different strengths by different base-level factors. In this paper we respond to these criticisms for van Woerkom's result-based hierarchical models. We argue that in some examples Bench-Capon seems to interpret intermediate factors as dimensions, and that applying van Woerkom's dimension-based version of the hierarchical result model to these examples avoids Bench-Capon's criticisms.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T16:33:33Z</published>
    <arxiv:comment>This is the long version of a paper with the same title presented at the 38th International Conference on Legal Knowledge and Information Systems</arxiv:comment>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Henry Prakken</name>
    </author>
    <author>
      <name>Wijnand van Woerkom</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13501v1</id>
    <title>Behavior-Aware and Generalizable Defense Against Black-Box Adversarial Attacks for ML-Based IDS</title>
    <updated>2025-12-15T16:29:23Z</updated>
    <link href="https://arxiv.org/abs/2512.13501v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13501v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Machine learning based intrusion detection systems are increasingly targeted by black box adversarial attacks, where attackers craft evasive inputs using indirect feedback such as binary outputs or behavioral signals like response time and resource usage. While several defenses have been proposed, including input transformation, adversarial training, and surrogate detection, they often fall short in practice. Most are tailored to specific attack types, require internal model access, or rely on static mechanisms that fail to generalize across evolving attack strategies. Furthermore, defenses such as input transformation can degrade intrusion detection system performance, making them unsuitable for real time deployment.
  To address these limitations, we propose Adaptive Feature Poisoning, a lightweight and proactive defense mechanism designed specifically for realistic black box scenarios. Adaptive Feature Poisoning assumes that probing can occur silently and continuously, and introduces dynamic and context aware perturbations to selected traffic features, corrupting the attacker feedback loop without impacting detection capabilities. The method leverages traffic profiling, change point detection, and adaptive scaling to selectively perturb features that an attacker is likely exploiting, based on observed deviations.
  We evaluate Adaptive Feature Poisoning against multiple realistic adversarial attack strategies, including silent probing, transferability based attacks, and decision boundary based attacks. The results demonstrate its ability to confuse attackers, degrade attack effectiveness, and preserve detection performance. By offering a generalizable, attack agnostic, and undetectable defense, Adaptive Feature Poisoning represents a significant step toward practical and robust adversarial resilience in machine learning based intrusion detection systems.</summary>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T16:29:23Z</published>
    <arxiv:primary_category term="cs.CR"/>
    <author>
      <name>Sabrine Ennaji</name>
    </author>
    <author>
      <name>Elhadj Benkhelifa</name>
    </author>
    <author>
      <name>Luigi Vincenzo Mancini</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13497v1</id>
    <title>On-Device Continual Learning for Unsupervised Visual Anomaly Detection in Dynamic Manufacturing</title>
    <updated>2025-12-15T16:27:23Z</updated>
    <link href="https://arxiv.org/abs/2512.13497v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13497v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>In modern manufacturing, Visual Anomaly Detection (VAD) is essential for automated inspection and consistent product quality. Yet, increasingly dynamic and flexible production environments introduce key challenges: First, frequent product changes in small-batch and on-demand manufacturing require rapid model updates. Second, legacy edge hardware lacks the resources to train and run large AI models. Finally, both anomalous and normal training data are often scarce, particularly for newly introduced product variations. We investigate on-device continual learning for unsupervised VAD with localization, extending the PatchCore to incorporate online learning for real-world industrial scenarios. The proposed method leverages a lightweight feature extractor and an incremental coreset update mechanism based on k-center selection, enabling rapid, memory-efficient adaptation from limited data while eliminating costly cloud retraining. Evaluations on an industrial use case are conducted using a testbed designed to emulate flexible production with frequent variant changes in a controlled environment. Our method achieves a 12% AUROC improvement over the baseline, an 80% reduction in memory usage, and faster training compared to batch retraining. These results confirm that our method delivers accurate, resource-efficient, and adaptive VAD suitable for dynamic and smart manufacturing.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T16:27:23Z</published>
    <arxiv:comment>Accepted by European Conference on EDGE AI Technologies and Applications (EEAI) 2025</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Haoyu Ren</name>
    </author>
    <author>
      <name>Kay Koehle</name>
    </author>
    <author>
      <name>Kirill Dorofeev</name>
    </author>
    <author>
      <name>Darko Anicic</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13495v1</id>
    <title>Soul: Breathe Life into Digital Human for High-fidelity Long-term Multimodal Animation</title>
    <updated>2025-12-15T16:25:56Z</updated>
    <link href="https://arxiv.org/abs/2512.13495v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13495v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We propose a multimodal-driven framework for high-fidelity long-term digital human animation termed $\textbf{Soul}$, which generates semantically coherent videos from a single-frame portrait image, text prompts, and audio, achieving precise lip synchronization, vivid facial expressions, and robust identity preservation. We construct Soul-1M, containing 1 million finely annotated samples with a precise automated annotation pipeline (covering portrait, upper-body, full-body, and multi-person scenes) to mitigate data scarcity, and we carefully curate Soul-Bench for comprehensive and fair evaluation of audio-/text-guided animation methods. The model is built on the Wan2.2-5B backbone, integrating audio-injection layers and multiple training strategies together with threshold-aware codebook replacement to ensure long-term generation consistency. Meanwhile, step/CFG distillation and a lightweight VAE are used to optimize inference efficiency, achieving an 11.4$\times$ speedup with negligible quality loss. Extensive experiments show that Soul significantly outperforms current leading open-source and commercial models on video quality, video-text alignment, identity preservation, and lip-synchronization accuracy, demonstrating broad applicability in real-world scenarios such as virtual anchors and film production. Project page at https://zhangzjn.github.io/projects/Soul/</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T16:25:56Z</published>
    <arxiv:comment>Project page: https://zhangzjn.github.io/projects/Soul/</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Jiangning Zhang</name>
    </author>
    <author>
      <name>Junwei Zhu</name>
    </author>
    <author>
      <name>Zhenye Gan</name>
    </author>
    <author>
      <name>Donghao Luo</name>
    </author>
    <author>
      <name>Chuming Lin</name>
    </author>
    <author>
      <name>Feifan Xu</name>
    </author>
    <author>
      <name>Xu Peng</name>
    </author>
    <author>
      <name>Jianlong Hu</name>
    </author>
    <author>
      <name>Yuansen Liu</name>
    </author>
    <author>
      <name>Yijia Hong</name>
    </author>
    <author>
      <name>Weijian Cao</name>
    </author>
    <author>
      <name>Han Feng</name>
    </author>
    <author>
      <name>Xu Chen</name>
    </author>
    <author>
      <name>Chencan Fu</name>
    </author>
    <author>
      <name>Keke He</name>
    </author>
    <author>
      <name>Xiaobin Hu</name>
    </author>
    <author>
      <name>Chengjie Wang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13494v1</id>
    <title>SkipCat: Rank-Maximized Low-Rank Compression of Large Language Models via Shared Projection and Block Skipping</title>
    <updated>2025-12-15T16:25:55Z</updated>
    <link href="https://arxiv.org/abs/2512.13494v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13494v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Large language models (LLM) have achieved remarkable performance across a wide range of tasks. However, their substantial parameter sizes pose significant challenges for deployment on edge devices with limited computational and memory resources. Low-rank compression is a promising approach to address this issue, as it reduces both computational and memory costs, making LLM more suitable for resource-constrained environments. Nonetheless, naïve low-rank compression methods require a significant reduction in the retained rank to achieve meaningful memory and computation savings. For a low-rank model, the ranks need to be reduced by more than half to yield efficiency gains. Such aggressive truncation, however, typically results in substantial performance degradation. To address this trade-off, we propose SkipCat, a novel low-rank compression framework that enables the use of higher ranks while achieving the same compression rates. First, we introduce an intra-layer shared low-rank projection method, where multiple matrices that share the same input use a common projection. This reduces redundancy and improves compression efficiency. Second, we propose a block skipping technique that omits computations and memory transfers for selected sub-blocks within the low-rank decomposition. These two techniques jointly enable our compressed model to retain more effective ranks under the same compression budget. Experimental results show that, without any additional fine-tuning, our method outperforms previous low-rank compression approaches by 7% accuracy improvement on zero-shot tasks under the same compression rate. These results highlight the effectiveness of our rank-maximized compression strategy in preserving model performance under tight resource constraints.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T16:25:55Z</published>
    <arxiv:comment>Accepted by AAAI 2026</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Yu-Chen Lu</name>
    </author>
    <author>
      <name>Sheng-Feng Yu</name>
    </author>
    <author>
      <name>Hui-Hsien Weng</name>
    </author>
    <author>
      <name>Pei-Shuo Wang</name>
    </author>
    <author>
      <name>Yu-Fang Hu</name>
    </author>
    <author>
      <name>Liang Hung-Chun</name>
    </author>
    <author>
      <name>Hung-Yueh Chiang</name>
    </author>
    <author>
      <name>Kai-Chiang Wu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13492v1</id>
    <title>Transform Trained Transformer: Accelerating Naive 4K Video Generation Over 10$\times$</title>
    <updated>2025-12-15T16:25:39Z</updated>
    <link href="https://arxiv.org/abs/2512.13492v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13492v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Native 4K (2160$\times$3840) video generation remains a critical challenge due to the quadratic computational explosion of full-attention as spatiotemporal resolution increases, making it difficult for models to strike a balance between efficiency and quality. This paper proposes a novel Transformer retrofit strategy termed $\textbf{T3}$ ($\textbf{T}$ransform $\textbf{T}$rained $\textbf{T}$ransformer) that, without altering the core architecture of full-attention pretrained models, significantly reduces compute requirements by optimizing their forward logic. Specifically, $\textbf{T3-Video}$ introduces a multi-scale weight-sharing window attention mechanism and, via hierarchical blocking together with an axis-preserving full-attention design, can effect an "attention pattern" transformation of a pretrained model using only modest compute and data. Results on 4K-VBench show that $\textbf{T3-Video}$ substantially outperforms existing approaches: while delivering performance improvements (+4.29$\uparrow$ VQA and +0.08$\uparrow$ VTC), it accelerates native 4K video generation by more than 10$\times$. Project page at https://zhangzjn.github.io/projects/T3-Video</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T16:25:39Z</published>
    <arxiv:comment>Project page: https://zhangzjn.github.io/projects/T3-Video</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Jiangning Zhang</name>
    </author>
    <author>
      <name>Junwei Zhu</name>
    </author>
    <author>
      <name>Teng Hu</name>
    </author>
    <author>
      <name>Yabiao Wang</name>
    </author>
    <author>
      <name>Donghao Luo</name>
    </author>
    <author>
      <name>Weijian Cao</name>
    </author>
    <author>
      <name>Zhenye Gan</name>
    </author>
    <author>
      <name>Xiaobin Hu</name>
    </author>
    <author>
      <name>Zhucun Xue</name>
    </author>
    <author>
      <name>Chengjie Wang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13491v1</id>
    <title>From Zipf's Law to Neural Scaling through Heaps' Law and Hilberg's Hypothesis</title>
    <updated>2025-12-15T16:25:06Z</updated>
    <link href="https://arxiv.org/abs/2512.13491v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13491v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We inspect the deductive connection between the neural scaling law and Zipf's law -- two statements discussed in machine learning and quantitative linguistics. The neural scaling law describes how the cross entropy rate of a foundation model -- such as a large language model -- changes with respect to the amount of training tokens, parameters, and compute. By contrast, Zipf's law posits that the distribution of tokens exhibits a power law tail. Whereas similar claims have been made in more specific settings, we show that the neural scaling law is a consequence of Zipf's law under certain broad assumptions that we reveal systematically. The derivation steps are as follows: We derive Heaps' law on the vocabulary growth from Zipf's law, Hilberg's hypothesis on the entropy scaling from Heaps' law, and the neural scaling from Hilberg's hypothesis. We illustrate these inference steps by a toy example of the Santa Fe process that satisfies all the four statistical laws.</summary>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T16:25:06Z</published>
    <arxiv:comment>32 pages, no figures</arxiv:comment>
    <arxiv:primary_category term="cs.IT"/>
    <author>
      <name>Łukasz Dębowski</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13488v1</id>
    <title>SIGMA: An AI-Empowered Training Stack on Early-Life Hardware</title>
    <updated>2025-12-15T16:24:32Z</updated>
    <link href="https://arxiv.org/abs/2512.13488v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13488v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>An increasing variety of AI accelerators is being considered for large-scale training. However, enabling large-scale training on early-life AI accelerators faces three core challenges: frequent system disruptions and undefined failure modes that undermine reliability; numerical errors and training instabilities that threaten correctness and convergence; and the complexity of parallelism optimization combined with unpredictable local noise that degrades efficiency. To address these challenges, SIGMA is an open-source training stack designed to improve the reliability, stability, and efficiency of large-scale distributed training on early-life AI hardware. The core of this initiative is the LUCIA TRAINING PLATFORM (LTP), the system optimized for clusters with early-life AI accelerators. Since its launch in March 2025, LTP has significantly enhanced training reliability and operational productivity. Over the past five months, it has achieved an impressive 94.45% effective cluster accelerator utilization, while also substantially reducing node recycling and job-recovery times. Building on the foundation of LTP, the LUCIA TRAINING FRAMEWORK (LTF) successfully trained SIGMA-MOE, a 200B MoE model, using 2,048 AI accelerators. This effort delivered remarkable stability and efficiency outcomes, achieving 21.08% MFU, state-of-the-art downstream accuracy, and encountering only one stability incident over a 75-day period. Together, these advances establish SIGMA, which not only tackles the critical challenges of large-scale training but also establishes a new benchmark for AI infrastructure and platform innovation, offering a robust, cost-effective alternative to prevailing established accelerator stacks and significantly advancing AI capabilities and scalability. The source code of SIGMA is available at https://github.com/microsoft/LuciaTrainingPlatform.</summary>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T16:24:32Z</published>
    <arxiv:comment>22 pages, 7 figures</arxiv:comment>
    <arxiv:primary_category term="cs.DC"/>
    <author>
      <name>Lei Qu</name>
    </author>
    <author>
      <name>Lianhai Ren</name>
    </author>
    <author>
      <name>Peng Cheng</name>
    </author>
    <author>
      <name>Rui Gao</name>
    </author>
    <author>
      <name>Ruizhe Wang</name>
    </author>
    <author>
      <name>Tianyu Chen</name>
    </author>
    <author>
      <name>Xiao Liu</name>
    </author>
    <author>
      <name>Xingjian Zhang</name>
    </author>
    <author>
      <name>Yeyun Gong</name>
    </author>
    <author>
      <name>Yifan Xiong</name>
    </author>
    <author>
      <name>Yucheng Ding</name>
    </author>
    <author>
      <name>Yuting Jiang</name>
    </author>
    <author>
      <name>Zhenghao Lin</name>
    </author>
    <author>
      <name>Zhongxin Guo</name>
    </author>
    <author>
      <name>Ziyue Yang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13487v1</id>
    <title>Advancing Bangla Machine Translation Through Informal Datasets</title>
    <updated>2025-12-15T16:22:45Z</updated>
    <link href="https://arxiv.org/abs/2512.13487v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13487v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Bangla is the sixth most widely spoken language globally, with approximately 234 million native speakers. However, progress in open-source Bangla machine translation remains limited. Most online resources are in English and often remain untranslated into Bangla, excluding millions from accessing essential information. Existing research in Bangla translation primarily focuses on formal language, neglecting the more commonly used informal language. This is largely due to the lack of pairwise Bangla-English data and advanced translation models. If datasets and models can be enhanced to better handle natural, informal Bangla, millions of people will benefit from improved online information access. In this research, we explore current state-of-the-art models and propose improvements to Bangla translation by developing a dataset from informal sources like social media and conversational texts. This work aims to advance Bangla machine translation by focusing on informal language translation and improving accessibility for Bangla speakers in the digital world.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T16:22:45Z</published>
    <arxiv:comment>33 pages, 13 figures</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Ayon Roy</name>
    </author>
    <author>
      <name>Risat Rahaman</name>
    </author>
    <author>
      <name>Sadat Shibly</name>
    </author>
    <author>
      <name>Udoy Saha Joy</name>
    </author>
    <author>
      <name>Abdulla Al Kafi</name>
    </author>
    <author>
      <name>Farig Yousuf Sadeque</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13482v1</id>
    <title>Real-Time AI-Driven Milling Digital Twin Towards Extreme Low-Latency</title>
    <updated>2025-12-15T16:18:36Z</updated>
    <link href="https://arxiv.org/abs/2512.13482v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13482v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Digital twin (DT) enables smart manufacturing by leveraging real-time data, AI models, and intelligent control systems. This paper presents a state-of-the-art analysis on the emerging field of DTs in the context of milling. The critical aspects of DT are explored through the lens of virtual models of physical milling, data flow from physical milling to virtual model, and feedback from virtual model to physical milling. Live data streaming protocols and virtual modeling methods are highlighted. A case study showcases the transformative capability of a real-time machine learning-driven live DT of tool-work contact in a milling process. Future research directions are outlined to achieve the goals of Industry 4.0 and beyond.</summary>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T16:18:36Z</published>
    <arxiv:primary_category term="eess.SY"/>
    <author>
      <name>Wenyi Liu</name>
    </author>
    <author>
      <name>R. Sharma</name>
    </author>
    <author>
      <name>W. "Grace" Guo</name>
    </author>
    <author>
      <name>J. Yi</name>
    </author>
    <author>
      <name>Y. B. Guo</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13481v1</id>
    <title>neuralFOMO: Can LLMs Handle Being Second Best? Measuring Envy-Like Preferences in Multi-Agent Settings</title>
    <updated>2025-12-15T16:17:12Z</updated>
    <link href="https://arxiv.org/abs/2512.13481v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13481v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Envy is a common human behavior that shapes competitiveness and can alter outcomes in team settings. As large language models (LLMs) increasingly act on behalf of humans in collaborative and competitive workflows, there is a pressing need to evaluate whether and under what conditions they exhibit envy-like preferences. In this paper, we test whether LLMs show envy-like behavior toward each other. We considered two scenarios: (1) A point allocation game that tests whether a model tries to win over its peer. (2) A workplace setting observing behaviour when recognition is unfair. Our findings reveal consistent evidence of envy-like patterns in certain LLMs, with large variation across models and contexts. For instance, GPT-5-mini and Claude-3.7-Sonnet show a clear tendency to pull down the peer model to equalize outcomes, whereas Mistral-Small-3.2-24B instead focuses on maximizing its own individual gains. These results highlight the need to consider competitive dispositions as a safety and design factor in LLM-based multi-agent systems.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T16:17:12Z</published>
    <arxiv:comment>Under Review</arxiv:comment>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Ojas Pungalia</name>
    </author>
    <author>
      <name>Rashi Upadhyay</name>
    </author>
    <author>
      <name>Abhishek Mishra</name>
    </author>
    <author>
      <name>Abhiram H</name>
    </author>
    <author>
      <name>Tejasvi Alladi</name>
    </author>
    <author>
      <name>Sujan Yenuganti</name>
    </author>
    <author>
      <name>Dhruv Kumar</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13480v1</id>
    <title>Element-wise Modulation of Random Matrices for Efficient Neural Layers</title>
    <updated>2025-12-15T16:16:53Z</updated>
    <link href="https://arxiv.org/abs/2512.13480v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13480v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Fully connected layers are a primary source of memory and computational overhead in deep neural networks due to their dense, often redundant parameterization. While various compression techniques exist, they frequently introduce complex engineering trade-offs or degrade model performance. We propose the Parametrized Random Projection (PRP) layer, a novel approach that decouples feature mixing from adaptation by utilizing a fixed random matrix modulated by lightweight, learnable element-wise parameters. This architecture drastically reduces the trainable parameter count to a linear scale while retaining reliable accuracy across various benchmarks. The design serves as a stable, computationally efficient solution for architectural scaling and deployment in resource-limited settings.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T16:16:53Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Maksymilian Szorc</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13478v1</id>
    <title>Non-Resolution Reasoning: A Framework for Preserving Semantic Ambiguity in Language Models</title>
    <updated>2025-12-15T16:14:32Z</updated>
    <link href="https://arxiv.org/abs/2512.13478v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13478v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Premature semantic collapse -- the forced early commitment to a single meaning -- remains a core architectural limitation of current language models. Softmax-driven competition and greedy decoding cause models to discard valid interpretations before sufficient context is available, resulting in brittle reasoning and context failures. We introduce Non-Resolution Reasoning (NRR), a general computational framework that preserves semantic ambiguity during inference and performs resolution only when explicitly required. NRR integrates three components: (1) Multi-Vector Embeddings that maintain multiple viable interpretations per token, (2) Non-Collapsing Attention that prevents winner-take-all dynamics across layers, and (3) Contextual Identity Tracking (CIT), which assigns context-specific identities to recurring entities (e.g., distinguishing "Dr. Smith the cardiologist" from "Dr. Smith the researcher"). These mechanisms are unified by an external Resolution Operator $ρ$ that makes semantic commitment explicit, controllable, and task-dependent. Unlike standard architectures, NRR separates representation from resolution, allowing a single model to shift between creative, factual, and ambiguity-preserving reasoning without retraining. A synthetic evaluation demonstrates NRR's ability to preserve ambiguity and track context: CIT-enhanced models achieve 90.9% accuracy on out-of-distribution identity-shift tasks, compared to 9.1% for transformer baselines. NRR provides a principled alternative to premature collapse, reframing ambiguity as an explicit representational state rather than a failure mode. The question is not whether AI should resolve ambiguity, but when, how, and under whose control.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T16:14:32Z</published>
    <arxiv:comment>19 pages</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Kei Saito</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13474v1</id>
    <title>Mapping of the system of software-related emissions and shared responsibilities</title>
    <updated>2025-12-15T16:12:19Z</updated>
    <link href="https://arxiv.org/abs/2512.13474v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13474v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The global climate is experiencing a rapid and unprecedented warming trend. The ICT sector is a notable contributor to global greenhouse gas emissions, with its environmental impact continuing to expand. Addressing this issue is vital for achieving the objectives of the Paris Agreement, particularly the goal of limiting global temperature rise to 1.5°C. At the European Union level, regulatory measures such as the CSRD and the CSDD impose obligations on companies, including those within the ICT sector, to recognize and mitigate their environmental footprint. This study provides a comprehensive system mapping aimed at enhancing the awareness and understanding of software-related emissions and the corresponding responsibilities borne by the ICT sector. The mapping identifies the primary sources of carbon emissions and energy consumption within the ICT domain while also outlining the key responsibilities of the stakeholders accountable throughout the software lifecycle.</summary>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T16:12:19Z</published>
    <arxiv:comment>8 pages, 8 figures, 2 tables, 2025 IEEE/ACM 9th International Workshop on Green and Sustainable Software (GREENS)</arxiv:comment>
    <arxiv:primary_category term="cs.SE"/>
    <author>
      <name>Laura Partanen</name>
    </author>
    <author>
      <name>Antti Sipila</name>
    </author>
    <author>
      <name>Md Sanaul Haque</name>
    </author>
    <author>
      <name>Jari Porras</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13472v1</id>
    <title>Scaling Laws for Code: Every Programming Language Matters</title>
    <updated>2025-12-15T16:07:34Z</updated>
    <link href="https://arxiv.org/abs/2512.13472v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13472v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Code large language models (Code LLMs) are powerful but costly to train, with scaling laws predicting performance from model size, data, and compute. However, different programming languages (PLs) have varying impacts during pre-training that significantly affect base model performance, leading to inaccurate performance prediction. Besides, existing works focus on language-agnostic settings, neglecting the inherently multilingual nature of modern software development. Therefore, it is first necessary to investigate the scaling laws of different PLs, and then consider their mutual influences to arrive at the final multilingual scaling law. In this paper, we present the first systematic exploration of scaling laws for multilingual code pre-training, conducting over 1000+ experiments (Equivalent to 336,000+ H800 hours) across multiple PLs, model sizes (0.2B to 14B parameters), and dataset sizes (1T tokens). We establish comprehensive scaling laws for code LLMs across multiple PLs, revealing that interpreted languages (e.g., Python) benefit more from increased model size and data than compiled languages (e.g., Rust). The study demonstrates that multilingual pre-training provides synergistic benefits, particularly between syntactically similar PLs. Further, the pre-training strategy of the parallel pairing (concatenating code snippets with their translations) significantly enhances cross-lingual abilities with favorable scaling properties. Finally, a proportion-dependent multilingual scaling law is proposed to optimally allocate training tokens by prioritizing high-utility PLs (e.g., Python), balancing high-synergy pairs (e.g., JavaScript-TypeScript), and reducing allocation to fast-saturating languages (Rust), achieving superior average performance across all PLs compared to uniform distribution under the same compute budget.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T16:07:34Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Jian Yang</name>
    </author>
    <author>
      <name>Shawn Guo</name>
    </author>
    <author>
      <name>Lin Jing</name>
    </author>
    <author>
      <name>Wei Zhang</name>
    </author>
    <author>
      <name>Aishan Liu</name>
    </author>
    <author>
      <name>Chuan Hao</name>
    </author>
    <author>
      <name>Zhoujun Li</name>
    </author>
    <author>
      <name>Wayne Xin Zhao</name>
    </author>
    <author>
      <name>Xianglong Liu</name>
    </author>
    <author>
      <name>Weifeng Lv</name>
    </author>
    <author>
      <name>Bryan Dai</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13465v1</id>
    <title>PoseAnything: Universal Pose-guided Video Generation with Part-aware Temporal Coherence</title>
    <updated>2025-12-15T16:03:26Z</updated>
    <link href="https://arxiv.org/abs/2512.13465v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13465v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Pose-guided video generation refers to controlling the motion of subjects in generated video through a sequence of poses. It enables precise control over subject motion and has important applications in animation. However, current pose-guided video generation methods are limited to accepting only human poses as input, thus generalizing poorly to pose of other subjects. To address this issue, we propose PoseAnything, the first universal pose-guided video generation framework capable of handling both human and non-human characters, supporting arbitrary skeletal inputs. To enhance consistency preservation during motion, we introduce Part-aware Temporal Coherence Module, which divides the subject into different parts, establishes part correspondences, and computes cross-attention between corresponding parts across frames to achieve fine-grained part-level consistency. Additionally, we propose Subject and Camera Motion Decoupled CFG, a novel guidance strategy that, for the first time, enables independent camera movement control in pose-guided video generation, by separately injecting subject and camera motion control information into the positive and negative anchors of CFG. Furthermore, we present XPose, a high-quality public dataset containing 50,000 non-human pose-video pairs, along with an automated pipeline for annotation and filtering. Extensive experiments demonstrate that Pose-Anything significantly outperforms state-of-the-art methods in both effectiveness and generalization.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T16:03:26Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Ruiyan Wang</name>
    </author>
    <author>
      <name>Teng Hu</name>
    </author>
    <author>
      <name>Kaihui Huang</name>
    </author>
    <author>
      <name>Zihan Su</name>
    </author>
    <author>
      <name>Ran Yi</name>
    </author>
    <author>
      <name>Lizhuang Ma</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13460v1</id>
    <title>DP-EMAR: A Differentially Private Framework for Autonomous Model Weight Repair in Federated IoT Systems</title>
    <updated>2025-12-15T15:56:58Z</updated>
    <link href="https://arxiv.org/abs/2512.13460v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13460v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Federated Learning (FL) enables decentralized model training without sharing raw data, but model weight distortion remains a major challenge in resource constrained IoT networks. In multi tier Federated IoT (Fed-IoT) systems, unstable connectivity and adversarial interference can silently alter transmitted parameters, degrading convergence. We propose DP-EMAR, a differentially private, error model based autonomous repair framework that detects and reconstructs transmission induced distortions during FL aggregation. DP-EMAR estimates corruption patterns and applies adaptive correction before privacy noise is added, enabling reliable in network repair without violating confidentiality. By integrating Differential Privacy (DP) with Secure Aggregation (SA), the framework distinguishes DP noise from genuine transmission errors. Experiments on heterogeneous IoT sensor and graph datasets show that DP-EMAR preserves convergence stability and maintains near baseline performance under communication corruption while ensuring strict (epsilon, delta)-DP guarantees. The framework enhances robustness, communication efficiency, and trust in privacy preserving Federated IoT learning.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T15:56:58Z</published>
    <arxiv:comment>Accepted and presented at the AI-IoT Workshop, co-located with COMSNETS 2025</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <arxiv:journal_ref>Proc. International Conference on Communication Systems and Networks (COMSNETS), 2025</arxiv:journal_ref>
    <author>
      <name>Chethana Prasad Kabgere</name>
    </author>
    <author>
      <name>Shylaja S S</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13458v1</id>
    <title>SSAS: Cross-subject EEG-based Emotion Recognition through Source Selection with Adversarial Strategy</title>
    <updated>2025-12-15T15:56:04Z</updated>
    <link href="https://arxiv.org/abs/2512.13458v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13458v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Electroencephalographic (EEG) signals have long been applied in the field of affective brain-computer interfaces (aBCIs). Cross-subject EEG-based emotion recognition has demonstrated significant potential in practical applications due to its suitability across diverse people. However, most studies on cross-subject EEG-based emotion recognition neglect the presence of inter-individual variability and negative transfer phenomena during model training. To address this issue, a cross-subject EEG-based emotion recognition through source selection with adversarial strategy is introduced in this paper. The proposed method comprises two modules: the source selection network (SS) and the adversarial strategies network (AS). The SS uses domain labels to reverse-engineer the training process of domain adaptation. Its key idea is to disrupt class separability and magnify inter-domain differences, thereby raising the classification difficulty and forcing the model to learn domain-invariant yet emotion-relevant representations. The AS gets the source domain selection results and the pretrained domain discriminators from SS. The pretrained domain discriminators compute a novel loss aimed at enhancing the performance of domain classification during adversarial training, ensuring the balance of adversarial strategies. This paper provides theoretical insights into the proposed method and achieves outstanding performance on two EEG-based emotion datasets, SEED and SEED-IV. The code can be found at https://github.com/liuyici/SSAS.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T15:56:04Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Yici Liu</name>
    </author>
    <author>
      <name>Qi Wei Oung</name>
    </author>
    <author>
      <name>Hoi Leong Lee</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13454v1</id>
    <title>Test-Time Modification: Inverse Domain Transformation for Robust Perception</title>
    <updated>2025-12-15T15:51:30Z</updated>
    <link href="https://arxiv.org/abs/2512.13454v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13454v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Generative foundation models contain broad visual knowledge and can produce diverse image variations, making them particularly promising for advancing domain generalization tasks. While they can be used for training data augmentation, synthesizing comprehensive target-domain variations remains slow, expensive, and incomplete. We propose an alternative: using diffusion models at test time to map target images back to the source distribution where the downstream model was trained. This approach requires only a source domain description, preserves the task model, and eliminates large-scale synthetic data generation. We demonstrate consistent improvements across segmentation, detection, and classification tasks under challenging environmental shifts in real-to-real domain generalization scenarios with unknown target distributions. Our analysis spans multiple generative and downstream models, including an ensemble variant for enhanced robustness. The method achieves substantial relative gains: 137% on BDD100K-Night, 68% on ImageNet-R, and 62% on DarkZurich.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T15:51:30Z</published>
    <arxiv:comment>Preprint</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Arpit Jadon</name>
    </author>
    <author>
      <name>Joshua Niemeijer</name>
    </author>
    <author>
      <name>Yuki M. Asano</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13444v1</id>
    <title>A Data Annotation Requirements Representation and Specification (DARS)</title>
    <updated>2025-12-15T15:41:30Z</updated>
    <link href="https://arxiv.org/abs/2512.13444v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13444v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>With the rise of AI-enabled cyber-physical systems, data annotation has become a critical yet often overlooked process in the development of these intelligent information systems. Existing work in requirements engineering (RE) has explored how requirements for AI systems and their data can be represented. However, related interviews with industry professionals show that data annotations and their related requirements introduce distinct challenges, indicating a need for annotation-specific requirement representations. We propose the Data Annotation Requirements Representation and Specification (DARS), including an Annotation Negotiation Card to align stakeholders on objectives and constraints, and a Scenario-Based Annotation Specification to express atomic and verifiable data annotation requirements. We evaluate DARS with an automotive perception case related to an ongoing project, and a mapping against 18 real-world data annotation error types. The results suggest that DARS mitigates root causes of completeness, accuracy, and consistency annotation errors. By integrating DARS into RE, this work improves the reliability of safety-critical systems using data annotations and demonstrates how engineering frameworks must evolve for data-dependent components of today's intelligent information systems.</summary>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T15:41:30Z</published>
    <arxiv:comment>17 pages, 3 figures, currently submitted and under review</arxiv:comment>
    <arxiv:primary_category term="cs.SE"/>
    <author>
      <name>Yi Peng</name>
    </author>
    <author>
      <name>Hina Saeeda</name>
    </author>
    <author>
      <name>Hans-Martin Heyn</name>
    </author>
    <author>
      <name>Jennifer Horkoff</name>
    </author>
    <author>
      <name>Eric Knauss</name>
    </author>
    <author>
      <name>Fredrick Warg</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13442v1</id>
    <title>XNNTab -- Interpretable Neural Networks for Tabular Data using Sparse Autoencoders</title>
    <updated>2025-12-15T15:39:59Z</updated>
    <link href="https://arxiv.org/abs/2512.13442v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13442v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>In data-driven applications relying on tabular data, where interpretability is key, machine learning models such as decision trees and linear regression are applied. Although neural networks can provide higher predictive performance, they are not used because of their blackbox nature. In this work, we present XNNTab, a neural architecture that combines the expressiveness of neural networks and interpretability. XNNTab first learns highly non-linear feature representations, which are decomposed into monosemantic features using a sparse autoencoder (SAE). These features are then assigned human-interpretable concepts, making the overall model prediction intrinsically interpretable. XNNTab outperforms interpretable predictive models, and achieves comparable performance to its non-interpretable counterparts.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T15:39:59Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Khawla Elhadri</name>
    </author>
    <author>
      <name>Jörg Schlötterer</name>
    </author>
    <author>
      <name>Christin Seifert</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13441v1</id>
    <title>Large language models are not about language</title>
    <updated>2025-12-15T15:36:42Z</updated>
    <link href="https://arxiv.org/abs/2512.13441v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13441v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Large Language Models are useless for linguistics, as they are probabilistic models that require a vast amount of data to analyse externalized strings of words. In contrast, human language is underpinned by a mind-internal computational system that recursively generates hierarchical thought structures. The language system grows with minimal external input and can readily distinguish between real language and impossible languages.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T15:36:42Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Johan J. Bolhuis</name>
    </author>
    <author>
      <name>Andrea Moro</name>
    </author>
    <author>
      <name>Stephen Crain</name>
    </author>
    <author>
      <name>Sandiway Fong</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13440v1</id>
    <title>IMILIA: interpretable multiple instance learning for inflammation prediction in IBD from H&amp;E whole slide images</title>
    <updated>2025-12-15T15:36:09Z</updated>
    <link href="https://arxiv.org/abs/2512.13440v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13440v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>As the therapeutic target for Inflammatory Bowel Disease (IBD) shifts toward histologic remission, the accurate assessment of microscopic inflammation has become increasingly central for evaluating disease activity and response to treatment. In this work, we introduce IMILIA (Interpretable Multiple Instance Learning for Inflammation Analysis), an end-to-end framework designed for the prediction of inflammation presence in IBD digitized slides stained with hematoxylin and eosin (H&amp;E), followed by the automated computation of markers characterizing tissue regions driving the predictions. IMILIA is composed of an inflammation prediction module, consisting of a Multiple Instance Learning (MIL) model, and an interpretability module, divided in two blocks: HistoPLUS, for cell instance detection, segmentation and classification; and EpiSeg, for epithelium segmentation. IMILIA achieves a cross-validation ROC-AUC of 0.83 on the discovery cohort, and a ROC-AUC of 0.99 and 0.84 on two external validation cohorts. The interpretability module yields biologically consistent insights: tiles with higher predicted scores show increased densities of immune cells (lymphocytes, plasmocytes, neutrophils and eosinophils), whereas lower-scored tiles predominantly contain normal epithelial cells. Notably, these patterns were consistent across all datasets. Code and models to partially replicate the results on the public IBDColEpi dataset can be found at https://github.com/owkin/imilia.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T15:36:09Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Thalyssa Baiocco-Rodrigues</name>
    </author>
    <author>
      <name>Antoine Olivier</name>
    </author>
    <author>
      <name>Reda Belbahri</name>
    </author>
    <author>
      <name>Thomas Duboudin</name>
    </author>
    <author>
      <name>Pierre-Antoine Bannier</name>
    </author>
    <author>
      <name>Benjamin Adjadj</name>
    </author>
    <author>
      <name>Katharina Von Loga</name>
    </author>
    <author>
      <name>Nathan Noiry</name>
    </author>
    <author>
      <name>Maxime Touzot</name>
    </author>
    <author>
      <name>Hector Roux de Bezieux</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13438v1</id>
    <title>From User Interface to Agent Interface: Efficiency Optimization of UI Representations for LLM Agents</title>
    <updated>2025-12-15T15:34:06Z</updated>
    <link href="https://arxiv.org/abs/2512.13438v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13438v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>While Large Language Model (LLM) agents show great potential for automated UI navigation such as automated UI testing and AI assistants, their efficiency has been largely overlooked. Our motivating study reveals that inefficient UI representation creates a critical performance bottleneck. However, UI representation optimization, formulated as the task of automatically generating programs that transform UI representations, faces two unique challenges. First, the lack of Boolean oracles, which traditional program synthesis uses to decisively validate semantic correctness, poses a fundamental challenge to co-optimization of token efficiency and completeness. Second, the need to process large, complex UI trees as input while generating long, compositional transformation programs, making the search space vast and error-prone. Toward addressing the preceding limitations, we present UIFormer, the first automated optimization framework that synthesizes UI transformation programs by conducting constraint-based optimization with structured decomposition of the complex synthesis task. First, UIFormer restricts the program space using a domain-specific language (DSL) that captures UI-specific operations. Second, UIFormer conducts LLM-based iterative refinement with correctness and efficiency rewards, providing guidance for achieving the efficiency-completeness co-optimization. UIFormer operates as a lightweight plugin that applies transformation programs for seamless integration with existing LLM agents, requiring minimal modifications to their core logic. Evaluations across three UI navigation benchmarks spanning Android and Web platforms with five LLMs demonstrate that UIFormer achieves 48.7% to 55.8% token reduction with minimal runtime overhead while maintaining or improving agent performance. Real-world industry deployment at WeChat further validates the practical impact of UIFormer.</summary>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T15:34:06Z</published>
    <arxiv:primary_category term="cs.SE"/>
    <author>
      <name>Dezhi Ran</name>
    </author>
    <author>
      <name>Zhi Gong</name>
    </author>
    <author>
      <name>Yuzhe Guo</name>
    </author>
    <author>
      <name>Mengzhou Wu</name>
    </author>
    <author>
      <name>Yuan Cao</name>
    </author>
    <author>
      <name>Haochuan Lu</name>
    </author>
    <author>
      <name>Hengyu Zhang</name>
    </author>
    <author>
      <name>Xia Zeng</name>
    </author>
    <author>
      <name>Gang Cao</name>
    </author>
    <author>
      <name>Liangchao Yao</name>
    </author>
    <author>
      <name>Yuetang Deng</name>
    </author>
    <author>
      <name>Wei Yang</name>
    </author>
    <author>
      <name>Tao Xie</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13434v1</id>
    <title>Self-Supervised Ultrasound Representation Learning for Renal Anomaly Prediction in Prenatal Imaging</title>
    <updated>2025-12-15T15:28:02Z</updated>
    <link href="https://arxiv.org/abs/2512.13434v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13434v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Prenatal ultrasound is the cornerstone for detecting congenital anomalies of the kidneys and urinary tract, but diagnosis is limited by operator dependence and suboptimal imaging conditions. We sought to assess the performance of a self-supervised ultrasound foundation model for automated fetal renal anomaly classification using a curated dataset of 969 two-dimensional ultrasound images. A pretrained Ultrasound Self-Supervised Foundation Model with Masked Autoencoding (USF-MAE) was fine-tuned for binary and multi-class classification of normal kidneys, urinary tract dilation, and multicystic dysplastic kidney. Models were compared with a DenseNet-169 convolutional baseline using cross-validation and an independent test set. USF-MAE consistently improved upon the baseline across all evaluation metrics in both binary and multi-class settings. USF-MAE achieved an improvement of about 1.87% (AUC) and 7.8% (F1-score) on the validation set, 2.32% (AUC) and 4.33% (F1-score) on the independent holdout test set. The largest gains were observed in the multi-class setting, where the improvement in AUC was 16.28% and 46.15% in F1-score. To facilitate model interpretability, Score-CAM visualizations were adapted for a transformer architecture and show that model predictions were informed by known, clinically relevant renal structures, including the renal pelvis in urinary tract dilation and cystic regions in multicystic dysplastic kidney. These results show that ultrasound-specific self-supervised learning can generate a useful representation as a foundation for downstream diagnostic tasks. The proposed framework offers a robust, interpretable approach to support the prenatal detection of renal anomalies and demonstrates the promise of foundation models in obstetric imaging.</summary>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T15:28:02Z</published>
    <arxiv:comment>14 pages, 8 figures, 4 tables</arxiv:comment>
    <arxiv:primary_category term="eess.IV"/>
    <author>
      <name>Youssef Megahed</name>
    </author>
    <author>
      <name>Inok Lee</name>
    </author>
    <author>
      <name>Robin Ducharme</name>
    </author>
    <author>
      <name>Kevin Dick</name>
    </author>
    <author>
      <name>Adrian D. C. Chan</name>
    </author>
    <author>
      <name>Steven Hawken</name>
    </author>
    <author>
      <name>Mark C. Walker</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13430v1</id>
    <title>Weak Enforcement and Low Compliance in PCI~DSS: A Comparative Security Study</title>
    <updated>2025-12-15T15:19:33Z</updated>
    <link href="https://arxiv.org/abs/2512.13430v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13430v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Although credit and debit card data continue to be a prime target for attackers, organizational adherence to the Payment Card Industry Data Security Standard (PCI DSS) remains surprisingly low. Despite prior work showing that PCI DSS can reduce card fraud, only 32.4% of organizations were fully compliant in 2022, suggesting possible deficiencies in enforcement mechanisms. This study compares PCI DSS with three data security frameworks, HIPAA, NIS2, and GDPR, to examine how enforcement mechanisms relate to implementation success. The analysis reveals that PCI DSS significantly lags far behind these security frameworks and that its sanctions are orders of magnitude smaller than those under GDPR and NIS2. The findings indicate a positive association between stronger, multi-modal enforcement (including public disclosure, license actions, and imprisonment) and higher implementation rates, and highlights the structural weakness of PCI DSS's bank-dependent monitoring model. Enhanced non-monetary sanctions and the creation of an independent supervisory authority are recommended to increase transparency, reduce conflicts of interest, and improve PCI DSS compliance without discouraging card acceptance.</summary>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T15:19:33Z</published>
    <arxiv:comment>6 pages, 1 figure, 7 tables</arxiv:comment>
    <arxiv:primary_category term="cs.CR"/>
    <author>
      <name>Soonwon Park</name>
    </author>
    <author>
      <name>John D. Hastings</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13428v1</id>
    <title>A Domain-Adapted Lightweight Ensemble for Resource-Efficient Few-Shot Plant Disease Classification</title>
    <updated>2025-12-15T15:17:29Z</updated>
    <link href="https://arxiv.org/abs/2512.13428v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13428v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Accurate and timely identification of plant leaf diseases is essential for resilient and sustainable agriculture, yet most deep learning approaches rely on large annotated datasets and computationally intensive models that are unsuitable for data-scarce and resource-constrained environments. To address these challenges we present a few-shot learning approach within a lightweight yet efficient framework that combines domain-adapted MobileNetV2 and MobileNetV3 models as feature extractors, along with a feature fusion technique to generate robust feature representation. For the classification task, the fused features are passed through a Bi-LSTM classifier enhanced with attention mechanisms to capture sequential dependencies and focus on the most relevant features, thereby achieving optimal classification performance even in complex, real-world environments with noisy or cluttered backgrounds. The proposed framework was evaluated across multiple experimental setups, including both laboratory-controlled and field-captured datasets. On tomato leaf diseases from the PlantVillage dataset, it consistently improved performance across 1 to 15 shot scenarios, reaching 98.23+-0.33% at 15 shot, closely approaching the 99.98% SOTA benchmark achieved by a Transductive LSTM with attention, while remaining lightweight and mobile-friendly. Under real-world conditions using field images from the Dhan Shomadhan dataset, it maintained robust performance, reaching 69.28+-1.49% at 15-shot and demonstrating strong resilience to complex backgrounds. Notably, it also outperformed the previous SOTA accuracy of 96.0% on six diseases from PlantVillage, achieving 99.72% with only 15-shot learning. With a compact model size of approximately 40 MB and inference complexity of approximately 1.12 GFLOPs, this work establishes a scalable, mobile-ready foundation for precise plant disease diagnostics in data-scarce regions.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T15:17:29Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Anika Islam</name>
    </author>
    <author>
      <name>Tasfia Tahsin</name>
    </author>
    <author>
      <name>Zaarin Anjum</name>
    </author>
    <author>
      <name>Md. Bakhtiar Hasan</name>
    </author>
    <author>
      <name>Md. Hasanul Kabir</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13427v1</id>
    <title>MineTheGap: Automatic Mining of Biases in Text-to-Image Models</title>
    <updated>2025-12-15T15:17:02Z</updated>
    <link href="https://arxiv.org/abs/2512.13427v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13427v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Text-to-Image (TTI) models generate images based on text prompts, which often leave certain aspects of the desired image ambiguous. When faced with these ambiguities, TTI models have been shown to exhibit biases in their interpretations. These biases can have societal impacts, e.g., when showing only a certain race for a stated occupation. They can also affect user experience when creating redundancy within a set of generated images instead of spanning diverse possibilities. Here, we introduce MineTheGap - a method for automatically mining prompts that cause a TTI model to generate biased outputs. Our method goes beyond merely detecting bias for a given prompt. Rather, it leverages a genetic algorithm to iteratively refine a pool of prompts, seeking for those that expose biases. This optimization process is driven by a novel bias score, which ranks biases according to their severity, as we validate on a dataset with known biases. For a given prompt, this score is obtained by comparing the distribution of generated images to the distribution of LLM-generated texts that constitute variations on the prompt. Code and examples are available on the project's webpage.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T15:17:02Z</published>
    <arxiv:comment>Code and examples are available on the project's webpage at https://noa-cohen.github.io/MineTheGap/</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Noa Cohen</name>
    </author>
    <author>
      <name>Nurit Spingarn-Eliezer</name>
    </author>
    <author>
      <name>Inbar Huberman-Spiegelglas</name>
    </author>
    <author>
      <name>Tomer Michaeli</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13422v1</id>
    <title>QMon: Monitoring the Execution of Quantum Circuits with Mid-Circuit Measurement and Reset</title>
    <updated>2025-12-15T15:14:53Z</updated>
    <link href="https://arxiv.org/abs/2512.13422v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13422v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Unlike classical software, where logging and runtime tracing can effectively reveal internal execution status, quantum circuits possess unique properties, such as the no-cloning theorem and measurement-induced collapse, that prevent direct observation or duplication of their states. These characteristics make it especially challenging to monitor the execution of quantum circuits, complicating essential tasks such as debugging and runtime monitoring. This paper presents QMON, a practical methodology that leverages mid-circuit measurements and reset operations to monitor the internal states of quantum circuits while preserving their original runtime behavior. QMON enables the instrumentation of monitoring operators at developer-specified locations within the circuit, allowing comparisons between expected and observed quantum-state probabilities at those locations. We evaluated QMON by analyzing its impact on circuit behavior, monitoring coverage, and effectiveness in bug localization. Experimental results involving 154 quantum circuits show that all circuits preserve their intended functionality after instrumentation and that QMON successfully detects and localizes various programming errors. Although monitoring coverage is limited by the need to preserve delicate quantum properties, such as entanglement, QMON effectively detects errors while introducing no or negligible disturbance to the original quantum states. QMON facilitates the development of more robust and reliable quantum software as the field continues to mature.</summary>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T15:14:53Z</published>
    <arxiv:primary_category term="cs.SE"/>
    <author>
      <name>Ning Ma</name>
    </author>
    <author>
      <name>Jianjun Zhao</name>
    </author>
    <author>
      <name>Foutse Khomh</name>
    </author>
    <author>
      <name>Shaukat Ali</name>
    </author>
    <author>
      <name>Heng Li</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13421v1</id>
    <title>RecTok: Reconstruction Distillation along Rectified Flow</title>
    <updated>2025-12-15T15:14:20Z</updated>
    <link href="https://arxiv.org/abs/2512.13421v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13421v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Visual tokenizers play a crucial role in diffusion models. The dimensionality of latent space governs both reconstruction fidelity and the semantic expressiveness of the latent feature. However, a fundamental trade-off is inherent between dimensionality and generation quality, constraining existing methods to low-dimensional latent spaces. Although recent works have leveraged vision foundation models to enrich the semantics of visual tokenizers and accelerate convergence, high-dimensional tokenizers still underperform their low-dimensional counterparts. In this work, we propose RecTok, which overcomes the limitations of high-dimensional visual tokenizers through two key innovations: flow semantic distillation and reconstruction--alignment distillation. Our key insight is to make the forward flow in flow matching semantically rich, which serves as the training space of diffusion transformers, rather than focusing on the latent space as in previous works. Specifically, our method distills the semantic information in VFMs into the forward flow trajectories in flow matching. And we further enhance the semantics by introducing a masked feature reconstruction loss. Our RecTok achieves superior image reconstruction, generation quality, and discriminative performance. It achieves state-of-the-art results on the gFID-50K under both with and without classifier-free guidance settings, while maintaining a semantically rich latent space structure. Furthermore, as the latent dimensionality increases, we observe consistent improvements. Code and model are available at https://shi-qingyu.github.io/rectok.github.io.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T15:14:20Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Qingyu Shi</name>
    </author>
    <author>
      <name>Size Wu</name>
    </author>
    <author>
      <name>Jinbin Bai</name>
    </author>
    <author>
      <name>Kaidong Yu</name>
    </author>
    <author>
      <name>Yujing Wang</name>
    </author>
    <author>
      <name>Yunhai Tong</name>
    </author>
    <author>
      <name>Xiangtai Li</name>
    </author>
    <author>
      <name>Xuelong Li</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13416v1</id>
    <title>Learning to Generate Cross-Task Unexploitable Examples</title>
    <updated>2025-12-15T15:05:38Z</updated>
    <link href="https://arxiv.org/abs/2512.13416v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13416v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Unexploitable example generation aims to transform personal images into their unexploitable (unlearnable) versions before they are uploaded online, thereby preventing unauthorized exploitation of online personal images. Recently, this task has garnered significant research attention due to its critical relevance to personal data privacy. Yet, despite recent progress, existing methods for this task can still suffer from limited practical applicability, as they can fail to generate examples that are broadly unexploitable across different real-world computer vision tasks. To deal with this problem, in this work, we propose a novel Meta Cross-Task Unexploitable Example Generation (MCT-UEG) framework. At the core of our framework, to optimize the unexploitable example generator for effectively producing broadly unexploitable examples, we design a flat-minima-oriented meta training and testing scheme. Extensive experiments show the efficacy of our framework.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T15:05:38Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Haoxuan Qu</name>
    </author>
    <author>
      <name>Qiuchi Xiang</name>
    </author>
    <author>
      <name>Yujun Cai</name>
    </author>
    <author>
      <name>Yirui Wu</name>
    </author>
    <author>
      <name>Majid Mirmehdi</name>
    </author>
    <author>
      <name>Hossein Rahmani</name>
    </author>
    <author>
      <name>Jun Liu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13415v1</id>
    <title>USTM: Unified Spatial and Temporal Modeling for Continuous Sign Language Recognition</title>
    <updated>2025-12-15T15:05:16Z</updated>
    <link href="https://arxiv.org/abs/2512.13415v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13415v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Continuous sign language recognition (CSLR) requires precise spatio-temporal modeling to accurately recognize sequences of gestures in videos. Existing frameworks often rely on CNN-based spatial backbones combined with temporal convolution or recurrent modules. These techniques fail in capturing fine-grained hand and facial cues and modeling long-range temporal dependencies. To address these limitations, we propose the Unified Spatio-Temporal Modeling (USTM) framework, a spatio-temporal encoder that effectively models complex patterns using a combination of a Swin Transformer backbone enhanced with lightweight temporal adapter with positional embeddings (TAPE). Our framework captures fine-grained spatial features alongside short and long-term temporal context, enabling robust sign language recognition from RGB videos without relying on multi-stream inputs or auxiliary modalities. Extensive experiments on benchmarked datasets including PHOENIX14, PHOENIX14T, and CSL-Daily demonstrate that USTM achieves state-of-the-art performance against RGB-based as well as multi-modal CSLR approaches, while maintaining competitive performance against multi-stream approaches. These results highlight the strength and efficacy of the USTM framework for CSLR. The code is available at https://github.com/gufranSabri/USTM</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T15:05:16Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Ahmed Abul Hasanaath</name>
    </author>
    <author>
      <name>Hamzah Luqman</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13414v1</id>
    <title>PSALM: applying Proportional SAmpLing strategy in Metamorphic testing</title>
    <updated>2025-12-15T15:04:15Z</updated>
    <link href="https://arxiv.org/abs/2512.13414v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13414v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Metamorphic testing (MT) alleviates the oracle problem by checking metamorphic relations (MRs) across multiple test executions. The fault detection effectiveness of MT is influenced not only by the choice and quality of MRs, but also by how source test cases and metamorphic groups (MGs) are selected. While substantial research has focused on designing, generating, and validating MRs, systematic methods for source test case selection and MG selection remain largely unexplored. Although the Proportional Sampling Strategy (PSS) provides strong theoretical guarantees in traditional testing, its assumptions cannot be directly applied in MT due to differences in selection domains, test units, and failure distributions. This paper proposes PSALM, an adaptation of PSS to MT for both source test case selection and MG selection. We formally prove that PSALM is never inferior to random selection regardless of how the source test case and MG domains are partitioned. We further identify the conditions under which applying PSALM to source test case selection and MG selection yields identical effectiveness. A comprehensive empirical study on eight subject programs and 184 mutants shows that the results are consistent with our theoretical analysis and that PSALM generally performs more effectively than existing selection strategies such as ART and MT-ART. These results demonstrate that PSALM provides a theoretically grounded and practically effective selection strategy for MT.</summary>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T15:04:15Z</published>
    <arxiv:primary_category term="cs.SE"/>
    <author>
      <name>Zenghui Zhou</name>
    </author>
    <author>
      <name>Pak-Lok Poon</name>
    </author>
    <author>
      <name>Zheng Zheng</name>
    </author>
    <author>
      <name>Xiao-Yi Zhang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13411v1</id>
    <title>Computer vision training dataset generation for robotic environments using Gaussian splatting</title>
    <updated>2025-12-15T15:00:17Z</updated>
    <link href="https://arxiv.org/abs/2512.13411v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13411v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>This paper introduces a novel pipeline for generating large-scale, highly realistic, and automatically labeled datasets for computer vision tasks in robotic environments. Our approach addresses the critical challenges of the domain gap between synthetic and real-world imagery and the time-consuming bottleneck of manual annotation. We leverage 3D Gaussian Splatting (3DGS) to create photorealistic representations of the operational environment and objects. These assets are then used in a game engine where physics simulations create natural arrangements. A novel, two-pass rendering technique combines the realism of splats with a shadow map generated from proxy meshes. This map is then algorithmically composited with the image to add both physically plausible shadows and subtle highlights, significantly enhancing realism. Pixel-perfect segmentation masks are generated automatically and formatted for direct use with object detection models like YOLO. Our experiments show that a hybrid training strategy, combining a small set of real images with a large volume of our synthetic data, yields the best detection and segmentation performance, confirming this as an optimal strategy for efficiently achieving robust and accurate models.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T15:00:17Z</published>
    <arxiv:comment>Code available at: https://patrykni.github.io/UnitySplat2Data/</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Patryk Niżeniec</name>
    </author>
    <author>
      <name>Marcin Iwanowski</name>
    </author>
  </entry>
</feed>
