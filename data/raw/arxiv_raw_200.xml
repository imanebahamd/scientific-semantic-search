<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/" xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns="http://www.w3.org/2005/Atom">
  <id>https://arxiv.org/api/JSZ73027W7AINDlyUmEdfUcRtsg</id>
  <title>arXiv Query: search_query=cat:cs.AI OR cat:cs.LG OR cat:cs.CL OR cat:cs.CV OR cat:cs.NE OR cat:cs.SE OR cat:cs.CR OR cat:cs.DC&amp;id_list=&amp;start=200&amp;max_results=100</title>
  <updated>2025-12-16T13:50:32Z</updated>
  <link href="https://arxiv.org/api/query?search_query=cat:cs.AI+OR+(cat:cs.LG+OR+(cat:cs.CL+OR+(cat:cs.CV+OR+(cat:cs.NE+OR+(cat:cs.SE+OR+(cat:cs.CR+OR+cat:cs.DC))))))&amp;start=200&amp;max_results=100&amp;id_list=" type="application/atom+xml"/>
  <opensearch:itemsPerPage>100</opensearch:itemsPerPage>
  <opensearch:totalResults>566020</opensearch:totalResults>
  <opensearch:startIndex>200</opensearch:startIndex>
  <entry>
    <id>http://arxiv.org/abs/2512.13069v1</id>
    <title>Multi-fidelity aerodynamic data fusion by autoencoder transfer learning</title>
    <updated>2025-12-15T08:06:52Z</updated>
    <link href="https://arxiv.org/abs/2512.13069v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13069v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Accurate aerodynamic prediction often relies on high-fidelity simulations; however, their prohibitive computational costs severely limit their applicability in data-driven modeling. This limitation motivates the development of multi-fidelity strategies that leverage inexpensive low-fidelity information without compromising accuracy. Addressing this challenge, this work presents a multi-fidelity deep learning framework that combines autoencoder-based transfer learning with a newly developed Multi-Split Conformal Prediction (MSCP) strategy to achieve uncertainty-aware aerodynamic data fusion under extreme data scarcity. The methodology leverages abundant Low-Fidelity (LF) data to learn a compact latent physics representation, which acts as a frozen knowledge base for a decoder that is subsequently fine-tuned using scarce HF samples. Tested on surface-pressure distributions for NACA airfoils (2D) and a transonic wing (3D) databases, the model successfully corrects LF deviations and achieves high-accuracy pressure predictions using minimal HF training data. Furthermore, the MSCP framework produces robust, actionable uncertainty bands with pointwise coverage exceeding 95%. By combining extreme data efficiency with uncertainty quantification, this work offers a scalable and reliable solution for aerodynamic regression in data-scarce environments.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.flu-dyn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T08:06:52Z</published>
    <arxiv:comment>29 pages, 13 figures</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Javier Nieto-Centenero</name>
    </author>
    <author>
      <name>Esther Andrés</name>
    </author>
    <author>
      <name>Rodrigo Castellanos</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13063v1</id>
    <title>LLM Rationalis? Measuring Bargaining Capabilities of AI Negotiators</title>
    <updated>2025-12-15T07:50:09Z</updated>
    <link href="https://arxiv.org/abs/2512.13063v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13063v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Bilateral negotiation is a complex, context-sensitive task in which human negotiators dynamically adjust anchors, pacing, and flexibility to exploit power asymmetries and informal cues. We introduce a unified mathematical framework for modeling concession dynamics based on a hyperbolic tangent curve, and propose two metrics burstiness tau and the Concession-Rigidity Index (CRI) to quantify the timing and rigidity of offer trajectories. We conduct a large-scale empirical comparison between human negotiators and four state-of-the-art large language models (LLMs) across natural-language and numeric-offers settings, with and without rich market context, as well as six controlled power-asymmetry scenarios. Our results reveal that, unlike humans who smoothly adapt to situations and infer the opponents position and strategies, LLMs systematically anchor at extremes of the possible agreement zone for negotiations and optimize for fixed points irrespective of leverage or context. Qualitative analysis further shows limited strategy diversity and occasional deceptive tactics used by LLMs. Moreover the ability of LLMs to negotiate does not improve with better models. These findings highlight fundamental limitations in current LLM negotiation capabilities and point to the need for models that better internalize opponent reasoning and context-dependent strategy.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T07:50:09Z</published>
    <arxiv:comment>Published in the First Workshop on Multi-Turn Interactions in Large Language Models at Neurips 2025</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Cheril Shah</name>
    </author>
    <author>
      <name>Akshit Agarwal</name>
    </author>
    <author>
      <name>Kanak Garg</name>
    </author>
    <author>
      <name>Mourad Heddaya</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13060v1</id>
    <title>Deep Q-Learning-Based Intelligent Scheduling for ETL Optimization in Heterogeneous Data Environments</title>
    <updated>2025-12-15T07:38:47Z</updated>
    <link href="https://arxiv.org/abs/2512.13060v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13060v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>This paper addresses the challenges of low scheduling efficiency, unbalanced resource allocation, and poor adaptability in ETL (Extract-Transform-Load) processes under heterogeneous data environments by proposing an intelligent scheduling optimization framework based on deep Q-learning. The framework formalizes the ETL scheduling process as a Markov Decision Process and enables adaptive decision-making by a reinforcement learning agent in high-dimensional state spaces to dynamically optimize task allocation and resource scheduling. The model consists of a state representation module, a feature embedding network, a Q-value estimator, and a reward evaluation mechanism, which collectively consider task dependencies, node load states, and data flow characteristics to derive the optimal scheduling strategy in complex environments. A multi-objective reward function is designed to balance key performance indicators such as average scheduling delay, task completion rate, throughput, and resource utilization. Sensitivity experiments further verify the model's robustness under changes in hyperparameters, environmental dynamics, and data scale. Experimental results show that the proposed deep Q-learning scheduling framework significantly reduces scheduling delay, improves system throughput, and enhances execution stability under multi-source heterogeneous task conditions, demonstrating the strong potential of reinforcement learning in complex data scheduling and resource management, and providing an efficient and scalable optimization strategy for intelligent data pipeline construction.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T07:38:47Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Kangning Gao</name>
    </author>
    <author>
      <name>Yi Hu</name>
    </author>
    <author>
      <name>Cong Nie</name>
    </author>
    <author>
      <name>Wei Li</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13059v1</id>
    <title>An Open and Reproducible Deep Research Agent for Long-Form Question Answering</title>
    <updated>2025-12-15T07:37:53Z</updated>
    <link href="https://arxiv.org/abs/2512.13059v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13059v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We present an open deep research system for long-form question answering, selected as a winning system in the text-to-text track of the MMU-RAG competition at NeurIPS 2025. The system combines an open-source large language model (LLM) with an open web search API to perform iterative retrieval, reasoning, and synthesis in real-world open-domain settings. To enhance reasoning quality, we apply preference tuning based on LLM-as-a-judge feedback that evaluates multiple aspects, including clarity, insightfulness, and factuality. Our experimental results show that the proposed method consistently improves answer quality across all three aspects. Our source code is publicly available at https://github.com/efficient-deep-research/efficient-deep-research.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T07:37:53Z</published>
    <arxiv:comment>Technical report of a winning system in the NeurIPS MMU-RAG competition</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Ikuya Yamada</name>
    </author>
    <author>
      <name>Wataru Ikeda</name>
    </author>
    <author>
      <name>Ko Yoshida</name>
    </author>
    <author>
      <name>Mengyu Ye</name>
    </author>
    <author>
      <name>Hinata Sugimoto</name>
    </author>
    <author>
      <name>Masatoshi Suzuki</name>
    </author>
    <author>
      <name>Hisanori Ozaki</name>
    </author>
    <author>
      <name>Jun Suzuki</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13055v1</id>
    <title>Towards Test-time Efficient Visual Place Recognition via Asymmetric Query Processing</title>
    <updated>2025-12-15T07:30:17Z</updated>
    <link href="https://arxiv.org/abs/2512.13055v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13055v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Visual Place Recognition (VPR) has advanced significantly with high-capacity foundation models like DINOv2, achieving remarkable performance. Nonetheless, their substantial computational cost makes deployment on resource-constrained devices impractical. In this paper, we introduce an efficient asymmetric VPR framework that incorporates a high-capacity gallery model for offline feature extraction with a lightweight query network for online processing. A key challenge in this setting is ensuring compatibility between these heterogeneous networks, which conventional approaches address through computationally expensive k-NN-based compatible training. To overcome this, we propose a geographical memory bank that structures gallery features using geolocation metadata inherent in VPR databases, eliminating the need for exhaustive k-NN computations. Additionally, we introduce an implicit embedding augmentation technique that enhances the query network to model feature variations despite its limited capacity. Extensive experiments demonstrate that our method not only significantly reduces computational costs but also outperforms existing asymmetric retrieval techniques, establishing a new aspect for VPR in resource-limited environments. The code is available at https://github.com/jaeyoon1603/AsymVPR</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T07:30:17Z</published>
    <arxiv:comment>AAAI 2026</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Jaeyoon Kim</name>
    </author>
    <author>
      <name>Yoonki Cho</name>
    </author>
    <author>
      <name>Sung-Eui Yoon</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13047v1</id>
    <title>Sharpen the Spec, Cut the Code: A Case for Generative File System with SYSSPEC</title>
    <updated>2025-12-15T07:15:01Z</updated>
    <link href="https://arxiv.org/abs/2512.13047v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13047v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>File systems are critical OS components that require constant evolution to support new hardware and emerging application needs. However, the traditional paradigm of developing features, fixing bugs, and maintaining the system incurs significant overhead, especially as systems grow in complexity. This paper proposes a new paradigm, generative file systems, which leverages Large Language Models (LLMs) to generate and evolve a file system from prompts, effectively addressing the need for robust evolution. Despite the widespread success of LLMs in code generation, attempts to create a functional file system have thus far been unsuccessful, mainly due to the ambiguity of natural language prompts.
  This paper introduces SYSSPEC, a framework for developing generative file systems. Its key insight is to replace ambiguous natural language with principles adapted from formal methods. Instead of imprecise prompts, SYSSPEC employs a multi-part specification that accurately describes a file system's functionality, modularity, and concurrency. The specification acts as an unambiguous blueprint, guiding LLMs to generate expected code flexibly. To manage evolution, we develop a DAG-structured patch that operates on the specification itself, enabling new features to be added without violating existing invariants. Moreover, the SYSSPEC toolchain features a set of LLM-based agents with mechanisms to mitigate hallucination during construction and evolution. We demonstrate our approach by generating SPECFS, a concurrent file system. SPECFS passes hundreds of regression tests, matching a manually-coded baseline. We further confirm its evolvability by seamlessly integrating 10 real-world features from Ext4. Our work shows that a specification-guided approach makes generating and evolving complex systems not only feasible but also highly effective.</summary>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T07:15:01Z</published>
    <arxiv:primary_category term="cs.OS"/>
    <author>
      <name>Qingyuan Liu</name>
    </author>
    <author>
      <name>Zou Mo</name>
    </author>
    <author>
      <name>Hengbin Zhang</name>
    </author>
    <author>
      <name>Dong Du</name>
    </author>
    <author>
      <name>Yubin Xia</name>
    </author>
    <author>
      <name>Haibo Chen</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13043v1</id>
    <title>GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training</title>
    <updated>2025-12-15T07:11:56Z</updated>
    <link href="https://arxiv.org/abs/2512.13043v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13043v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Multi-turn reinforcement learning (RL) for multi-modal agents built upon vision-language models (VLMs) is hampered by sparse rewards and long-horizon credit assignment. Recent methods densify the reward by querying a teacher that provides step-level feedback, e.g., Guided Thought Reinforcement (GTR) and On-Policy Distillation, but rely on costly, often privileged models as the teacher, limiting practicality and reproducibility. We introduce GTR-Turbo, a highly efficient upgrade to GTR, which matches the performance without training or querying an expensive teacher model. Specifically, GTR-Turbo merges the weights of checkpoints produced during the ongoing RL training, and then uses this merged model as a "free" teacher to guide the subsequent RL via supervised fine-tuning or soft logit distillation. This design removes dependence on privileged VLMs (e.g., GPT or Gemini), mitigates the "entropy collapse" observed in prior work, and keeps training stable. Across diverse visual agentic tasks, GTR-Turbo improves the accuracy of the baseline model by 10-30% while reducing wall-clock training time by 50% and compute cost by 60% relative to GTR.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T07:11:56Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Tong Wei</name>
    </author>
    <author>
      <name>Yijun Yang</name>
    </author>
    <author>
      <name>Changhao Zhang</name>
    </author>
    <author>
      <name>Junliang Xing</name>
    </author>
    <author>
      <name>Yuanchun Shi</name>
    </author>
    <author>
      <name>Zongqing Lu</name>
    </author>
    <author>
      <name>Deheng Ye</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13040v1</id>
    <title>Understanding Structured Financial Data with LLMs: A Case Study on Fraud Detection</title>
    <updated>2025-12-15T07:09:11Z</updated>
    <link href="https://arxiv.org/abs/2512.13040v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13040v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Detecting fraud in financial transactions typically relies on tabular models that demand heavy feature engineering to handle high-dimensional data and offer limited interpretability, making it difficult for humans to understand predictions. Large Language Models (LLMs), in contrast, can produce human-readable explanations and facilitate feature analysis, potentially reducing the manual workload of fraud analysts and informing system refinements. However, they perform poorly when applied directly to tabular fraud detection due to the difficulty of reasoning over many features, the extreme class imbalance, and the absence of contextual information. To bridge this gap, we introduce FinFRE-RAG, a two-stage approach that applies importance-guided feature reduction to serialize a compact subset of numeric/categorical attributes into natural language and performs retrieval-augmented in-context learning over label-aware, instance-level exemplars. Across four public fraud datasets and three families of open-weight LLMs, FinFRE-RAG substantially improves F1/MCC over direct prompting and is competitive with strong tabular baselines in several settings. Although these LLMs still lag behind specialized classifiers, they narrow the performance gap and provide interpretable rationales, highlighting their value as assistive tools in fraud analysis.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T07:09:11Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Xuwei Tan</name>
    </author>
    <author>
      <name>Yao Ma</name>
    </author>
    <author>
      <name>Xueru Zhang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13039v1</id>
    <title>Bi-Erasing: A Bidirectional Framework for Concept Removal in Diffusion Models</title>
    <updated>2025-12-15T07:08:35Z</updated>
    <link href="https://arxiv.org/abs/2512.13039v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13039v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Concept erasure, which fine-tunes diffusion models to remove undesired or harmful visual concepts, has become a mainstream approach to mitigating unsafe or illegal image generation in text-to-image models.However, existing removal methods typically adopt a unidirectional erasure strategy by either suppressing the target concept or reinforcing safe alternatives, making it difficult to achieve a balanced trade-off between concept removal and generation quality. To address this limitation, we propose a novel Bidirectional Image-Guided Concept Erasure (Bi-Erasing) framework that performs concept suppression and safety enhancement simultaneously. Specifically, based on the joint representation of text prompts and corresponding images, Bi-Erasing introduces two decoupled image branches: a negative branch responsible for suppressing harmful semantics and a positive branch providing visual guidance for safe alternatives. By jointly optimizing these complementary directions, our approach achieves a balance between erasure efficacy and generation usability. In addition, we apply mask-based filtering to the image branches to prevent interference from irrelevant content during the erasure process. Across extensive experiment evaluations, the proposed Bi-Erasing outperforms baseline methods in balancing concept removal effectiveness and visual fidelity.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T07:08:35Z</published>
    <arxiv:comment>Under Review</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Hao Chen</name>
    </author>
    <author>
      <name>Yiwei Wang</name>
    </author>
    <author>
      <name>Songze Li</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13037v1</id>
    <title>Progressive Refinement of E-commerce Search Ranking Based on Short-Term Activities of the Buyer</title>
    <updated>2025-12-15T07:07:32Z</updated>
    <link href="https://arxiv.org/abs/2512.13037v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13037v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>In e-commerce shopping, aligning search results with a buyer's immediate needs and preferences presents a significant challenge, particularly in adapting search results throughout the buyer's shopping journey as they move from the initial stages of browsing to making a purchase decision or shift from one intent to another. This study presents a systematic approach to adapting e-commerce search results based on the current context. We start with basic methods and incrementally incorporate more contextual information and state-of-the-art techniques to improve the search outcomes. By applying this evolving contextual framework to items displayed on the search engine results page (SERP), we progressively align search outcomes more closely with the buyer's interests and current search intentions. Our findings demonstrate that this incremental enhancement, from simple heuristic autoregressive features to advanced sequence models, significantly improves ranker performance. The integration of contextual techniques enhances the performance of our production ranker, leading to improved search results in both offline and online A/B testing in terms of Mean Reciprocal Rank (MRR). Overall, the paper details iterative methodologies and their substantial contributions to search result contextualization on e-commerce platforms.</summary>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T07:07:32Z</published>
    <arxiv:primary_category term="cs.IR"/>
    <arxiv:journal_ref>SIGIR '25: The 48th International ACM SIGIR Conference on Research and Development in Information Retrieval, Padua Italy, July 13 - 18, 2025</arxiv:journal_ref>
    <author>
      <name>Taoran Sheng</name>
    </author>
    <author>
      <name>Sathappan Muthiah</name>
    </author>
    <author>
      <name>Atiq Islam</name>
    </author>
    <author>
      <name>Jinming Feng</name>
    </author>
    <arxiv:doi>10.1145/3726302.3731966</arxiv:doi>
    <link rel="related" href="https://doi.org/10.1145/3726302.3731966" title="doi"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13034v1</id>
    <title>Alada: Alternating Adaptation of Momentum Method for Memory-Efficient Matrix Optimization</title>
    <updated>2025-12-15T07:04:51Z</updated>
    <link href="https://arxiv.org/abs/2512.13034v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13034v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>This work proposes Alada, an adaptive momentum method for stochastic optimization over large-scale matrices. Alada employs a rank-one factorization approach to estimate the second moment of gradients, where factors are updated alternatively to minimize the estimation error. Alada achieves sublinear memory overheads and can be readily extended to optimizing tensor-shaped variables.We also equip Alada with a first moment estimation rule, which enhances the algorithm's robustness without incurring additional memory overheads. The theoretical performance of Alada aligns with that of traditional methods such as Adam. Numerical studies conducted on several natural language processing tasks demonstrate the reduction in memory overheads and the robustness in training large models relative to Adam and its variants.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T07:04:51Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Xiaoyu He</name>
    </author>
    <author>
      <name>Yu Cai</name>
    </author>
    <author>
      <name>Jin Jia</name>
    </author>
    <author>
      <name>Canxi Huang</name>
    </author>
    <author>
      <name>Wenqing Chen</name>
    </author>
    <author>
      <name>Zibin Zheng</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13033v1</id>
    <title>Scaling Bidirectional Spans and Span Violations in Attention Mechanism</title>
    <updated>2025-12-15T07:03:24Z</updated>
    <link href="https://arxiv.org/abs/2512.13033v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13033v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The canonical $O(N^2)$ Transformer remains the empirical performance frontier in sequence modeling, and its training can be further optimized by addressing geometric inefficiency. We propose an optimization framework that leverages an asymmetric projection to decompose the backward-pass gradients into parallel spans and orthogonal violations, while keeping the canonical forward-pass $QKV$ structure intact. Through consistent experimental validation across various decomposition and projection setups, we provide strong theoretical evidence: the standard attention gradient is suboptimal. We demonstrated that selectively scaling these components, focusing primarily on $0^{th}$ order bidirectional parallel spans, yields the most effective learning signal. On the limited WikiText-2 dataset, and using a crude configuration, this method achieved a $0.56\%$ reduction in validation loss, confirming the framework's fundamental validity and suggesting significant potential gains on larger datasets and deeper training regimes</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T07:03:24Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Jongwook Kim</name>
    </author>
    <author>
      <name>Sangheon Yun</name>
    </author>
    <author>
      <name>Sukjin Yoon</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13031v1</id>
    <title>Comprehensive Evaluation of Rule-Based, Machine Learning, and Deep Learning in Human Estimation Using Radio Wave Sensing: Accuracy, Spatial Generalization, and Output Granularity Trade-offs</title>
    <updated>2025-12-15T07:02:06Z</updated>
    <link href="https://arxiv.org/abs/2512.13031v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13031v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>This study presents the first comprehensive comparison of rule-based methods, traditional machine learning models, and deep learning models in radio wave sensing with frequency modulated continuous wave multiple input multiple output radar. We systematically evaluated five approaches in two indoor environments with distinct layouts: a rule-based connected component method; three traditional machine learning models, namely k-nearest neighbors, random forest, and support vector machine; and a deep learning model combining a convolutional neural network and long short term memory. In the training environment, the convolutional neural network long short term memory model achieved the highest accuracy, while traditional machine learning models provided moderate performance. In a new layout, however, all learning based methods showed significant degradation, whereas the rule-based method remained stable. Notably, for binary detection of presence versus absence of people, all models consistently achieved high accuracy across layouts. These results demonstrate that high capacity models can produce fine grained outputs with high accuracy in the same environment, but they are vulnerable to domain shift. In contrast, rule-based methods cannot provide fine grained outputs but exhibit robustness against domain shift. Moreover, regardless of the model type, a clear trade off was revealed between spatial generalization performance and output granularity.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T07:02:06Z</published>
    <arxiv:comment>10 pages, 5 figures. A comprehensive comparison of rule-based, machine learning, and deep learning approaches for human estimation using FMCW MIMO radar, focusing on accuracy, spatial generalization, and output granularity</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Tomoya Tanaka</name>
    </author>
    <author>
      <name>Tomonori Ikeda</name>
    </author>
    <author>
      <name>Ryo Yonemoto</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13030v1</id>
    <title>Motus: A Unified Latent Action World Model</title>
    <updated>2025-12-15T06:58:40Z</updated>
    <link href="https://arxiv.org/abs/2512.13030v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13030v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>While a general embodied agent must function as a unified system, current methods are built on isolated models for understanding, world modeling, and control. This fragmentation prevents unifying multimodal generative capabilities and hinders learning from large-scale, heterogeneous data. In this paper, we propose Motus, a unified latent action world model that leverages existing general pretrained models and rich, sharable motion information. Motus introduces a Mixture-of-Transformer (MoT) architecture to integrate three experts (i.e., understanding, video generation, and action) and adopts a UniDiffuser-style scheduler to enable flexible switching between different modeling modes (i.e., world models, vision-language-action models, inverse dynamics models, video generation models, and video-action joint prediction models). Motus further leverages the optical flow to learn latent actions and adopts a recipe with three-phase training pipeline and six-layer data pyramid, thereby extracting pixel-level "delta action" and enabling large-scale action pretraining. Experiments show that Motus achieves superior performance against state-of-the-art methods in both simulation (a +15% improvement over X-VLA and a +45% improvement over Pi0.5) and real-world scenarios(improved by +11~48%), demonstrating unified modeling of all functionalities and priors significantly benefits downstream robotic tasks.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T06:58:40Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Hongzhe Bi</name>
    </author>
    <author>
      <name>Hengkai Tan</name>
    </author>
    <author>
      <name>Shenghao Xie</name>
    </author>
    <author>
      <name>Zeyuan Wang</name>
    </author>
    <author>
      <name>Shuhe Huang</name>
    </author>
    <author>
      <name>Haitian Liu</name>
    </author>
    <author>
      <name>Ruowen Zhao</name>
    </author>
    <author>
      <name>Yao Feng</name>
    </author>
    <author>
      <name>Chendong Xiang</name>
    </author>
    <author>
      <name>Yinze Rong</name>
    </author>
    <author>
      <name>Hongyan Zhao</name>
    </author>
    <author>
      <name>Hanyu Liu</name>
    </author>
    <author>
      <name>Zhizhong Su</name>
    </author>
    <author>
      <name>Lei Ma</name>
    </author>
    <author>
      <name>Hang Su</name>
    </author>
    <author>
      <name>Jun Zhu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13019v1</id>
    <title>SneakPeek: Future-Guided Instructional Streaming Video Generation</title>
    <updated>2025-12-15T06:32:57Z</updated>
    <link href="https://arxiv.org/abs/2512.13019v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13019v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Instructional video generation is an emerging task that aims to synthesize coherent demonstrations of procedural activities from textual descriptions. Such capability has broad implications for content creation, education, and human-AI interaction, yet existing video diffusion models struggle to maintain temporal consistency and controllability across long sequences of multiple action steps. We introduce a pipeline for future-driven streaming instructional video generation, dubbed SneakPeek, a diffusion-based autoregressive framework designed to generate precise, stepwise instructional videos conditioned on an initial image and structured textual prompts. Our approach introduces three key innovations to enhance consistency and controllability: (1) predictive causal adaptation, where a causal model learns to perform next-frame prediction and anticipate future keyframes; (2) future-guided self-forcing with a dual-region KV caching scheme to address the exposure bias issue at inference time; (3) multi-prompt conditioning, which provides fine-grained and procedural control over multi-step instructions. Together, these components mitigate temporal drift, preserve motion consistency, and enable interactive video generation where future prompt updates dynamically influence ongoing streaming video generation. Experimental results demonstrate that our method produces temporally coherent and semantically faithful instructional videos that accurately follow complex, multi-step task descriptions.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T06:32:57Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Cheeun Hong</name>
    </author>
    <author>
      <name>German Barquero</name>
    </author>
    <author>
      <name>Fadime Sener</name>
    </author>
    <author>
      <name>Markos Georgopoulos</name>
    </author>
    <author>
      <name>Edgar Schönfeld</name>
    </author>
    <author>
      <name>Stefan Popov</name>
    </author>
    <author>
      <name>Yuming Du</name>
    </author>
    <author>
      <name>Oscar Mañas</name>
    </author>
    <author>
      <name>Albert Pumarola</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13018v1</id>
    <title>Comprehensive Deployment-Oriented Assessment for Cross-Environment Generalization in Deep Learning-Based mmWave Radar Sensing</title>
    <updated>2025-12-15T06:29:51Z</updated>
    <link href="https://arxiv.org/abs/2512.13018v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13018v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>This study presents the first comprehensive evaluation of spatial generalization techniques, which are essential for the practical deployment of deep learning-based radio-frequency (RF) sensing. Focusing on people counting in indoor environments using frequency-modulated continuous-wave (FMCW) multiple-input multiple-output (MIMO) radar, we systematically investigate a broad set of approaches, including amplitude-based statistical preprocessing (sigmoid weighting and threshold zeroing), frequency-domain filtering, autoencoder-based background suppression, data augmentation strategies, and transfer learning. Experimental results collected across two environments with different layouts demonstrate that sigmoid-based amplitude weighting consistently achieves superior cross-environment performance, yielding 50.1% and 55.2% reductions in root-mean-square error (RMSE) and mean absolute error (MAE), respectively, compared with baseline methods. Data augmentation provides additional though modest benefits, with improvements up to 8.8% in MAE. By contrast, transfer learning proves indispensable for large spatial shifts, achieving 82.1% and 91.3% reductions in RMSE and MAE, respectively, with 540 target-domain samples. Taken together, these findings establish a highly practical direction for developing radar sensing systems capable of maintaining robust accuracy under spatial variations by integrating deep learning models with amplitude-based preprocessing and efficient transfer learning.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T06:29:51Z</published>
    <arxiv:comment>8 pages, 6 figures. Comprehensive evaluation of preprocessing, data augmentation, and transfer learning for cross-environment generalization in deep learning-based mmWave radar sensing</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Tomoya Tanaka</name>
    </author>
    <author>
      <name>Tomonori Ikeda</name>
    </author>
    <author>
      <name>Ryo Yonemoto</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13015v1</id>
    <title>What Happens Next? Next Scene Prediction with a Unified Video Model</title>
    <updated>2025-12-15T06:22:57Z</updated>
    <link href="https://arxiv.org/abs/2512.13015v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13015v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Recent unified models for joint understanding and generation have significantly advanced visual generation capabilities. However, their focus on conventional tasks like text-to-video generation has left the temporal reasoning potential of unified models largely underexplored. To address this gap, we introduce Next Scene Prediction (NSP), a new task that pushes unified video models toward temporal and causal reasoning. Unlike text-to-video generation, NSP requires predicting plausible futures from preceding context, demanding deeper understanding and reasoning. To tackle this task, we propose a unified framework combining Qwen-VL for comprehension and LTX for synthesis, bridged by a latent query embedding and a connector module. This model is trained in three stages on our newly curated, large-scale NSP dataset: text-to-video pre-training, supervised fine-tuning, and reinforcement learning (via GRPO) with our proposed causal consistency reward. Experiments demonstrate our model achieves state-of-the-art performance on our benchmark, advancing the capability of generalist multimodal systems to anticipate what happens next.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T06:22:57Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Xinjie Li</name>
    </author>
    <author>
      <name>Zhimin Chen</name>
    </author>
    <author>
      <name>Rui Zhao</name>
    </author>
    <author>
      <name>Florian Schiffers</name>
    </author>
    <author>
      <name>Zhenyu Liao</name>
    </author>
    <author>
      <name>Vimal Bhat</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13014v1</id>
    <title>JoDiffusion: Jointly Diffusing Image with Pixel-Level Annotations for Semantic Segmentation Promotion</title>
    <updated>2025-12-15T06:21:30Z</updated>
    <link href="https://arxiv.org/abs/2512.13014v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13014v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Given the inherently costly and time-intensive nature of pixel-level annotation, the generation of synthetic datasets comprising sufficiently diverse synthetic images paired with ground-truth pixel-level annotations has garnered increasing attention recently for training high-performance semantic segmentation models. However, existing methods necessitate to either predict pseudo annotations after image generation or generate images conditioned on manual annotation masks, which incurs image-annotation semantic inconsistency or scalability problem. To migrate both problems with one stone, we present a novel dataset generative diffusion framework for semantic segmentation, termed JoDiffusion. Firstly, given a standard latent diffusion model, JoDiffusion incorporates an independent annotation variational auto-encoder (VAE) network to map annotation masks into the latent space shared by images. Then, the diffusion model is tailored to capture the joint distribution of each image and its annotation mask conditioned on a text prompt. By doing these, JoDiffusion enables simultaneously generating paired images and semantically consistent annotation masks solely conditioned on text prompts, thereby demonstrating superior scalability. Additionally, a mask optimization strategy is developed to mitigate the annotation noise produced during generation. Experiments on Pascal VOC, COCO, and ADE20K datasets show that the annotated dataset generated by JoDiffusion yields substantial performance improvements in semantic segmentation compared to existing methods.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T06:21:30Z</published>
    <arxiv:comment>Accepted at AAAI 2026</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Haoyu Wang</name>
    </author>
    <author>
      <name>Lei Zhang</name>
    </author>
    <author>
      <name>Wenrui Liu</name>
    </author>
    <author>
      <name>Dengyang Jiang</name>
    </author>
    <author>
      <name>Wei Wei</name>
    </author>
    <author>
      <name>Chen Ding</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13010v1</id>
    <title>Deep Learning-Driven Inversion Framework for Shear Modulus Estimation in Magnetic Resonance Elastography (DIME)</title>
    <updated>2025-12-15T06:13:25Z</updated>
    <link href="https://arxiv.org/abs/2512.13010v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13010v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The Multimodal Direct Inversion (MMDI) algorithm is widely used in Magnetic Resonance Elastography (MRE) to estimate tissue shear stiffness. However, MMDI relies on the Helmholtz equation, which assumes wave propagation in a uniform, homogeneous, and infinite medium. Furthermore, the use of the Laplacian operator makes MMDI highly sensitive to noise, which compromises the accuracy and reliability of stiffness estimates. In this study, we propose the Deep-Learning driven Inversion Framework for Shear Modulus Estimation in MRE (DIME), aimed at enhancing the robustness of inversion. DIME is trained on the displacement fields-stiffness maps pair generated through Finite Element Modelling (FEM) simulations. To capture local wave behavior and improve robustness to global image variations, DIME is trained on small image patches. We first validated DIME using homogeneous and heterogeneous datasets simulated with FEM, where DIME produced stiffness maps with low inter-pixel variability, accurate boundary delineation, and higher correlation with ground truth (GT) compared to MMDI. Next, DIME was evaluated in a realistic anatomy-informed simulated liver dataset with known GT and compared directly to MMDI. DIME reproduced ground-truth stiffness patterns with high fidelity (r = 0.99, R^2 = 0.98), while MMDI showed greater underestimation. After validating DIME on synthetic data, we tested the model in in vivo liver MRE data from eight healthy and seven fibrotic subjects. DIME preserved physiologically consistent stiffness patterns and closely matched MMDI, which showed directional bias. Overall, DIME showed higher correlation with ground truth and visually similar stiffness patterns, whereas MMDI displayed a larger bias that can potentially be attributed to directional filtering. These preliminary results highlight the feasibility of DIME for clinical applications in MRE.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.TO" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T06:13:25Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Hassan Iftikhar</name>
    </author>
    <author>
      <name>Rizwan Ahmad</name>
    </author>
    <author>
      <name>Arunark Kolipaka</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13008v1</id>
    <title>TWLR: Text-Guided Weakly-Supervised Lesion Localization and Severity Regression for Explainable Diabetic Retinopathy Grading</title>
    <updated>2025-12-15T06:08:16Z</updated>
    <link href="https://arxiv.org/abs/2512.13008v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13008v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Accurate medical image analysis can greatly assist clinical diagnosis, but its effectiveness relies on high-quality expert annotations Obtaining pixel-level labels for medical images, particularly fundus images, remains costly and time-consuming. Meanwhile, despite the success of deep learning in medical imaging, the lack of interpretability limits its clinical adoption. To address these challenges, we propose TWLR, a two-stage framework for interpretable diabetic retinopathy (DR) assessment. In the first stage, a vision-language model integrates domain-specific ophthalmological knowledge into text embeddings to jointly perform DR grading and lesion classification, effectively linking semantic medical concepts with visual features. The second stage introduces an iterative severity regression framework based on weakly-supervised semantic segmentation. Lesion saliency maps generated through iterative refinement direct a progressive inpainting mechanism that systematically eliminates pathological features, effectively downgrading disease severity toward healthier fundus appearances. Critically, this severity regression approach achieves dual benefits: accurate lesion localization without pixel-level supervision and providing an interpretable visualization of disease-to-healthy transformations. Experimental results on the FGADR, DDR, and a private dataset demonstrate that TWLR achieves competitive performance in both DR classification and lesion segmentation, offering a more explainable and annotation-efficient solution for automated retinal image analysis.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T06:08:16Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Xi Luo</name>
    </author>
    <author>
      <name>Shixin Xu</name>
    </author>
    <author>
      <name>Ying Xie</name>
    </author>
    <author>
      <name>JianZhong Hu</name>
    </author>
    <author>
      <name>Yuwei He</name>
    </author>
    <author>
      <name>Yuhui Deng</name>
    </author>
    <author>
      <name>Huaxiong Huang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13007v1</id>
    <title>Light Field Based 6DoF Tracking of Previously Unobserved Objects</title>
    <updated>2025-12-15T06:04:49Z</updated>
    <link href="https://arxiv.org/abs/2512.13007v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13007v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Object tracking is an important step in robotics and reautonomous driving pipelines, which has to generalize to previously unseen and complex objects. Existing high-performing methods often rely on pre-captured object views to build explicit reference models, which restricts them to a fixed set of known objects. However, such reference models can struggle with visually complex appearance, reducing the quality of tracking. In this work, we introduce an object tracking method based on light field images that does not depend on a pre-trained model, while being robust to complex visual behavior, such as reflections. We extract semantic and geometric features from light field inputs using vision foundation models and convert them into view-dependent Gaussian splats. These splats serve as a unified object representation, supporting differentiable rendering and pose optimization. We further introduce a light field object tracking dataset containing challenging reflective objects with precise ground truth poses. Experiments demonstrate that our method is competitive with state-of-the-art model-based trackers in these difficult cases, paving the way toward universal object tracking in robotic systems. Code/data available at https://github.com/nagonch/LiFT-6DoF.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T06:04:49Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Nikolai Goncharov</name>
    </author>
    <author>
      <name>James L. Gray</name>
    </author>
    <author>
      <name>Donald G. Dansereau</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13006v1</id>
    <title>Few-Step Distillation for Text-to-Image Generation: A Practical Guide</title>
    <updated>2025-12-15T05:58:36Z</updated>
    <link href="https://arxiv.org/abs/2512.13006v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13006v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Diffusion distillation has dramatically accelerated class-conditional image synthesis, but its applicability to open-ended text-to-image (T2I) generation is still unclear. We present the first systematic study that adapts and compares state-of-the-art distillation techniques on a strong T2I teacher model, FLUX.1-lite. By casting existing methods into a unified framework, we identify the key obstacles that arise when moving from discrete class labels to free-form language prompts. Beyond a thorough methodological analysis, we offer practical guidelines on input scaling, network architecture, and hyperparameters, accompanied by an open-source implementation and pretrained student models. Our findings establish a solid foundation for deploying fast, high-fidelity, and resource-efficient diffusion generators in real-world T2I applications. Code is available on github.com/alibaba-damo-academy/T2I-Distill.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T05:58:36Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Yifan Pu</name>
    </author>
    <author>
      <name>Yizeng Han</name>
    </author>
    <author>
      <name>Zhiwei Tang</name>
    </author>
    <author>
      <name>Jiasheng Tang</name>
    </author>
    <author>
      <name>Fan Wang</name>
    </author>
    <author>
      <name>Bohan Zhuang</name>
    </author>
    <author>
      <name>Gao Huang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13003v1</id>
    <title>General OOD Detection via Model-aware and Subspace-aware Variable Priority</title>
    <updated>2025-12-15T05:55:35Z</updated>
    <link href="https://arxiv.org/abs/2512.13003v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13003v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Out-of-distribution (OOD) detection is essential for determining when a supervised model encounters inputs that differ meaningfully from its training distribution. While widely studied in classification, OOD detection for regression and survival analysis remains limited due to the absence of discrete labels and the challenge of quantifying predictive uncertainty. We introduce a framework for OOD detection that is simultaneously model aware and subspace aware, and that embeds variable prioritization directly into the detection step. The method uses the fitted predictor to construct localized neighborhoods around each test case that emphasize the features driving the model's learned relationship and downweight directions that are less relevant to prediction. It produces OOD scores without relying on global distance metrics or estimating the full feature density. The framework is applicable across outcome types, and in our implementation we use random forests, where the rule structure yields transparent neighborhoods and effective scoring. Experiments on synthetic and real data benchmarks designed to isolate functional shifts show consistent improvements over existing methods. We further demonstrate the approach in an esophageal cancer survival study, where distribution shifts related to lymphadenectomy identify patterns relevant to surgical guidelines.</summary>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T05:55:35Z</published>
    <arxiv:comment>29 pages, 11 figures</arxiv:comment>
    <arxiv:primary_category term="stat.ML"/>
    <author>
      <name>Min Lu</name>
    </author>
    <author>
      <name>Hemant Ishwaran</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12997v1</id>
    <title>Calibrating Uncertainty for Zero-Shot Adversarial CLIP</title>
    <updated>2025-12-15T05:41:08Z</updated>
    <link href="https://arxiv.org/abs/2512.12997v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12997v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>CLIP delivers strong zero-shot classification but remains highly vulnerable to adversarial attacks. Previous work of adversarial fine-tuning largely focuses on matching the predicted logits between clean and adversarial examples, which overlooks uncertainty calibration and may degrade the zero-shot generalization. A common expectation in reliable uncertainty estimation is that predictive uncertainty should increase as inputs become more difficult or shift away from the training distribution. However, we frequently observe the opposite in the adversarial setting: perturbations not only degrade accuracy but also suppress uncertainty, leading to severe miscalibration and unreliable over-confidence. This overlooked phenomenon highlights a critical reliability gap beyond robustness. To bridge this gap, we propose a novel adversarial fine-tuning objective for CLIP considering both prediction accuracy and uncertainty alignments. By reparameterizing the output of CLIP as the concentration parameter of a Dirichlet distribution, we propose a unified representation that captures relative semantic structure and the magnitude of predictive confidence. Our objective aligns these distributions holistically under perturbations, moving beyond single-logit anchoring and restoring calibrated uncertainty. Experiments on multiple zero-shot classification benchmarks demonstrate that our approach effectively restores calibrated uncertainty and achieves competitive adversarial robustness while maintaining clean accuracy.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T05:41:08Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Wenjing lu</name>
    </author>
    <author>
      <name>Zerui Tao</name>
    </author>
    <author>
      <name>Dongping Zhang</name>
    </author>
    <author>
      <name>Yuning Qiu</name>
    </author>
    <author>
      <name>Yang Yang</name>
    </author>
    <author>
      <name>Qibin Zhao</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12989v1</id>
    <title>Quantigence: A Multi-Agent AI Framework for Quantum Security Research</title>
    <updated>2025-12-15T05:27:10Z</updated>
    <link href="https://arxiv.org/abs/2512.12989v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12989v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Cryptographically Relevant Quantum Computers (CRQCs) pose a structural threat to the global digital economy. Algorithms like Shor's factoring and Grover's search threaten to dismantle the public-key infrastructure (PKI) securing sovereign communications and financial transactions. While the timeline for fault-tolerant CRQCs remains probabilistic, the "Store-Now, Decrypt-Later" (SNDL) model necessitates immediate migration to Post-Quantum Cryptography (PQC). This transition is hindered by the velocity of research, evolving NIST standards, and heterogeneous deployment environments. To address this, we present Quantigence, a theory-driven multi-agent AI framework for structured quantum-security analysis. Quantigence decomposes research objectives into specialized roles - Cryptographic Analyst, Threat Modeler, Standards Specialist, and Risk Assessor - coordinated by a supervisory agent. Using "cognitive parallelism," agents reason independently to maintain context purity while execution is serialized on resource-constrained hardware (e.g., NVIDIA RTX 2060). The framework integrates external knowledge via the Model Context Protocol (MCP) and prioritizes vulnerabilities using the Quantum-Adjusted Risk Score (QARS), a formal extension of Mosca's Theorem. Empirical validation shows Quantigence achieves a 67% reduction in research turnaround time and superior literature coverage compared to manual workflows, democratizing access to high-fidelity quantum risk assessment.</summary>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T05:27:10Z</published>
    <arxiv:comment>13 pages, 2 figures</arxiv:comment>
    <arxiv:primary_category term="cs.MA"/>
    <author>
      <name>Abdulmalik Alquwayfili</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12987v1</id>
    <title>Tackling Snow-Induced Challenges: Safe Autonomous Lane-Keeping with Robust Reinforcement Learning</title>
    <updated>2025-12-15T05:23:23Z</updated>
    <link href="https://arxiv.org/abs/2512.12987v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12987v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>This paper proposes two new algorithms for the lane keeping system (LKS) in autonomous vehicles (AVs) operating under snowy road conditions. These algorithms use deep reinforcement learning (DRL) to handle uncertainties and slippage. They include Action-Robust Recurrent Deep Deterministic Policy Gradient (AR-RDPG) and end-to-end Action-Robust convolutional neural network Attention Deterministic Policy Gradient (AR-CADPG), two action-robust approaches for decision-making. In the AR-RDPG method, within the perception layer, camera images are first denoised using multi-scale neural networks. Then, the centerline coefficients are extracted by a pre-trained deep convolutional neural network (DCNN). These coefficients, concatenated with the driving characteristics, are used as input to the control layer. The AR-CADPG method presents an end-to-end approach in which a convolutional neural network (CNN) and an attention mechanism are integrated within a DRL framework. Both methods are first trained in the CARLA simulator and validated under various snowy scenarios. Real-world experiments on a Jetson Nano-based autonomous vehicle confirm the feasibility and stability of the learned policies. Among the two models, the AR-CADPG approach demonstrates superior path-tracking accuracy and robustness, highlighting the effectiveness of combining temporal memory, adversarial resilience, and attention mechanisms in AVs.</summary>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T05:23:23Z</published>
    <arxiv:primary_category term="cs.RO"/>
    <author>
      <name>Amin Jalal Aghdasian</name>
    </author>
    <author>
      <name>Farzaneh Abdollahi</name>
    </author>
    <author>
      <name>Ali Kamali Iglie</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12984v1</id>
    <title>VoroLight: Learning Quality Volumetric Voronoi Meshes from General Inputs</title>
    <updated>2025-12-15T05:01:59Z</updated>
    <link href="https://arxiv.org/abs/2512.12984v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12984v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We present VoroLight, a differentiable framework for 3D shape reconstruction based on Voronoi meshing. Our approach generates smooth, watertight surfaces and topologically consistent volumetric meshes directly from diverse inputs, including images, implicit shape level-set fields, point clouds and meshes. VoroLight operates in three stages: it first initializes a surface using a differentiable Voronoi formulation, then refines surface quality through a polygon-face sphere training stage, and finally reuses the differentiable Voronoi formulation for volumetric optimization with additional interior generator points. Project page: https://jiayinlu19960224.github.io/vorolight/</summary>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T05:01:59Z</published>
    <arxiv:primary_category term="cs.CG"/>
    <author>
      <name>Jiayin Lu</name>
    </author>
    <author>
      <name>Ying Jiang</name>
    </author>
    <author>
      <name>Yin Yang</name>
    </author>
    <author>
      <name>Chenfanfu Jiang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12982v1</id>
    <title>Scaling Up AI-Generated Image Detection via Generator-Aware Prototypes</title>
    <updated>2025-12-15T04:58:08Z</updated>
    <link href="https://arxiv.org/abs/2512.12982v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12982v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The pursuit of a universal AI-generated image (AIGI) detector often relies on aggregating data from numerous generators to improve generalization. However, this paper identifies a paradoxical phenomenon we term the Benefit then Conflict dilemma, where detector performance stagnates and eventually degrades as source diversity expands. Our systematic analysis, diagnoses this failure by identifying two core issues: severe data-level heterogeneity, which causes the feature distributions of real and synthetic images to increasingly overlap, and a critical model-level bottleneck from fixed, pretrained encoders that cannot adapt to the rising complexity. To address these challenges, we propose Generator-Aware Prototype Learning (GAPL), a framework that constrain representation with a structured learning paradigm. GAPL learns a compact set of canonical forgery prototypes to create a unified, low-variance feature space, effectively countering data heterogeneity.To resolve the model bottleneck, it employs a two-stage training scheme with Low-Rank Adaptation, enhancing its discriminative power while preserving valuable pretrained knowledge. This approach establishes a more robust and generalizable decision boundary. Through extensive experiments, we demonstrate that GAPL achieves state-of-the-art performance, showing superior detection accuracy across a wide variety of GAN and diffusion-based generators. Code is available at https://github.com/UltraCapture/GAPL</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T04:58:08Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Ziheng Qin</name>
    </author>
    <author>
      <name>Yuheng Ji</name>
    </author>
    <author>
      <name>Renshuai Tao</name>
    </author>
    <author>
      <name>Yuxuan Tian</name>
    </author>
    <author>
      <name>Yuyang Liu</name>
    </author>
    <author>
      <name>Yipu Wang</name>
    </author>
    <author>
      <name>Xiaolong Zheng</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12981v1</id>
    <title>CoDeQ: End-to-End Joint Model Compression with Dead-Zone Quantizer for High-Sparsity and Low-Precision Networks</title>
    <updated>2025-12-15T04:53:32Z</updated>
    <link href="https://arxiv.org/abs/2512.12981v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12981v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>While joint pruning--quantization is theoretically superior to sequential application, current joint methods rely on auxiliary procedures outside the training loop for finding compression parameters. This reliance adds engineering complexity and hyperparameter tuning, while also lacking a direct data-driven gradient signal, which might result in sub-optimal compression. In this paper, we introduce CoDeQ, a simple, fully differentiable method for joint pruning--quantization. Our approach builds on a key observation: the dead-zone of a scalar quantizer is equivalent to magnitude pruning, and can be used to induce sparsity directly within the quantization operator. Concretely, we parameterize the dead-zone width and learn it via backpropagation, alongside the quantization parameters. This design provides explicit control of sparsity, regularized by a single global hyperparameter, while decoupling sparsity selection from bit-width selection. The result is a method for Compression with Dead-zone Quantizer (CoDeQ) that supports both fixed-precision and mixed-precision quantization (controlled by an optional second hyperparameter). It simultaneously determines the sparsity pattern and quantization parameters in a single end-to-end optimization. Consequently, CoDeQ does not require any auxiliary procedures, making the method architecture-agnostic and straightforward to implement. On ImageNet with ResNet-18, CoDeQ reduces bit operations to ~5% while maintaining close to full precision accuracy in both fixed and mixed-precision regimes.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T04:53:32Z</published>
    <arxiv:comment>Source code at https://github.com/saintslab/CoDeQ</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Jonathan Wenshøj</name>
    </author>
    <author>
      <name>Tong Chen</name>
    </author>
    <author>
      <name>Bob Pepin</name>
    </author>
    <author>
      <name>Raghavendra Selvan</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12977v1</id>
    <title>VLCache: Computing 2% Vision Tokens and Reusing 98% for Vision-Language Inference</title>
    <updated>2025-12-15T04:45:47Z</updated>
    <link href="https://arxiv.org/abs/2512.12977v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12977v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>This paper presents VLCache, a cache reuse framework that exploits both Key-Value (KV) cache and encoder cache from prior multimodal inputs to eliminate costly recomputation when the same multimodal inputs recur. Unlike previous heuristic approaches, we formally identify the cumulative reuse error effect and demonstrate how to minimize the non-prefix cache reuse error effectively. We further analyze the varying importance of model layers and propose a dynamic, layer-aware recomputation strategy to balance accuracy and efficiency. Experimental results show that VLCache achieves an accuracy on par with full recomputation, while requiring only 2-5% of the tokens to compute, yielding 1.2x-16x TTFT speedups. The proposed VLCache pipeline has been integrated into SGLang, enabling significantly faster inference in practical deployments.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T04:45:47Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Shengling Qin</name>
    </author>
    <author>
      <name>Hao Yu</name>
    </author>
    <author>
      <name>Chenxin Wu</name>
    </author>
    <author>
      <name>Zheng Li</name>
    </author>
    <author>
      <name>Yizhong Cao</name>
    </author>
    <author>
      <name>Zhengyang Zhuge</name>
    </author>
    <author>
      <name>Yuxin Zhou</name>
    </author>
    <author>
      <name>Wentao Yao</name>
    </author>
    <author>
      <name>Yi Zhang</name>
    </author>
    <author>
      <name>Zhengheng Wang</name>
    </author>
    <author>
      <name>Shuai Bai</name>
    </author>
    <author>
      <name>Jianwei Zhang</name>
    </author>
    <author>
      <name>Junyang Lin</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12976v1</id>
    <title>Authors Should Annotate</title>
    <updated>2025-12-15T04:45:09Z</updated>
    <link href="https://arxiv.org/abs/2512.12976v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12976v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The status quo for labeling text is third-party annotation, but there are many cases where information directly from the document's source would be preferable over a third-person proxy, especially for egocentric features like sentiment and belief. We introduce author labeling, an annotation technique where the writer of the document itself annotates the data at the moment of creation. We collaborate with a commercial chatbot with over 10,000 users to deploy an author labeling annotation system for subjective features related to product recommendation. This system identifies task-relevant queries, generates on-the-fly labeling questions, and records authors' answers in real time. We train and deploy an online-learning model architecture for product recommendation that continuously improves from author labeling and find it achieved a 534% increase in click-through rate compared to an industry advertising baseline running concurrently. We then compare the quality and practicality of author labeling to three traditional annotation approaches for sentiment analysis and find author labeling to be higher quality, faster to acquire, and cheaper. These findings reinforce existing literature that annotations, especially for egocentric and subjective beliefs, are significantly higher quality when labeled by the author rather than a third party. To facilitate broader scientific adoption, we release an author labeling service for the research community at academic.echollm.io.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T04:45:09Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Marcus Ma</name>
    </author>
    <author>
      <name>Cole Johnson</name>
    </author>
    <author>
      <name>Nolan Bridges</name>
    </author>
    <author>
      <name>Jackson Trager</name>
    </author>
    <author>
      <name>Georgios Chochlakis</name>
    </author>
    <author>
      <name>Shrikanth Narayanan</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12975v1</id>
    <title>Application of Deep Learning in Biological Data Compression</title>
    <updated>2025-12-15T04:40:23Z</updated>
    <link href="https://arxiv.org/abs/2512.12975v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12975v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Cryogenic electron microscopy (Cryo-EM) has become an essential tool for capturing high-resolution biological structures. Despite its advantage in visualizations, the large storage size of Cryo-EM data file poses significant challenges for researchers and educators. This paper investigates the application of deep learning, specifically implicit neural representation (INR), to compress Cryo-EM biological data. The proposed approach first extracts the binary map of each file according to the density threshold. The density map is highly repetitive, ehich can be effectively compressed by GZIP. The neural network then trains to encode spatial density information, allowing the storage of network parameters and learnable latent vectors. To improve reconstruction accuracy, I further incorporate the positional encoding to enhance spatial representation and a weighted Mean Squared Error (MSE) loss function to balance density distribution variations. Using this approach, my aim is to provide a practical and efficient biological data compression solution that can be used for educational and research purpose, while maintaining a reasonable compression ratio and reconstruction quality from file to file.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T04:40:23Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Chunyu Zou</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12970v1</id>
    <title>Towards Open Standards for Systemic Complexity in Digital Forensics</title>
    <updated>2025-12-15T04:18:56Z</updated>
    <link href="https://arxiv.org/abs/2512.12970v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12970v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The intersection of artificial intelligence (AI) and digital forensics (DF) is becoming increasingly complex, ubiquitous, and pervasive, with overlapping techniques and technologies being adopted in all types of scientific and technical inquiry. Despite incredible advances, forensic sciences are not exempt from errors and remain vulnerable to fallibility. To mitigate the limitations of errors in DF, the systemic complexity is identified and addressed with the adoption of human-readable artifacts and open standards. A DF AI model schema based on the state of the art is outlined.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T04:18:56Z</published>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Paola Di Maio</name>
    </author>
    <arxiv:doi>10.6084/m9.figshare.2965171</arxiv:doi>
    <link rel="related" href="https://doi.org/10.6084/m9.figshare.2965171" title="doi"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12967v1</id>
    <title>QwenLong-L1.5: Post-Training Recipe for Long-Context Reasoning and Memory Management</title>
    <updated>2025-12-15T04:11:11Z</updated>
    <link href="https://arxiv.org/abs/2512.12967v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12967v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We introduce QwenLong-L1.5, a model that achieves superior long-context reasoning capabilities through systematic post-training innovations. The key technical breakthroughs of QwenLong-L1.5 are as follows: (1) Long-Context Data Synthesis Pipeline: We develop a systematic synthesis framework that generates challenging reasoning tasks requiring multi-hop grounding over globally distributed evidence. By deconstructing documents into atomic facts and their underlying relationships, and then programmatically composing verifiable reasoning questions, our approach creates high-quality training data at scale, moving substantially beyond simple retrieval tasks to enable genuine long-range reasoning capabilities. (2) Stabilized Reinforcement Learning for Long-Context Training: To overcome the critical instability in long-context RL, we introduce task-balanced sampling with task-specific advantage estimation to mitigate reward bias, and propose Adaptive Entropy-Controlled Policy Optimization (AEPO) that dynamically regulates exploration-exploitation trade-offs. (3) Memory-Augmented Architecture for Ultra-Long Contexts: Recognizing that even extended context windows cannot accommodate arbitrarily long sequences, we develop a memory management framework with multi-stage fusion RL training that seamlessly integrates single-pass reasoning with iterative memory-based processing for tasks exceeding 4M tokens. Based on Qwen3-30B-A3B-Thinking, QwenLong-L1.5 achieves performance comparable to GPT-5 and Gemini-2.5-Pro on long-context reasoning benchmarks, surpassing its baseline by 9.90 points on average. On ultra-long tasks (1M~4M tokens), QwenLong-L1.5's memory-agent framework yields a 9.48-point gain over the agent baseline. Additionally, the acquired long-context reasoning ability translates to enhanced performance in general domains like scientific reasoning, memory tool using, and extended dialogue.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T04:11:11Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Weizhou Shen</name>
    </author>
    <author>
      <name>Ziyi Yang</name>
    </author>
    <author>
      <name>Chenliang Li</name>
    </author>
    <author>
      <name>Zhiyuan Lu</name>
    </author>
    <author>
      <name>Miao Peng</name>
    </author>
    <author>
      <name>Huashan Sun</name>
    </author>
    <author>
      <name>Yingcheng Shi</name>
    </author>
    <author>
      <name>Shengyi Liao</name>
    </author>
    <author>
      <name>Shaopeng Lai</name>
    </author>
    <author>
      <name>Bo Zhang</name>
    </author>
    <author>
      <name>Dayiheng Liu</name>
    </author>
    <author>
      <name>Fei Huang</name>
    </author>
    <author>
      <name>Jingren Zhou</name>
    </author>
    <author>
      <name>Ming Yan</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12965v1</id>
    <title>Challenges and Enablers: Remote Work for People with Disabilities in Software Development Teams</title>
    <updated>2025-12-15T04:05:36Z</updated>
    <link href="https://arxiv.org/abs/2512.12965v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12965v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The increasing adoption of remote and hybrid work modalities in the technology sector has brought new opportunities and challenges for the inclusion of people with disabilities (PWD) in software development teams (SDT). This study investigates how remote work affects PWDs' experience in mixed-ability SDT, focusing on the unique challenges and strategies that emerge in remote environments. We conducted an online survey with \totalSurveyResponses valid responses, encompassing PWD, their leaders, and teammates, to capture sociotechnical aspects of their experiences with remote collaboration. To deepen our understanding, we carried out 14 structured interviews with software developers who self-identified as having disabilities (six autistic individuals, six with physical disabilities, and two who are d/Deaf). Our analysis combines quantitative data with qualitative coding of open-ended survey responses and interview transcripts. The results reveal that, despite the barriers faced by team members with disabilities, their teammates and leaders have a limited perception of the daily challenges involved in sustaining collaborative remote work. These findings highlight opportunities for improvement in accessibility tools, communication strategies, and adaptive management approaches.</summary>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T04:05:36Z</published>
    <arxiv:comment>11 pages</arxiv:comment>
    <arxiv:primary_category term="cs.SE"/>
    <arxiv:journal_ref>ICSE SEIS 2026</arxiv:journal_ref>
    <author>
      <name>Thayssa Rocha</name>
    </author>
    <author>
      <name>Luciano Teran</name>
    </author>
    <author>
      <name>Marcelle Mota</name>
    </author>
    <author>
      <name>Cleidson de Souza</name>
    </author>
    <author>
      <name>Kiev Gama</name>
    </author>
    <author>
      <name>Gustavo Pinto</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12963v1</id>
    <title>SCAdapter: Content-Style Disentanglement for Diffusion Style Transfer</title>
    <updated>2025-12-15T04:02:14Z</updated>
    <link href="https://arxiv.org/abs/2512.12963v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12963v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Diffusion models have emerged as the leading approach for style transfer, yet they struggle with photo-realistic transfers, often producing painting-like results or missing detailed stylistic elements. Current methods inadequately address unwanted influence from original content styles and style reference content features. We introduce SCAdapter, a novel technique leveraging CLIP image space to effectively separate and integrate content and style features. Our key innovation systematically extracts pure content from content images and style elements from style references, ensuring authentic transfers. This approach is enhanced through three components: Controllable Style Adaptive Instance Normalization (CSAdaIN) for precise multi-style blending, KVS Injection for targeted style integration, and a style transfer consistency objective maintaining process coherence. Comprehensive experiments demonstrate SCAdapter significantly outperforms state-of-the-art methods in both conventional and diffusion-based baselines. By eliminating DDIM inversion and inference-stage optimization, our method achieves at least $2\times$ faster inference than other diffusion-based approaches, making it both more effective and efficient for practical applications.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T04:02:14Z</published>
    <arxiv:comment>Accepted to WACV 2026</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Luan Thanh Trinh</name>
    </author>
    <author>
      <name>Kenji Doi</name>
    </author>
    <author>
      <name>Atsuki Osanai</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12952v1</id>
    <title>Leveraging Compression to Construct Transferable Bitrate Ladders</title>
    <updated>2025-12-15T03:38:26Z</updated>
    <link href="https://arxiv.org/abs/2512.12952v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12952v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Over the past few years, per-title and per-shot video encoding techniques have demonstrated significant gains as compared to conventional techniques such as constant CRF encoding and the fixed bitrate ladder. These techniques have demonstrated that constructing content-gnostic per-shot bitrate ladders can provide significant bitrate gains and improved Quality of Experience (QoE) for viewers under various network conditions. However, constructing a convex hull for every video incurs a significant computational overhead. Recently, machine learning-based bitrate ladder construction techniques have emerged as a substitute for convex hull construction. These methods operate by extracting features from source videos to train machine learning (ML) models to construct content-adaptive bitrate ladders. Here, we present a new ML-based bitrate ladder construction technique that accurately predicts the VMAF scores of compressed videos, by analyzing the compression procedure and by making perceptually relevant measurements on the source videos prior to compression. We evaluate the performance of our proposed framework against leading prior methods on a large corpus of videos. Since training ML models on every encoder setting is time-consuming, we also investigate how per-shot bitrate ladders perform under different encoding settings. We evaluate the performance of all models against the fixed bitrate ladder and the best possible convex hull constructed using exhaustive encoding with Bjontegaard-delta metrics.</summary>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T03:38:26Z</published>
    <arxiv:comment>Under Review in IEEE Transactions on Image Processing</arxiv:comment>
    <arxiv:primary_category term="eess.IV"/>
    <author>
      <name>Krishna Srikar Durbha</name>
    </author>
    <author>
      <name>Hassene Tmar</name>
    </author>
    <author>
      <name>Ping-Hao Wu</name>
    </author>
    <author>
      <name>Ioannis Katsavounidis</name>
    </author>
    <author>
      <name>Alan C. Bovik</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12950v1</id>
    <title>Building from Scratch: A Multi-Agent Framework with Human-in-the-Loop for Multilingual Legal Terminology Mapping</title>
    <updated>2025-12-15T03:29:21Z</updated>
    <link href="https://arxiv.org/abs/2512.12950v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12950v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Accurately mapping legal terminology across languages remains a significant challenge, especially for language pairs like Chinese and Japanese, which share a large number of homographs with different meanings. Existing resources and standardized tools for these languages are limited. To address this, we propose a human-AI collaborative approach for building a multilingual legal terminology database, based on a multi-agent framework. This approach integrates advanced large language models and legal domain experts throughout the entire process-from raw document preprocessing, article-level alignment, to terminology extraction, mapping, and quality assurance. Unlike a single automated pipeline, our approach places greater emphasis on how human experts participate in this multi-agent system. Humans and AI agents take on different roles: AI agents handle specific, repetitive tasks, such as OCR, text segmentation, semantic alignment, and initial terminology extraction, while human experts provide crucial oversight, review, and supervise the outputs with contextual knowledge and legal judgment. We tested the effectiveness of this framework using a trilingual parallel corpus comprising 35 key Chinese statutes, along with their English and Japanese translations. The experimental results show that this human-in-the-loop, multi-agent workflow not only improves the precision and consistency of multilingual legal terminology mapping but also offers greater scalability compared to traditional manual methods.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T03:29:21Z</published>
    <arxiv:comment>43 pages, 6 fingures, accepted in Artificial Intelligence and Law (2025)</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Lingyi Meng</name>
    </author>
    <author>
      <name>Maolin Liu</name>
    </author>
    <author>
      <name>Hao Wang</name>
    </author>
    <author>
      <name>Yilan Cheng</name>
    </author>
    <author>
      <name>Qi Yang</name>
    </author>
    <author>
      <name>Idlkaid Mohanmmed</name>
    </author>
    <arxiv:doi>10.1007/s10506-025-09490-6</arxiv:doi>
    <link rel="related" href="https://doi.org/10.1007/s10506-025-09490-6" title="doi"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12949v1</id>
    <title>FlashFuser: Expanding the Scale of Kernel Fusion for Compute-Intensive Operators via Inter-Core Connection</title>
    <updated>2025-12-15T03:27:49Z</updated>
    <link href="https://arxiv.org/abs/2512.12949v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12949v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The scaling of computation throughput continues to outpace improvements in memory bandwidth, making many deep learning workloads memory-bound. Kernel fusion is a key technique to alleviate this problem, but the fusion strategies of existing compilers and frameworks are limited to using local scratchpad memory. When the intermediate results exceed the limited capacity (such as FFN), the fusion fails. Although modern GPUs (like the NVIDIA H100) now incorporate an inter-core connection mechanism known as Distributed Shared Memory(DSM)--providing a larger, high-bandwidth, and low-latency on-chip memory pool--this hardware potential has yet to be exploited by software frameworks. To bridge this gap, we present FlashFuser, the first compiler framework to utilize inter-core connection for kernel fusion on modern GPUs. FlashFuser extends established fusion techniques to the DSM domain through three core contributions. First, we propose a powerful DSM-based communication abstraction that formalizes complex cluster-based data exchange patterns, such as reduce, shuffle and multiply. Second, we introduce a dataflow analyzer that generalizes loop scheduling, resource mapping, and tile selection to the distributed memory hierarchy; it determines the optimal execution order and tile sizes by quantifying data movement across memory levels. Finally, FlashFuser integrates these components into a unified search engine that employs analytical cost modeling and DSM-aware pruning strategies to efficiently discover the optimal execution plan. Our evaluation on an NVIDIA H100 GPU shows that FlashFuser reduces memory access by 58% and delivers kernel speedups of 3.3x against highly-tuned libraries and 4.1x against state-of-the-art compilers, resulting in a 1.24x end-to-end speedup.</summary>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T03:27:49Z</published>
    <arxiv:primary_category term="cs.DC"/>
    <author>
      <name>Ziyu Huang</name>
    </author>
    <author>
      <name>Yangjie Zhou</name>
    </author>
    <author>
      <name>Zihan Liu</name>
    </author>
    <author>
      <name>Xinhao Luo</name>
    </author>
    <author>
      <name>Yijia Diao</name>
    </author>
    <author>
      <name>Minyi Guo</name>
    </author>
    <author>
      <name>Jidong Zhai</name>
    </author>
    <author>
      <name>Yu Feng</name>
    </author>
    <author>
      <name>Chen Zhang</name>
    </author>
    <author>
      <name>Anbang Wu</name>
    </author>
    <author>
      <name>Jingwen Leng</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12947v1</id>
    <title>Understanding When Graph Convolutional Networks Help: A Diagnostic Study on Label Scarcity and Structural Properties</title>
    <updated>2025-12-15T03:23:50Z</updated>
    <link href="https://arxiv.org/abs/2512.12947v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12947v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Graph Convolutional Networks (GCNs) have become a standard approach for semi-supervised node classification, yet practitioners lack clear guidance on when GCNs provide meaningful improvements over simpler baselines. We present a diagnostic study using the Amazon Computers co-purchase data to understand when and why GCNs help. Through systematic experiments with simulated label scarcity, feature ablation, and per-class analysis, we find that GCN performance depends critically on the interaction between graph homophily and feature quality. GCNs provide the largest gains under extreme label scarcity, where they leverage neighborhood structure to compensate for limited supervision. Surprisingly, GCNs can match their original performance even when node features are replaced with random noise, suggesting that structure alone carries sufficient signal on highly homophilous graphs. However, GCNs hurt performance when homophily is low and features are already strong, as noisy neighbors corrupt good predictions. Our quadrant analysis reveals that GCNs help in three of four conditions and only hurt when low homophily meets strong features. These findings offer practical guidance for practitioners deciding whether to adopt graph-based methods.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T03:23:50Z</published>
    <arxiv:comment>10 pages, 8 tables,5 figures</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Nischal Subedi</name>
    </author>
    <author>
      <name>Ember Kerstetter</name>
    </author>
    <author>
      <name>Winnie Li</name>
    </author>
    <author>
      <name>Silo Murphy</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12945v1</id>
    <title>SLIM-VDB: A Real-Time 3D Probabilistic Semantic Mapping Framework</title>
    <updated>2025-12-15T03:16:04Z</updated>
    <link href="https://arxiv.org/abs/2512.12945v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12945v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>This paper introduces SLIM-VDB, a new lightweight semantic mapping system with probabilistic semantic fusion for closed-set or open-set dictionaries. Advances in data structures from the computer graphics community, such as OpenVDB, have demonstrated significantly improved computational and memory efficiency in volumetric scene representation. Although OpenVDB has been used for geometric mapping in robotics applications, semantic mapping for scene understanding with OpenVDB remains unexplored. In addition, existing semantic mapping systems lack support for integrating both fixed-category and open-language label predictions within a single framework. In this paper, we propose a novel 3D semantic mapping system that leverages the OpenVDB data structure and integrates a unified Bayesian update framework for both closed- and open-set semantic fusion. Our proposed framework, SLIM-VDB, achieves significant reduction in both memory and integration times compared to current state-of-the-art semantic mapping approaches, while maintaining comparable mapping accuracy. An open-source C++ codebase with a Python interface is available at https://github.com/umfieldrobotics/slim-vdb.</summary>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T03:16:04Z</published>
    <arxiv:comment>Accepted into R-AL</arxiv:comment>
    <arxiv:primary_category term="cs.RO"/>
    <author>
      <name>Anja Sheppard</name>
    </author>
    <author>
      <name>Parker Ewen</name>
    </author>
    <author>
      <name>Joey Wilson</name>
    </author>
    <author>
      <name>Advaith V. Sethuraman</name>
    </author>
    <author>
      <name>Benard Adewole</name>
    </author>
    <author>
      <name>Anran Li</name>
    </author>
    <author>
      <name>Yuzhen Chen</name>
    </author>
    <author>
      <name>Ram Vasudevan</name>
    </author>
    <author>
      <name>Katherine A. Skinner</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12941v1</id>
    <title>UAGLNet: Uncertainty-Aggregated Global-Local Fusion Network with Cooperative CNN-Transformer for Building Extraction</title>
    <updated>2025-12-15T02:59:16Z</updated>
    <link href="https://arxiv.org/abs/2512.12941v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12941v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Building extraction from remote sensing images is a challenging task due to the complex structure variations of the buildings. Existing methods employ convolutional or self-attention blocks to capture the multi-scale features in the segmentation models, while the inherent gap of the feature pyramids and insufficient global-local feature integration leads to inaccurate, ambiguous extraction results. To address this issue, in this paper, we present an Uncertainty-Aggregated Global-Local Fusion Network (UAGLNet), which is capable to exploit high-quality global-local visual semantics under the guidance of uncertainty modeling. Specifically, we propose a novel cooperative encoder, which adopts hybrid CNN and transformer layers at different stages to capture the local and global visual semantics, respectively. An intermediate cooperative interaction block (CIB) is designed to narrow the gap between the local and global features when the network becomes deeper. Afterwards, we propose a Global-Local Fusion (GLF) module to complementarily fuse the global and local representations. Moreover, to mitigate the segmentation ambiguity in uncertain regions, we propose an Uncertainty-Aggregated Decoder (UAD) to explicitly estimate the pixel-wise uncertainty to enhance the segmentation accuracy. Extensive experiments demonstrate that our method achieves superior performance to other state-of-the-art methods. Our code is available at https://github.com/Dstate/UAGLNet</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T02:59:16Z</published>
    <arxiv:comment>IEEE TGRS</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Siyuan Yao</name>
    </author>
    <author>
      <name>Dongxiu Liu</name>
    </author>
    <author>
      <name>Taotao Li</name>
    </author>
    <author>
      <name>Shengjie Li</name>
    </author>
    <author>
      <name>Wenqi Ren</name>
    </author>
    <author>
      <name>Xiaochun Cao</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12939v1</id>
    <title>Continuous Edit Distance, Geodesics and Barycenters of Time-varying Persistence Diagrams</title>
    <updated>2025-12-15T02:57:21Z</updated>
    <link href="https://arxiv.org/abs/2512.12939v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12939v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We introduce the Continuous Edit Distance (CED), a geodesic and elastic distance for time-varying persistence diagrams (TVPDs). The CED extends edit-distance ideas to TVPDs by combining local substitution costs with penalized deletions/insertions, controlled by two parameters: \(α\) (trade-off between temporal misalignment and diagram discrepancy) and \(β\) (gap penalty). We also provide an explicit construction of CED-geodesics. Building on these ingredients, we present two practical barycenter solvers, one stochastic and one greedy, that monotonically decrease the CED Frechet energy. Empirically, the CED is robust to additive perturbations (both temporal and spatial), recovers temporal shifts, and supports temporal pattern search. On real-life datasets, the CED achieves clustering performance comparable or better than standard elastic dissimilarities, while our clustering based on CED-barycenters yields superior classification results. Overall, the CED equips TVPD analysis with a principled distance, interpretable geodesics, and practical barycenters, enabling alignment, comparison, averaging, and clustering directly in the space of TVPDs. A C++ implementation is provided for reproducibility at the following address https://github.com/sebastien-tchitchek/ContinuousEditDistance.</summary>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T02:57:21Z</published>
    <arxiv:comment>30 pages, 13 figures, 2 tables</arxiv:comment>
    <arxiv:primary_category term="cs.CG"/>
    <author>
      <name>Sebastien Tchitchek</name>
    </author>
    <author>
      <name>Mohamed Kissi</name>
    </author>
    <author>
      <name>Julien Tierny</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12936v1</id>
    <title>Content Adaptive based Motion Alignment Framework for Learned Video Compression</title>
    <updated>2025-12-15T02:51:47Z</updated>
    <link href="https://arxiv.org/abs/2512.12936v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12936v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Recent advances in end-to-end video compression have shown promising results owing to their unified end-to-end learning optimization. However, such generalized frameworks often lack content-specific adaptation, leading to suboptimal compression performance. To address this, this paper proposes a content adaptive based motion alignment framework that improves performance by adapting encoding strategies to diverse content characteristics. Specifically, we first introduce a two-stage flow-guided deformable warping mechanism that refines motion compensation with coarse-to-fine offset prediction and mask modulation, enabling precise feature alignment. Second, we propose a multi-reference quality aware strategy that adjusts distortion weights based on reference quality, and applies it to hierarchical training to reduce error propagation. Third, we integrate a training-free module that downsamples frames by motion magnitude and resolution to obtain smooth motion estimation. Experimental results on standard test datasets demonstrate that our framework CAMA achieves significant improvements over state-of-the-art Neural Video Compression models, achieving a 24.95% BD-rate (PSNR) savings over our baseline model DCVC-TCM, while also outperforming reproduced DCVC-DC and traditional codec HM-16.25.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T02:51:47Z</published>
    <arxiv:comment>Accepted to Data Compression Conference (DCC) 2026 as a poster paper</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Tiange Zhang</name>
    </author>
    <author>
      <name>Xiandong Meng</name>
    </author>
    <author>
      <name>Siwei Ma</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12935v1</id>
    <title>Unified Interactive Multimodal Moment Retrieval via Cascaded Embedding-Reranking and Temporal-Aware Score Fusion</title>
    <updated>2025-12-15T02:50:43Z</updated>
    <link href="https://arxiv.org/abs/2512.12935v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12935v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The exponential growth of video content has created an urgent need for efficient multimodal moment retrieval systems. However, existing approaches face three critical challenges: (1) fixed-weight fusion strategies fail across cross modal noise and ambiguous queries, (2) temporal modeling struggles to capture coherent event sequences while penalizing unrealistic gaps, and (3) systems require manual modality selection, reducing usability. We propose a unified multimodal moment retrieval system with three key innovations. First, a cascaded dual-embedding pipeline combines BEIT-3 and SigLIP for broad retrieval, refined by BLIP-2 based reranking to balance recall and precision. Second, a temporal-aware scoring mechanism applies exponential decay penalties to large temporal gaps via beam search, constructing coherent event sequences rather than isolated frames. Third, Agent-guided query decomposition (GPT-4o) automatically interprets ambiguous queries, decomposes them into modality specific sub-queries (visual/OCR/ASR), and performs adaptive score fusion eliminating manual modality selection. Qualitative analysis demonstrates that our system effectively handles ambiguous queries, retrieves temporally coherent sequences, and dynamically adapts fusion strategies, advancing interactive moment search capabilities.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T02:50:43Z</published>
    <arxiv:comment>Accepted at AAAI Workshop 2026</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Toan Le Ngo Thanh</name>
    </author>
    <author>
      <name>Phat Ha Huu</name>
    </author>
    <author>
      <name>Tan Nguyen Dang Duy</name>
    </author>
    <author>
      <name>Thong Nguyen Le Minh</name>
    </author>
    <author>
      <name>Anh Nguyen Nhu Tinh</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12932v1</id>
    <title>Investigating Data Pruning for Pretraining Biological Foundation Models at Scale</title>
    <updated>2025-12-15T02:42:52Z</updated>
    <link href="https://arxiv.org/abs/2512.12932v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12932v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Biological foundation models (BioFMs), pretrained on large-scale biological sequences, have recently shown strong potential in providing meaningful representations for diverse downstream bioinformatics tasks. However, such models often rely on millions to billions of training sequences and billions of parameters, resulting in prohibitive computational costs and significant barriers to reproducibility and accessibility, particularly for academic labs. To address these challenges, we investigate the feasibility of data pruning for BioFM pretraining and propose a post-hoc influence-guided data pruning framework tailored to biological domains. Our approach introduces a subset-based self-influence formulation that enables efficient estimation of sample importance at low computational cost, and builds upon it two simple yet effective selection strategies, namely Top-k Influence (Top I) and Coverage-Centric Influence (CCI). We empirically validate our method on two representative BioFMs, RNA-FM and ESM-C. For RNA, our framework consistently outperforms random selection baselines under an extreme pruning rate of over 99 percent, demonstrating its effectiveness. Furthermore, we show the generalizability of our framework on protein-related tasks using ESM-C. In particular, our coreset even outperforms random subsets that are ten times larger in both RNA and protein settings, revealing substantial redundancy in biological sequence datasets. These findings underscore the potential of influence-guided data pruning to substantially reduce the computational cost of BioFM pretraining, paving the way for more efficient, accessible, and sustainable biological AI research.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T02:42:52Z</published>
    <arxiv:comment>Accepted by AAAI 2026</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Yifan Wu</name>
    </author>
    <author>
      <name>Jiyue Jiang</name>
    </author>
    <author>
      <name>Xichen Ye</name>
    </author>
    <author>
      <name>Yiqi Wang</name>
    </author>
    <author>
      <name>Chang Zhou</name>
    </author>
    <author>
      <name>Yitao Xu</name>
    </author>
    <author>
      <name>Jiayang Chen</name>
    </author>
    <author>
      <name>He Hu</name>
    </author>
    <author>
      <name>Weizhong Zhang</name>
    </author>
    <author>
      <name>Cheng Jin</name>
    </author>
    <author>
      <name>Jiao Yuan</name>
    </author>
    <author>
      <name>Yu Li</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12930v1</id>
    <title>SeVeDo: A Heterogeneous Transformer Accelerator for Low-Bit Inference via Hierarchical Group Quantization and SVD-Guided Mixed Precision</title>
    <updated>2025-12-15T02:29:08Z</updated>
    <link href="https://arxiv.org/abs/2512.12930v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12930v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Low-bit quantization is a promising technique for efficient transformer inference by reducing computational and memory overhead. However, aggressive bitwidth reduction remains challenging due to activation outliers, leading to accuracy degradation. Existing methods, such as outlier-handling and group quantization, achieve high accuracy but incur substantial energy consumption. To address this, we propose SeVeDo, an energy-efficient SVD-based heterogeneous accelerator that structurally separates outlier-sensitive components into a high-precision low-rank path, while the remaining computations are executed in a low-bit residual datapath with group quantization. To further enhance efficiency, Hierarchical Group Quantization (HGQ) combines coarse-grained floating-point scaling with fine-grained shifting, effectively reducing dequantization cost. Also, SVD-guided mixed precision (SVD-MP) statically allocates higher bitwidths to precision-sensitive components identified through low-rank decomposition, thereby minimizing floating-point operation cost. Experimental results show that SeVeDo achieves a peak energy efficiency of 13.8TOPS/W, surpassing conventional designs, with 12.7TOPS/W on ViT-Base and 13.4TOPS/W on Llama2-7B benchmarks.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T02:29:08Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Yuseon Choi</name>
    </author>
    <author>
      <name>Sangjin Kim</name>
    </author>
    <author>
      <name>Jungjun Oh</name>
    </author>
    <author>
      <name>Byeongcheol Kim</name>
    </author>
    <author>
      <name>Hoi-Jun Yoo</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12929v1</id>
    <title>MADTempo: An Interactive System for Multi-Event Temporal Video Retrieval with Query Augmentation</title>
    <updated>2025-12-15T02:25:46Z</updated>
    <link href="https://arxiv.org/abs/2512.12929v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12929v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The rapid expansion of video content across online platforms has accelerated the need for retrieval systems capable of understanding not only isolated visual moments but also the temporal structure of complex events. Existing approaches often fall short in modeling temporal dependencies across multiple events and in handling queries that reference unseen or rare visual concepts. To address these challenges, we introduce MADTempo, a video retrieval framework developed by our team, AIO_Trinh, that unifies temporal search with web-scale visual grounding. Our temporal search mechanism captures event-level continuity by aggregating similarity scores across sequential video segments, enabling coherent retrieval of multi-event queries. Complementarily, a Google Image Search-based fallback module expands query representations with external web imagery, effectively bridging gaps in pretrained visual embeddings and improving robustness against out-of-distribution (OOD) queries. Together, these components advance the temporal reasoning and generalization capabilities of modern video retrieval systems, paving the way for more semantically aware and adaptive retrieval across large-scale video corpora.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T02:25:46Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Huu-An Vu</name>
    </author>
    <author>
      <name>Van-Khanh Mai</name>
    </author>
    <author>
      <name>Trong-Tam Nguyen</name>
    </author>
    <author>
      <name>Quang-Duc Dam</name>
    </author>
    <author>
      <name>Tien-Huy Nguyen</name>
    </author>
    <author>
      <name>Thanh-Huong Le</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12928v1</id>
    <title>PROSERVE: Unified Multi-Priority Request Scheduling for LLM Serving</title>
    <updated>2025-12-15T02:24:50Z</updated>
    <link href="https://arxiv.org/abs/2512.12928v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12928v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The widespread deployment of large language models (LLMs) for interactive applications necessitates serving systems that can handle thousands of concurrent requests with diverse Service Level Objective (SLO) requirements. A critical yet often overlooked dimension in this context is the inherent priority difference among clients; for instance, business-critical functions demand higher performance guarantees, as fulfilling such requests yields significantly greater business value. However, existing LLM serving schedulers fail to jointly optimize for both SLO attainment and client-level priorities.
  To bridge this gap, we first \textit{formalize multi-priority request scheduling as a service gain maximization problem}, where satisfying latency requirements for requests of different priorities contributes varying levels of gain. We then propose PROSERVE, a unified two-tier scheduling framework designed to maximize overall service gain. At the engine level, SlideBatching dynamically adapts batch formation and request ordering under varying load conditions, employing a sliding boundary mechanism to balance deadline-first and density-first strategies. At the service level, GoRouting performs gain-oriented and capability-aware dispatching across distributed instances, proactively reserving capacity for future high-priority or long requests. Extensive evaluation across four open-source datasets and a real-world industrial trace demonstrates that \systemname{} consistently outperforms state-of-the-art baselines, improving system gain by up to 35% and boosting SLO attainment by up to 52%.</summary>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T02:24:50Z</published>
    <arxiv:comment>15 pages</arxiv:comment>
    <arxiv:primary_category term="cs.DC"/>
    <author>
      <name>Weizhe Huang</name>
    </author>
    <author>
      <name>Tao Peng</name>
    </author>
    <author>
      <name>Tongxuan Liu</name>
    </author>
    <author>
      <name>Donghe Jin</name>
    </author>
    <author>
      <name>Xianzhe Dong</name>
    </author>
    <author>
      <name>Ke Zhang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12925v1</id>
    <title>Sharpness-aware Dynamic Anchor Selection for Generalized Category Discovery</title>
    <updated>2025-12-15T02:24:06Z</updated>
    <link href="https://arxiv.org/abs/2512.12925v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12925v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Generalized category discovery (GCD) is an important and challenging task in open-world learning. Specifically, given some labeled data of known classes, GCD aims to cluster unlabeled data that contain both known and unknown classes. Current GCD methods based on parametric classification adopt the DINO-like pseudo-labeling strategy, where the sharpened probability output of one view is used as supervision information for the other view. However, large pre-trained models have a preference for some specific visual patterns, resulting in encoding spurious correlation for unlabeled data and generating noisy pseudo-labels. To address this issue, we propose a novel method, which contains two modules: Loss Sharpness Penalty (LSP) and Dynamic Anchor Selection (DAS). LSP enhances the robustness of model parameters to small perturbations by minimizing the worst-case loss sharpness of the model, which suppressing the encoding of trivial features, thereby reducing overfitting of noise samples and improving the quality of pseudo-labels. Meanwhile, DAS selects representative samples for the unknown classes based on KNN density and class probability during the model training and assigns hard pseudo-labels to them, which not only alleviates the confidence difference between known and unknown classes but also enables the model to quickly learn more accurate feature distribution for the unknown classes, thus further improving the clustering accuracy. Extensive experiments demonstrate that the proposed method can effectively mitigate the noise of pseudo-labels, and achieve state-of-the-art results on multiple GCD benchmarks.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T02:24:06Z</published>
    <arxiv:comment>Accepted by TMM2025</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Zhimao Peng</name>
    </author>
    <author>
      <name>Enguang Wang</name>
    </author>
    <author>
      <name>Fei Yang</name>
    </author>
    <author>
      <name>Xialei Liu</name>
    </author>
    <author>
      <name>Ming-Ming Cheng</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12922v1</id>
    <title>LLM-based Personalized Portfolio Recommender: Integrating Large Language Models and Reinforcement Learning for Intelligent Investment Strategy Optimization</title>
    <updated>2025-12-15T02:12:53Z</updated>
    <link href="https://arxiv.org/abs/2512.12922v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12922v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>In modern financial markets, investors increasingly seek personalized and adaptive portfolio strategies that reflect their individual risk preferences and respond to dynamic market conditions. Traditional rule-based or static optimization approaches often fail to capture the nonlinear interactions among investor behavior, market volatility, and evolving financial objectives. To address these limitations, this paper introduces the LLM-based Personalized Portfolio Recommender , an integrated framework that combines Large Language Models, reinforcement learning, and individualized risk preference modeling to support intelligent investment decision-making.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T02:12:53Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Bangyu Li</name>
    </author>
    <author>
      <name>Boping Gu</name>
    </author>
    <author>
      <name>Ziyang Ding</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12921v1</id>
    <title>Cisco Integrated AI Security and Safety Framework Report</title>
    <updated>2025-12-15T02:12:12Z</updated>
    <link href="https://arxiv.org/abs/2512.12921v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12921v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Artificial intelligence (AI) systems are being readily and rapidly adopted, increasingly permeating critical domains: from consumer platforms and enterprise software to networked systems with embedded agents. While this has unlocked potential for human productivity gains, the attack surface has expanded accordingly: threats now span content safety failures (e.g., harmful or deceptive outputs), model and data integrity compromise (e.g., poisoning, supply-chain tampering), runtime manipulations (e.g., prompt injection, tool and agent misuse), and ecosystem risks (e.g., orchestration abuse, multi-agent collusion). Existing frameworks such as MITRE ATLAS, National Institute of Standards and Technology (NIST) AI 100-2 Adversarial Machine Learning (AML) taxonomy, and OWASP Top 10s for Large Language Models (LLMs) and Agentic AI Applications provide valuable viewpoints, but each covers only slices of this multi-dimensional space.
  This paper presents Cisco's Integrated AI Security and Safety Framework ("AI Security Framework"), a unified, lifecycle-aware taxonomy and operationalization framework that can be used to classify, integrate, and operationalize the full range of AI risks. It integrates AI security and AI safety across modalities, agents, pipelines, and the broader ecosystem. The AI Security Framework is designed to be practical for threat identification, red-teaming, risk prioritization, and it is comprehensive in scope and can be extensible to emerging deployments in multimodal contexts, humanoids, wearables, and sensory infrastructures. We analyze gaps in prevailing frameworks, discuss design principles for our framework, and demonstrate how the taxonomy provides structure for understanding how modern AI systems fail, how adversaries exploit these failures, and how organizations can build defenses across the AI lifecycle that evolve alongside capability advancements.</summary>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T02:12:12Z</published>
    <arxiv:primary_category term="cs.CR"/>
    <author>
      <name>Amy Chang</name>
    </author>
    <author>
      <name>Tiffany Saade</name>
    </author>
    <author>
      <name>Sanket Mendapara</name>
    </author>
    <author>
      <name>Adam Swanda</name>
    </author>
    <author>
      <name>Ankit Garg</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12918v1</id>
    <title>Satisfiability Modulo Theory Meets Inductive Logic Programming</title>
    <updated>2025-12-15T02:08:32Z</updated>
    <link href="https://arxiv.org/abs/2512.12918v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12918v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Inductive Logic Programming (ILP) provides interpretable rule learning in relational domains, yet remains limited in its ability to induce and reason with numerical constraints. Classical ILP systems operate over discrete predicates and typically rely on discretisation or hand-crafted numerical predicates, making it difficult to infer thresholds or arithmetic relations that must hold jointly across examples. Recent work has begun to address these limitations through tighter integrations of ILP with Satisfiability Modulo Theories (SMT) or specialised numerical inference mechanisms. In this paper we investigate a modular alternative that couples the ILP system PyGol with the SMT solver Z3. Candidate clauses proposed by PyGol are interpreted as quantifier-free formulas over background theories such as linear or nonlinear real arithmetic, allowing numerical parameters to be instantiated and verified by the SMT solver while preserving ILP's declarative relational bias. This supports the induction of hybrid rules that combine symbolic predicates with learned numerical constraints, including thresholds, intervals, and multi-literal arithmetic relations. We formalise this SMT-ILP setting and evaluate it on a suite of synthetic datasets designed to probe linear, relational, nonlinear, and multi-hop reasoning. The results illustrate how a modular SMT-ILP architecture can extend the expressivity of symbolic rule learning, complementing prior numerical ILP approaches while providing a flexible basis for future extensions toward richer theory-aware induction.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T02:08:32Z</published>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Nijesh Upreti</name>
    </author>
    <author>
      <name>Vaishak Belle</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12917v1</id>
    <title>Efficient Quantum-resistant Delegable Data Analysis Scheme with Revocation and Keyword Search in Mobile Cloud Computing</title>
    <updated>2025-12-15T02:07:52Z</updated>
    <link href="https://arxiv.org/abs/2512.12917v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12917v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>With the rapid growth of smart devices and mobile internet, large-scale data processing is becoming increasingly important, while mobile devices remain resource-constrained. Mobile Cloud Computing (MCC) addresses this limitation by offloading tasks to the cloud. Nevertheless, the widespread adoption of MCC also raises challenges such as data privacy, selective computation, efficient revocation, and keyword search. Additionally, the development of quantum computers also threatens data security in MCC. To address these challenges, we propose an efficient quantum-resistant delegable data analysis scheme with revocation and keyword search (EQDDA-RKS) for MCC. In the proposed scheme, an authorised mobile device can perform keyword searches and compute inner product values over encrypted data without disclosing any additional information. Meanwhile, if a user's function key is compromised, it can be revoked. To alleviate the burden on mobile devices, most of the computation which should be executed by the mobile device is outsourced to a cloud server, and the mobile device only needs to interact with a central authority once. Furthermore, an authorised mobile device can temporarily delegate its keyword search and function computation rights to a delegatee in case the device becomes unavailable due to power depletion, going offline, etc. Our scheme is formally proven secure in the standard model against quantum attacks, chosen plaintext attacks, chosen keyword attacks, and outside keyword guessing attacks. Furthermore, the analysis demonstrates that the number of interactions between a mobile device and the central authority is $O(1)$ in our scheme, rather than growing linearly with the number of functions, which is well-suited for MCC scenarios.</summary>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T02:07:52Z</published>
    <arxiv:primary_category term="cs.CR"/>
    <author>
      <name>Yue Han</name>
    </author>
    <author>
      <name>Jinguang Han</name>
    </author>
    <author>
      <name>Jianying Zhou</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12914v1</id>
    <title>CTIGuardian: A Few-Shot Framework for Mitigating Privacy Leakage in Fine-Tuned LLMs</title>
    <updated>2025-12-15T01:59:14Z</updated>
    <link href="https://arxiv.org/abs/2512.12914v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12914v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Large Language Models (LLMs) are often fine-tuned to adapt their general-purpose knowledge to specific tasks and domains such as cyber threat intelligence (CTI). Fine-tuning is mostly done through proprietary datasets that may contain sensitive information. Owners expect their fine-tuned model to not inadvertently leak this information to potentially adversarial end users. Using CTI as a use case, we demonstrate that data-extraction attacks can recover sensitive information from fine-tuned models on CTI reports, underscoring the need for mitigation. Retraining the full model to eliminate this leakage is computationally expensive and impractical. We propose an alternative approach, which we call privacy alignment, inspired by safety alignment in LLMs. Just like safety alignment teaches the model to abide by safety constraints through a few examples, we enforce privacy alignment through few-shot supervision, integrating a privacy classifier and a privacy redactor, both handled by the same underlying LLM. We evaluate our system, called CTIGuardian, using GPT-4o mini and Mistral-7B Instruct models, benchmarking against Presidio, a named entity recognition (NER) baseline. Results show that CTIGuardian provides a better privacy-utility trade-off than NER based models. While we demonstrate its effectiveness on a CTI use case, the framework is generic enough to be applicable to other sensitive domains.</summary>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T01:59:14Z</published>
    <arxiv:comment>Accepted at the 18th Cybersecurity Experimentation and Test Workshop (CSET), in conjunction with ACSAC 2025</arxiv:comment>
    <arxiv:primary_category term="cs.CR"/>
    <author>
      <name>Shashie Dilhara Batan Arachchige</name>
    </author>
    <author>
      <name>Benjamin Zi Hao Zhao</name>
    </author>
    <author>
      <name>Hassan Jameel Asghar</name>
    </author>
    <author>
      <name>Dinusha Vatsalan</name>
    </author>
    <author>
      <name>Dali Kaafar</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12911v1</id>
    <title>Evaluating Singular Value Thresholds for DNN Weight Matrices based on Random Matrix Theory</title>
    <updated>2025-12-15T01:49:20Z</updated>
    <link href="https://arxiv.org/abs/2512.12911v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12911v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>This study evaluates thresholds for removing singular values from singular value decomposition-based low-rank approximations of deep neural network weight matrices. Each weight matrix is modeled as the sum of signal and noise matrices. The low-rank approximation is obtained by removing noise-related singular values using a threshold based on random matrix theory. To assess the adequacy of this threshold, we propose an evaluation metric based on the cosine similarity between the singular vectors of the signal and original weight matrices. The proposed metric is used in numerical experiments to compare two threshold estimation methods.</summary>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T01:49:20Z</published>
    <arxiv:primary_category term="stat.ML"/>
    <author>
      <name>Kohei Nishikawa</name>
    </author>
    <author>
      <name>Koki Shimizu</name>
    </author>
    <author>
      <name>Hashiguchi Hiroki</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12907v1</id>
    <title>Machine Learning Architectures for the Estimation of Predicted Occupancy Grids in Road Traffic</title>
    <updated>2025-12-15T01:24:02Z</updated>
    <link href="https://arxiv.org/abs/2512.12907v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12907v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>This paper introduces a novel machine learning architecture for an efficient estimation of the probabilistic space-time representation of complex traffic scenarios. A detailed representation of the future traffic scenario is of significant importance for autonomous driving and for all active safety systems. In order to predict the future space-time representation of the traffic scenario, first the type of traffic scenario is identified and then the machine learning algorithm maps the current state of the scenario to possible future states. The input to the machine learning algorithms is the current state representation of a traffic scenario, termed as the Augmented Occupancy Grid (AOG). The output is the probabilistic space-time representation which includes uncertainties regarding the behaviour of the traffic participants and is termed as the Predicted Occupancy Grid (POG). The novel architecture consists of two Stacked Denoising Autoencoders (SDAs) and a set of Random Forests. It is then compared with the other two existing architectures that comprise of SDAs and DeconvNet. The architectures are validated with the help of simulations and the comparisons are made both in terms of accuracy and computational time. Also, a brief overview on the applications of POGs in the field of active safety is presented.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T01:24:02Z</published>
    <arxiv:comment>Journal of Advances in Information Technology</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Parthasarathy Nadarajan</name>
    </author>
    <author>
      <name>Michael Botsch</name>
    </author>
    <author>
      <name>Sebastian Sardina</name>
    </author>
    <arxiv:doi>10.12720/jait.9.1.1-9</arxiv:doi>
    <link rel="related" href="https://doi.org/10.12720/jait.9.1.1-9" title="doi"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12906v1</id>
    <title>Predictive Sample Assignment for Semantically Coherent Out-of-Distribution Detection</title>
    <updated>2025-12-15T01:18:38Z</updated>
    <link href="https://arxiv.org/abs/2512.12906v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12906v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Semantically coherent out-of-distribution detection (SCOOD) is a recently proposed realistic OOD detection setting: given labeled in-distribution (ID) data and mixed in-distribution and out-of-distribution unlabeled data as the training data, SCOOD aims to enable the trained model to accurately identify OOD samples in the testing data. Current SCOOD methods mainly adopt various clustering-based in-distribution sample filtering (IDF) strategies to select clean ID samples from unlabeled data, and take the remaining samples as auxiliary OOD data, which inevitably introduces a large number of noisy samples in training. To address the above issue, we propose a concise SCOOD framework based on predictive sample assignment (PSA). PSA includes a dual-threshold ternary sample assignment strategy based on the predictive energy score that can significantly improve the purity of the selected ID and OOD sample sets by assigning unconfident unlabeled data to an additional discard sample set, and a concept contrastive representation learning loss to further expand the distance between ID and OOD samples in the representation space to assist ID/OOD discrimination. In addition, we also introduce a retraining strategy to help the model fully fit the selected auxiliary ID/OOD samples. Experiments on two standard SCOOD benchmarks demonstrate that our approach outperforms the state-of-the-art methods by a significant margin.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T01:18:38Z</published>
    <arxiv:comment>Accepted by TCSVT2024</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Zhimao Peng</name>
    </author>
    <author>
      <name>Enguang Wang</name>
    </author>
    <author>
      <name>Xialei Liu</name>
    </author>
    <author>
      <name>Ming-Ming Cheng</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12905v1</id>
    <title>PAC-Bayes Bounds for Multivariate Linear Regression and Linear Autoencoders</title>
    <updated>2025-12-15T01:12:11Z</updated>
    <link href="https://arxiv.org/abs/2512.12905v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12905v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Linear Autoencoders (LAEs) have shown strong performance in state-of-the-art recommender systems. However, this success remains largely empirical, with limited theoretical understanding. In this paper, we investigate the generalizability -- a theoretical measure of model performance in statistical learning -- of multivariate linear regression and LAEs. We first propose a PAC-Bayes bound for multivariate linear regression, extending the earlier bound for single-output linear regression by Shalaeva et al., and establish sufficient conditions for its convergence. We then show that LAEs, when evaluated under a relaxed mean squared error, can be interpreted as constrained multivariate linear regression models on bounded data, to which our bound adapts. Furthermore, we develop theoretical methods to improve the computational efficiency of optimizing the LAE bound, enabling its practical evaluation on large models and real-world datasets. Experimental results demonstrate that our bound is tight and correlates well with practical ranking metrics such as Recall@K and NDCG@K.</summary>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T01:12:11Z</published>
    <arxiv:comment>Accepted at NeurIPS 2025 (https://openreview.net/forum?id=S1zkFSby8G)</arxiv:comment>
    <arxiv:primary_category term="stat.ML"/>
    <author>
      <name>Ruixin Guo</name>
    </author>
    <author>
      <name>Ruoming Jin</name>
    </author>
    <author>
      <name>Xinyu Li</name>
    </author>
    <author>
      <name>Yang Zhou</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12904v1</id>
    <title>OptHQC: Optimize HQC for High-Performance Post-Quantum Cryptography</title>
    <updated>2025-12-15T01:07:57Z</updated>
    <link href="https://arxiv.org/abs/2512.12904v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12904v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>As post-quantum cryptography (PQC) becomes increasingly critical for securing future communication systems, the performance overhead introduced by quantum-resistant algorithms presents a major computing challenge. HQC (Hamming Quasi-Cyclic) is a newly standardized code-based PQC scheme designed to replace classical key exchange methods. In this paper, we propose OptHQC, an optimized implementation of the HQC scheme to deliver high-performance cryptographic operations. Our approach provides a comprehensive analysis of each computational blocks in HQC and introduces optimizations across all three stages: key generation, encryption, and decryption. We first exploit data-level sparsity in vector multiplication to accelerate polynomial operations during vector generation. We then leverage instruction-level acceleration (e.g., AVX2) in hash computation to further improve performance. Last, we transform multiplication into lookup table indexing and optimize memory access patterns in syndrome computation and error vector recovery, which are the most computationally intensive operations in HQC. Overall, OptHQC achieves an average 55% speedup over the reference HQC implementation on CPU.</summary>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T01:07:57Z</published>
    <arxiv:primary_category term="cs.CR"/>
    <author>
      <name>Ben Dong</name>
    </author>
    <author>
      <name>Hui Feng</name>
    </author>
    <author>
      <name>Qian Wang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12903v1</id>
    <title>Next-generation reservoir computing validated by classification task</title>
    <updated>2025-12-15T01:06:23Z</updated>
    <link href="https://arxiv.org/abs/2512.12903v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12903v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>An emerging computing paradigm, so-called next-generation reservoir computing (NG-RC) is investigated. True to its namesake, NG-RC requires no actual reservoirs for input data mixing but rather computing the polynomial terms directly from the time series inputs. However, benchmark tests so far reported have been one-sided, limited to prediction tasks of temporal waveforms such as Lorenz 63 attractor and Mackey-Glass chaotic signal. We will demonstrate for the first time that NG-RC can perform classification task as good as conventional RC. This validates the versatile computational capability of NG-RC in tasks of both prediction and classification.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T01:06:23Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Ken-ichi Kitayama</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12901v1</id>
    <title>Predicted-occupancy grids for vehicle safety applications based on autoencoders and the Random Forest algorithm</title>
    <updated>2025-12-15T00:59:44Z</updated>
    <link href="https://arxiv.org/abs/2512.12901v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12901v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>In this paper, a probabilistic space-time representation of complex traffic scenarios is predicted using machine learning algorithms. Such a representation is significant for all active vehicle safety applications especially when performing dynamic maneuvers in a complex traffic scenario. As a first step, a hierarchical situation classifier is used to distinguish the different types of traffic scenarios. This classifier is responsible for identifying the type of the road infrastructure and the safety-relevant traffic participants of the driving environment. With each class representing similar traffic scenarios, a set of Random Forests (RFs) is individually trained to predict the probabilistic space-time representation, which depicts the future behavior of traffic participants. This representation is termed as a Predicted-Occupancy Grid (POG). The input to the RFs is an Augmented Occupancy Grid (AOG). In order to increase the learning accuracy of the RFs and to perform better predictions, the AOG is reduced to low-dimensional features using a Stacked Denoising Autoencoder (SDA). The excellent performance of the proposed machine learning approach consisting of SDAs and RFs is demonstrated in simulations and in experiments with real vehicles. An application of POGs to estimate the criticality of traffic scenarios and to determine safe trajectories is also presented.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T00:59:44Z</published>
    <arxiv:comment>2017 International Joint Conference on Neural Networks (IJCNN)</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Parthasarathy Nadarajan</name>
    </author>
    <author>
      <name>Michael Botsch</name>
    </author>
    <author>
      <name>Sebastian Sardina</name>
    </author>
    <arxiv:doi>10.1109/IJCNN.2017.7965995</arxiv:doi>
    <link rel="related" href="https://doi.org/10.1109/IJCNN.2017.7965995" title="doi"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12898v1</id>
    <title>Qonvolution: Towards Learning High-Frequency Signals with Queried Convolution</title>
    <updated>2025-12-15T00:46:09Z</updated>
    <link href="https://arxiv.org/abs/2512.12898v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12898v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Accurately learning high-frequency signals is a challenge in computer vision and graphics, as neural networks often struggle with these signals due to spectral bias or optimization difficulties. While current techniques like Fourier encodings have made great strides in improving performance, there remains scope for improvement when presented with high-frequency information. This paper introduces Queried-Convolutions (Qonvolutions), a simple yet powerful modification using the neighborhood properties of convolution. Qonvolution convolves a low-frequency signal with queries (such as coordinates) to enhance the learning of intricate high-frequency signals. We empirically demonstrate that Qonvolutions enhance performance across a variety of high-frequency learning tasks crucial to both the computer vision and graphics communities, including 1D regression, 2D super-resolution, 2D image regression, and novel view synthesis (NVS). In particular, by combining Gaussian splatting with Qonvolutions for NVS, we showcase state-of-the-art performance on real-world complex scenes, even outperforming powerful radiance field models on image quality.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T00:46:09Z</published>
    <arxiv:comment>28 pages, 8 figures, Project Page: https://abhi1kumar.github.io/qonvolution/</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Abhinav Kumar</name>
    </author>
    <author>
      <name>Tristan Aumentado-Armstrong</name>
    </author>
    <author>
      <name>Lazar Valkov</name>
    </author>
    <author>
      <name>Gopal Sharma</name>
    </author>
    <author>
      <name>Alex Levinshtein</name>
    </author>
    <author>
      <name>Radek Grzeszczuk</name>
    </author>
    <author>
      <name>Suren Kumar</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12896v1</id>
    <title>Probability Estimation for Predicted-Occupancy Grids in Vehicle Safety Applications Based on Machine Learning</title>
    <updated>2025-12-15T00:45:26Z</updated>
    <link href="https://arxiv.org/abs/2512.12896v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12896v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>This paper presents a method to predict the evolution of a complex traffic scenario with multiple objects. The current state of the scenario is assumed to be known from sensors and the prediction is taking into account various hypotheses about the behavior of traffic participants. This way, the uncertainties regarding the behavior of traffic participants can be modelled in detail. In the first part of this paper a model-based approach is presented to compute Predicted-Occupancy Grids (POG), which are introduced as a grid-based probabilistic representation of the future scenario hypotheses. However, due to the large number of possible trajectories for each traffic participant, the model-based approach comes with a very high computational load. Thus, a machine-learning approach is adopted for the computation of POGs. This work uses a novel grid-based representation of the current state of the traffic scenario and performs the mapping to POGs. This representation consists of augmented cells in an occupancy grid. The adopted machine-learning approach is based on the Random Forest algorithm. Simulations of traffic scenarios are performed to compare the machine-learning with the model-based approach. The results are promising and could enable the real-time computation of POGs for vehicle safety applications. With this detailed modelling of uncertainties, crucial components in vehicle safety systems like criticality estimation and trajectory planning can be improved.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T00:45:26Z</published>
    <arxiv:comment>2016 IEEE Intelligent Vehicles Symposium</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Parthasarathy Nadarajan</name>
    </author>
    <author>
      <name>Michael Botsch</name>
    </author>
    <arxiv:doi>10.1109/IVS.2016.7535556</arxiv:doi>
    <link rel="related" href="https://doi.org/10.1109/IVS.2016.7535556" title="doi"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12895v1</id>
    <title>Wait, Wait, Wait... Why Do Reasoning Models Loop?</title>
    <updated>2025-12-15T00:44:54Z</updated>
    <link href="https://arxiv.org/abs/2512.12895v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12895v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Reasoning models (e.g., DeepSeek-R1) generate long chains of thought to solve harder problems, but they often loop, repeating the same text at low temperatures or with greedy decoding. We study why this happens and what role temperature plays. With open reasoning models, we find that looping is common at low temperature. Larger models tend to loop less, and distilled students loop significantly even when their teachers rarely do. This points to mismatches between the training distribution and the learned model, which we refer to as errors in learning, as a key cause. To understand how such errors cause loops, we introduce a synthetic graph reasoning task and demonstrate two mechanisms. First, risk aversion caused by hardness of learning: when the correct progress-making action is hard to learn but an easy cyclic action is available, the model puts relatively more probability on the cyclic action and gets stuck. Second, even when there is no hardness, Transformers show an inductive bias toward temporally correlated errors, so the same few actions keep being chosen and loops appear. Higher temperature reduces looping by promoting exploration, but it does not fix the errors in learning, so generations remain much longer than necessary at high temperature; in this sense, temperature is a stopgap rather than a holistic solution. We end with a discussion of training-time interventions aimed at directly reducing errors in learning.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T00:44:54Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Charilaos Pipis</name>
    </author>
    <author>
      <name>Shivam Garg</name>
    </author>
    <author>
      <name>Vasilis Kontonis</name>
    </author>
    <author>
      <name>Vaishnavi Shrivastava</name>
    </author>
    <author>
      <name>Akshay Krishnamurthy</name>
    </author>
    <author>
      <name>Dimitris Papailiopoulos</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12889v1</id>
    <title>Distillation of Discrete Diffusion by Exact Conditional Distribution Matching</title>
    <updated>2025-12-15T00:16:10Z</updated>
    <link href="https://arxiv.org/abs/2512.12889v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12889v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Discrete diffusion models (DDMs) are a powerful class of generative models for categorical data, but they typically require many function evaluations for a single sample, making inference expensive. Existing acceleration methods either rely on approximate simulators, such as $τ$-leaping, or on distillation schemes that train new student models and auxiliary networks with proxy objectives. We propose a simple and principled distillation alternative based on \emph{conditional distribution matching}. Our key observation is that the reverse conditional distribution of clean data given a noisy state, $p_{0\mid t}(x_0 \mid x_t)$, admits a Markov decomposition through intermediate times and can be recovered from marginal density ratios and the known forward CTMC kernel. We exploit this structure to define distillation objectives that directly match conditional distributions between a pre-trained teacher and a low-NFE student, both for one-step and few-step samplers.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T00:16:10Z</published>
    <arxiv:comment>[work in progress]</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Yansong Gao</name>
    </author>
    <author>
      <name>Yu Sun</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12888v1</id>
    <title>Meta-GPT: Decoding the Metasurface Genome with Generative Artificial Intelligence</title>
    <updated>2025-12-15T00:09:14Z</updated>
    <link href="https://arxiv.org/abs/2512.12888v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12888v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Advancing artificial intelligence for physical sciences requires representations that are both interpretable and compatible with the underlying laws of nature. We introduce METASTRINGS, a symbolic language for photonics that expresses nanostructures as textual sequences encoding materials, geometries, and lattice configurations. Analogous to molecular textual representations in chemistry, METASTRINGS provides a framework connecting human interpretability with computational design by capturing the structural hierarchy of photonic metasurfaces. Building on this representation, we develop Meta-GPT, a foundation transformer model trained on METASTRINGS and finetuned with physics-informed supervised, reinforcement, and chain-of-thought learning. Across various design tasks, the model achieves &lt;3% mean-squared spectral error and maintains &gt;98% syntactic validity, generating diverse metasurface prototypes whose experimentally measured optical responses match their target spectra. These results demonstrate that Meta-GPT can learn the compositional rules of light-matter interactions through METASTRINGS, laying a rigorous foundation for AI-driven photonics and representing an important step toward a metasurface genome project.</summary>
    <category term="physics.optics" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T00:09:14Z</published>
    <arxiv:comment>Keywords: Physics-informed machine learning; Transformer models; Reinforcement learning; Chain-of-thought reasoning; Metasurfaces; Nanophotonics; Inverse design</arxiv:comment>
    <arxiv:primary_category term="physics.optics"/>
    <author>
      <name>David Dang</name>
      <arxiv:affiliation>Howard</arxiv:affiliation>
    </author>
    <author>
      <name>Stuart Love</name>
      <arxiv:affiliation>Howard</arxiv:affiliation>
    </author>
    <author>
      <name>Meena Salib</name>
      <arxiv:affiliation>Howard</arxiv:affiliation>
    </author>
    <author>
      <name>Quynh Dang</name>
      <arxiv:affiliation>Howard</arxiv:affiliation>
    </author>
    <author>
      <name>Samuel Rothfarb</name>
      <arxiv:affiliation>Howard</arxiv:affiliation>
    </author>
    <author>
      <name>Mysk Alnatour</name>
      <arxiv:affiliation>Howard</arxiv:affiliation>
    </author>
    <author>
      <name>Andrew Salij</name>
      <arxiv:affiliation>Howard</arxiv:affiliation>
    </author>
    <author>
      <name>Hou-Tong Chen</name>
      <arxiv:affiliation>Howard</arxiv:affiliation>
    </author>
    <author>
      <name>Ho Wai</name>
      <arxiv:affiliation>Howard</arxiv:affiliation>
    </author>
    <author>
      <name> Lee</name>
    </author>
    <author>
      <name>Wilton J. M. Kort-Kamp</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12887v1</id>
    <title>Revisiting 2D Foundation Models for Scalable 3D Medical Image Classification</title>
    <updated>2025-12-15T00:01:19Z</updated>
    <link href="https://arxiv.org/abs/2512.12887v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12887v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>3D medical image classification is essential for modern clinical workflows. Medical foundation models (FMs) have emerged as a promising approach for scaling to new tasks, yet current research suffers from three critical pitfalls: data-regime bias, suboptimal adaptation, and insufficient task coverage. In this paper, we address these pitfalls and introduce AnyMC3D, a scalable 3D classifier adapted from 2D FMs. Our method scales efficiently to new tasks by adding only lightweight plugins (about 1M parameters per task) on top of a single frozen backbone. This versatile framework also supports multi-view inputs, auxiliary pixel-level supervision, and interpretable heatmap generation. We establish a comprehensive benchmark of 12 tasks covering diverse pathologies, anatomies, and modalities, and systematically analyze state-of-the-art 3D classification techniques. Our analysis reveals key insights: (1) effective adaptation is essential to unlock FM potential, (2) general-purpose FMs can match medical-specific FMs if properly adapted, and (3) 2D-based methods surpass 3D architectures for 3D classification. For the first time, we demonstrate the feasibility of achieving state-of-the-art performance across diverse applications using a single scalable framework (including 1st place in the VLM3D challenge), eliminating the need for separate task-specific models.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T00:01:19Z</published>
    <arxiv:comment>1st Place in VLM3D Challenge</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Han Liu</name>
    </author>
    <author>
      <name>Bogdan Georgescu</name>
    </author>
    <author>
      <name>Yanbo Zhang</name>
    </author>
    <author>
      <name>Youngjin Yoo</name>
    </author>
    <author>
      <name>Michael Baumgartner</name>
    </author>
    <author>
      <name>Riqiang Gao</name>
    </author>
    <author>
      <name>Jianing Wang</name>
    </author>
    <author>
      <name>Gengyan Zhao</name>
    </author>
    <author>
      <name>Eli Gibson</name>
    </author>
    <author>
      <name>Dorin Comaniciu</name>
    </author>
    <author>
      <name>Sasa Grbic</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12885v1</id>
    <title>SignRAG: A Retrieval-Augmented System for Scalable Zero-Shot Road Sign Recognition</title>
    <updated>2025-12-14T23:56:34Z</updated>
    <link href="https://arxiv.org/abs/2512.12885v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12885v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Automated road sign recognition is a critical task for intelligent transportation systems, but traditional deep learning methods struggle with the sheer number of sign classes and the impracticality of creating exhaustive labeled datasets. This paper introduces a novel zero-shot recognition framework that adapts the Retrieval-Augmented Generation (RAG) paradigm to address this challenge. Our method first uses a Vision Language Model (VLM) to generate a textual description of a sign from an input image. This description is used to retrieve a small set of the most relevant sign candidates from a vector database of reference designs. Subsequently, a Large Language Model (LLM) reasons over the retrieved candidates to make a final, fine-grained recognition. We validate this approach on a comprehensive set of 303 regulatory signs from the Ohio MUTCD. Experimental results demonstrate the framework's effectiveness, achieving 95.58% accuracy on ideal reference images and 82.45% on challenging real-world road data. This work demonstrates the viability of RAG-based architectures for creating scalable and accurate systems for road sign recognition without task-specific training.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T23:56:34Z</published>
    <arxiv:comment>Submitted to IV 2026</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Minghao Zhu</name>
    </author>
    <author>
      <name>Zhihao Zhang</name>
    </author>
    <author>
      <name>Anmol Sidhu</name>
    </author>
    <author>
      <name>Keith Redmill</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12884v1</id>
    <title>Cross-Level Sensor Fusion with Object Lists via Transformer for 3D Object Detection</title>
    <updated>2025-12-14T23:56:16Z</updated>
    <link href="https://arxiv.org/abs/2512.12884v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12884v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>In automotive sensor fusion systems, smart sensors and Vehicle-to-Everything (V2X) modules are commonly utilized. Sensor data from these systems are typically available only as processed object lists rather than raw sensor data from traditional sensors. Instead of processing other raw data separately and then fusing them at the object level, we propose an end-to-end cross-level fusion concept with Transformer, which integrates highly abstract object list information with raw camera images for 3D object detection. Object lists are fed into a Transformer as denoising queries and propagated together with learnable queries through the latter feature aggregation process. Additionally, a deformable Gaussian mask, derived from the positional and size dimensional priors from the object lists, is explicitly integrated into the Transformer decoder. This directs attention toward the target area of interest and accelerates model training convergence. Furthermore, as there is no public dataset containing object lists as a standalone modality, we propose an approach to generate pseudo object lists from ground-truth bounding boxes by simulating state noise and false positives and negatives. As the first work to conduct cross-level fusion, our approach shows substantial performance improvements over the vision-based baseline on the nuScenes dataset. It demonstrates its generalization capability over diverse noise levels of simulated object lists and real detectors.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T23:56:16Z</published>
    <arxiv:comment>6 pages, 3 figures, accepted at IV2025</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <arxiv:journal_ref>2025 IEEE Intelligent Vehicles Symposium (IV)</arxiv:journal_ref>
    <author>
      <name>Xiangzhong Liu</name>
    </author>
    <author>
      <name>Jiajie Zhang</name>
    </author>
    <author>
      <name>Hao Shen</name>
    </author>
    <arxiv:doi>10.1109/IV64158.2025.11097627</arxiv:doi>
    <link rel="related" href="https://doi.org/10.1109/IV64158.2025.11097627" title="doi"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12881v1</id>
    <title>Unsupervised learning of multiscale switching dynamical system models from multimodal neural data</title>
    <updated>2025-12-14T23:49:12Z</updated>
    <link href="https://arxiv.org/abs/2512.12881v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12881v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Neural population activity often exhibits regime-dependent non-stationarity in the form of switching dynamics. Learning accurate switching dynamical system models can reveal how behavior is encoded in neural activity. Existing switching approaches have primarily focused on learning models from a single neural modality, either continuous Gaussian signals or discrete Poisson signals. However, multiple neural modalities are often recorded simultaneously to measure different spatiotemporal scales of brain activity, and all these modalities can encode behavior. Moreover, regime labels are typically unavailable in training data, posing a significant challenge for learning models of regime-dependent switching dynamics. To address these challenges, we develop a novel unsupervised learning algorithm that learns the parameters of switching multiscale dynamical system models using only multiscale neural observations. We demonstrate our method using both simulations and two distinct experimental datasets with multimodal spike-LFP observations during different motor tasks. We find that our switching multiscale dynamical system models more accurately decode behavior than switching single-scale dynamical models, showing the success of multiscale neural fusion. Further, our models outperform stationary multiscale models, illustrating the importance of tracking regime-dependent non-stationarity in multimodal neural data. The developed unsupervised learning framework enables more accurate modeling of complex multiscale neural dynamics by leveraging information in multimodal recordings while incorporating regime switches. This approach holds promise for improving the performance and robustness of brain-computer interfaces over time and for advancing our understanding of the neural basis of behavior.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T23:49:12Z</published>
    <arxiv:comment>30 pages, 8 figures</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>DongKyu Kim</name>
    </author>
    <author>
      <name>Han-Lin Hsieh</name>
    </author>
    <author>
      <name>Maryam M. Shanechi</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12880v1</id>
    <title>Improving Recursive Transformers with Mixture of LoRAs</title>
    <updated>2025-12-14T23:39:30Z</updated>
    <link href="https://arxiv.org/abs/2512.12880v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12880v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Parameter sharing in recursive transformers reduces model size but collapses layer-wise expressivity. We propose Mixture of LoRAs (MoL), a lightweight conditional-computation mechanism that inserts Low-Rank Adaptation (LoRA) experts inside a shared feed-forward network (FFN). MoL enables token-conditional weight-space modulation of the shared FFN without untying backbone parameters, unlike prior approaches that add fixed or externally attached adapters. We pretrain a modernised recursive architecture, ModernALBERT, integrating rotary embeddings, GeGLU, FlashAttention, and a distillation-based initialisation. Across GLUE, SQuAD-v2, and BEIR, ModernALBERT (50M--120M) achieves state-of-the-art performance among compact models and surpasses larger fully parameterised baselines. We also propose an expert-merging procedure that compresses MoL into a single adapter at inference while preserving accuracy, enabling efficient deployment. Our results show that conditional weight-space modulation effectively restores the expressivity lost under aggressive parameter sharing in recursive transformers.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T23:39:30Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Mohammadmahdi Nouriborji</name>
    </author>
    <author>
      <name>Morteza Rohanian</name>
    </author>
    <author>
      <name>Omid Rohanian</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12875v1</id>
    <title>Schrodinger Audio-Visual Editor: Object-Level Audiovisual Removal</title>
    <updated>2025-12-14T23:19:15Z</updated>
    <link href="https://arxiv.org/abs/2512.12875v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12875v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Joint editing of audio and visual content is crucial for precise and controllable content creation. This new task poses challenges due to the limitations of paired audio-visual data before and after targeted edits, and the heterogeneity across modalities. To address the data and modeling challenges in joint audio-visual editing, we introduce SAVEBench, a paired audiovisual dataset with text and mask conditions to enable object-grounded source-to-target learning. With SAVEBench, we train the Schrodinger Audio-Visual Editor (SAVE), an end-to-end flow-matching model that edits audio and video in parallel while keeping them aligned throughout processing. SAVE incorporates a Schrodinger Bridge that learns a direct transport from source to target audiovisual mixtures. Our evaluation demonstrates that the proposed SAVE model is able to remove the target objects in audio and visual content while preserving the remaining content, with stronger temporal synchronization and audiovisual semantic correspondence compared with pairwise combinations of an audio editor and a video editor.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T23:19:15Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Weihan Xu</name>
    </author>
    <author>
      <name>Kan Jen Cheng</name>
    </author>
    <author>
      <name>Koichi Saito</name>
    </author>
    <author>
      <name>Muhammad Jehanzeb Mirza</name>
    </author>
    <author>
      <name>Tingle Li</name>
    </author>
    <author>
      <name>Yisi Liu</name>
    </author>
    <author>
      <name>Alexander H. Liu</name>
    </author>
    <author>
      <name>Liming Wang</name>
    </author>
    <author>
      <name>Masato Ishii</name>
    </author>
    <author>
      <name>Takashi Shibuya</name>
    </author>
    <author>
      <name>Yuki Mitsufuji</name>
    </author>
    <author>
      <name>Gopala Anumanchipalli</name>
    </author>
    <author>
      <name>Paul Pu Liang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12870v1</id>
    <title>Optimal Labeler Assignment and Sampling for Active Learning in the Presence of Imperfect Labels</title>
    <updated>2025-12-14T23:06:37Z</updated>
    <link href="https://arxiv.org/abs/2512.12870v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12870v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Active Learning (AL) has garnered significant interest across various application domains where labeling training data is costly. AL provides a framework that helps practitioners query informative samples for annotation by oracles (labelers). However, these labels often contain noise due to varying levels of labeler accuracy. Additionally, uncertain samples are more prone to receiving incorrect labels because of their complexity. Learning from imperfectly labeled data leads to an inaccurate classifier. We propose a novel AL framework to construct a robust classification model by minimizing noise levels. Our approach includes an assignment model that optimally assigns query points to labelers, aiming to minimize the maximum possible noise within each cycle. Additionally, we introduce a new sampling method to identify the best query points, reducing the impact of label noise on classifier performance. Our experiments demonstrate that our approach significantly improves classification performance compared to several benchmark methods.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T23:06:37Z</published>
    <arxiv:comment>22 pages, 6 figures. Preprint under review</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Pouya Ahadi</name>
    </author>
    <author>
      <name>Blair Winograd</name>
    </author>
    <author>
      <name>Camille Zaug</name>
    </author>
    <author>
      <name>Karunesh Arora</name>
    </author>
    <author>
      <name>Lijun Wang</name>
    </author>
    <author>
      <name>Kamran Paynabar</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12869v1</id>
    <title>ERA-IT: Aligning Semantic Models with Revealed Economic Preference for Real-Time and Explainable Patent Valuation</title>
    <updated>2025-12-14T23:04:07Z</updated>
    <link href="https://arxiv.org/abs/2512.12869v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12869v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Valuing intangible assets under uncertainty remains a critical challenge in the strategic management of technological innovation due to the information asymmetry inherent in high-dimensional technical specifications. Traditional bibliometric indicators, such as citation counts, fail to address this friction in a timely manner due to the systemic latency inherent in data accumulation. To bridge this gap, this study proposes the Economic Reasoning Alignment via Instruction Tuning (ERA-IT) framework. We theoretically conceptualize patent renewal history as a revealed economic preference and leverage it as an objective supervisory signal to align the generative reasoning of Large Language Models (LLMs) with market realities, a process we term Eco-Semantic Alignment. Using a randomly sampled dataset of 10,000 European Patent Office patents across diverse technological domains, we trained the model not only to predict value tiers but also to reverse-engineer the Economic Chain-of-Thought from unstructured text. Empirical results demonstrate that ERA-IT significantly outperforms both conventional econometric models and zero-shot LLMs in predictive accuracy. More importantly, by generating explicit, logically grounded rationales for valuation, the framework serves as a transparent cognitive scaffold for decision-makers, reducing the opacity of black-box AI in high-stakes intellectual property management.</summary>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T23:04:07Z</published>
    <arxiv:primary_category term="cs.CE"/>
    <author>
      <name>Yoo Yongmin</name>
    </author>
    <author>
      <name>Kim Seungwoo</name>
    </author>
    <author>
      <name>Liu Jingjiang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12868v1</id>
    <title>Counting Clues: A Lightweight Probabilistic Baseline Can Match an LLM</title>
    <updated>2025-12-14T23:00:10Z</updated>
    <link href="https://arxiv.org/abs/2512.12868v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12868v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Large language models (LLMs) excel on multiple-choice clinical diagnosis benchmarks, yet it is unclear how much of this performance reflects underlying probabilistic reasoning. We study this through questions from MedQA, where the task is to select the most likely diagnosis. We introduce the Frequency-Based Probabilistic Ranker (FBPR), a lightweight method that scores options with a smoothed Naive Bayes over concept-diagnosis co-occurrence statistics from a large corpus. When co-occurrence statistics were sourced from the pretraining corpora for OLMo and Llama, FBPR achieves comparable performance to the corresponding LLMs pretrained on that same corpus. Direct LLM inference and FBPR largely get different questions correct, with an overlap only slightly above random chance, indicating complementary strengths of each method. These findings highlight the continued value of explicit probabilistic baselines: they provide a meaningful performance reference point and a complementary signal for potential hybridization. While the performance of LLMs seems to be driven by a mechanism other than simple frequency aggregation, we show that an approach similar to the historically grounded, low-complexity expert systems still accounts for a substantial portion of benchmark performance.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T23:00:10Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Furong Jia</name>
    </author>
    <author>
      <name>Yuan Pu</name>
    </author>
    <author>
      <name>Finn Guo</name>
    </author>
    <author>
      <name>Monica Agrawal</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12858v1</id>
    <title>Information-Consistent Language Model Recommendations through Group Relative Policy Optimization</title>
    <updated>2025-12-14T21:52:31Z</updated>
    <link href="https://arxiv.org/abs/2512.12858v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12858v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Large Language Models (LLMs) are increasingly deployed in business-critical domains such as finance, education, healthcare, and customer support, where users expect consistent and reliable recommendations. Yet LLMs often exhibit variability when prompts are phrased with minor differences, even when semantically equivalent. Such inconsistency undermines trust, complicates compliance, and disrupts user experience. While personalization is desirable in certain contexts, many enterprise scenarios-such as HR onboarding, customer support, or policy disclosure-require invariant information delivery regardless of phrasing or prior conversational history. Existing approaches, including retrieval-augmented generation (RAG) and temperature tuning, improve factuality or reduce stochasticity but cannot guarantee stability across equivalent prompts. In this paper, we propose a reinforcement learning framework based on Group Relative Policy Optimization (GRPO) to directly optimize for consistency. Unlike prior applications of GRPO, which have been limited to reasoning and code generation, we adapt GRPO to enforce stability of information content across groups of semantically equivalent prompts. We introduce entropy-based helpfulness and stability rewards, treating prompt variants as groups and resetting conversational context to isolate phrasing effects. Experiments on investment and job recommendation tasks show that our GRPO-trained model reduces variability more effectively than fine-tuning or decoding-based baselines. To our knowledge, this is a novel application of GRPO for aligning LLMs toward information consistency, reframing variability not as an acceptable feature of generative diversity but as a correctable flaw in enterprise deployments.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T21:52:31Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Sonal Prabhune</name>
    </author>
    <author>
      <name>Balaji Padmanabhan</name>
    </author>
    <author>
      <name>Kaushik Dutta</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12856v1</id>
    <title>Forgetful but Faithful: A Cognitive Memory Architecture and Benchmark for Privacy-Aware Generative Agents</title>
    <updated>2025-12-14T21:40:07Z</updated>
    <link href="https://arxiv.org/abs/2512.12856v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12856v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>As generative agents become increasingly sophisticated and deployed in long-term interactive scenarios, their memory management capabilities emerge as a critical bottleneck for both performance and privacy. Current approaches either maintain unlimited memory stores, leading to computational intractability and privacy concerns, or employ simplistic forgetting mechanisms that compromise agent coherence and functionality. This paper introduces the Memory-Aware Retention Schema (MaRS), a novel framework for human-centered memory management in generative agents, coupled with six theoretically-grounded forgetting policies that balance performance, privacy, and computational efficiency. We present the Forgetful but Faithful Agent (FiFA) benchmark, a comprehensive evaluation framework that assesses agent performance across narrative coherence, goal completion, social recall accuracy, privacy preservation, and cost efficiency. Through extensive experimentation involving 300 evaluation runs across multiple memory budgets and agent configurations, we demonstrate that our hybrid forgetting policy achieves superior performance (composite score: 0.911) while maintaining computational tractability and privacy guarantees. Our work establishes new benchmarks for memory-budgeted agent evaluation and provides practical guidelines for deploying generative agents in resource-constrained, privacy-sensitive environments. The theoretical foundations, implementation framework, and empirical results contribute to the emerging field of human-centered AI by addressing fundamental challenges in agent memory management that directly impact user trust, system scalability, and regulatory compliance.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T21:40:07Z</published>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Saad Alqithami</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12850v1</id>
    <title>KANELÉ: Kolmogorov-Arnold Networks for Efficient LUT-based Evaluation</title>
    <updated>2025-12-14T21:29:10Z</updated>
    <link href="https://arxiv.org/abs/2512.12850v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12850v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Low-latency, resource-efficient neural network inference on FPGAs is essential for applications demanding real-time capability and low power. Lookup table (LUT)-based neural networks are a common solution, combining strong representational power with efficient FPGA implementation. In this work, we introduce KANELÉ, a framework that exploits the unique properties of Kolmogorov-Arnold Networks (KANs) for FPGA deployment. Unlike traditional multilayer perceptrons (MLPs), KANs employ learnable one-dimensional splines with fixed domains as edge activations, a structure naturally suited to discretization and efficient LUT mapping. We present the first systematic design flow for implementing KANs on FPGAs, co-optimizing training with quantization and pruning to enable compact, high-throughput, and low-latency KAN architectures. Our results demonstrate up to a 2700x speedup and orders of magnitude resource savings compared to prior KAN-on-FPGA approaches. Moreover, KANELÉ matches or surpasses other LUT-based architectures on widely used benchmarks, particularly for tasks involving symbolic or physical formulas, while balancing resource usage across FPGA hardware. Finally, we showcase the versatility of the framework by extending it to real-time, power-efficient control systems.</summary>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ex" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T21:29:10Z</published>
    <arxiv:comment>International Symposium on Field-Programmable Gate Arrays 2026 (ISFPGA'2026)</arxiv:comment>
    <arxiv:primary_category term="cs.AR"/>
    <author>
      <name>Duc Hoang</name>
    </author>
    <author>
      <name>Aarush Gupta</name>
    </author>
    <author>
      <name>Philip Harris</name>
    </author>
    <arxiv:doi>10.1145/3748173.3779202</arxiv:doi>
    <link rel="related" href="https://doi.org/10.1145/3748173.3779202" title="doi"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12847v1</id>
    <title>HaShiFlex: A High-Throughput Hardened Shifter DNN Accelerator with Fine-Tuning Flexibility</title>
    <updated>2025-12-14T21:22:33Z</updated>
    <link href="https://arxiv.org/abs/2512.12847v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12847v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We introduce a high-throughput neural network accelerator that embeds most network layers directly in hardware, minimizing data transfer and memory usage while preserving a degree of flexibility via a small neural processing unit for the final classification layer. By leveraging power-of-two (Po2) quantization for weights, we replace multiplications with simple rewiring, effectively reducing each convolution to a series of additions. This streamlined approach offers high-throughput, energy-efficient processing, making it highly suitable for applications where model parameters remain stable, such as continuous sensing tasks at the edge or large-scale data center deployments. Furthermore, by including a strategically chosen reprogrammable final layer, our design achieves high throughput without sacrificing fine-tuning capabilities. We implement this accelerator in a 7nm ASIC flow using MobileNetV2 as a baseline and report throughput, area, accuracy, and sensitivity to quantization and pruning - demonstrating both the advantages and potential trade-offs of the proposed architecture. We find that for MobileNetV2, we can improve inference throughput by 20x over fully programmable GPUs, processing 1.21 million images per second through a full forward pass while retaining fine-tuning flexibility. If absolutely no post-deployment fine tuning is required, this advantage increases to 67x at 4 million images per second.</summary>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T21:22:33Z</published>
    <arxiv:comment>12 pages, 6 figures, 5 tables</arxiv:comment>
    <arxiv:primary_category term="cs.AR"/>
    <author>
      <name>Jonathan Herbst</name>
      <arxiv:affiliation>Brown University</arxiv:affiliation>
    </author>
    <author>
      <name>Michael Pellauer</name>
      <arxiv:affiliation>NVIDIA</arxiv:affiliation>
    </author>
    <author>
      <name>Sherief Reda</name>
      <arxiv:affiliation>Brown University</arxiv:affiliation>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12844v1</id>
    <title>Selective Conformal Risk Control</title>
    <updated>2025-12-14T21:18:28Z</updated>
    <link href="https://arxiv.org/abs/2512.12844v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12844v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Reliable uncertainty quantification is essential for deploying machine learning systems in high-stakes domains. Conformal prediction provides distribution-free coverage guarantees but often produces overly large prediction sets, limiting its practical utility. To address this issue, we propose \textit{Selective Conformal Risk Control} (SCRC), a unified framework that integrates conformal prediction with selective classification. The framework formulates uncertainty control as a two-stage problem: the first stage selects confident samples for prediction, and the second stage applies conformal risk control on the selected subset to construct calibrated prediction sets. We develop two algorithms under this framework. The first, SCRC-T, preserves exchangeability by computing thresholds jointly over calibration and test samples, offering exact finite-sample guarantees. The second, SCRC-I, is a calibration-only variant that provides PAC-style probabilistic guarantees while being more computational efficient. Experiments on two public datasets show that both methods achieve the target coverage and risk levels, with nearly identical performance, while SCRC-I exhibits slightly more conservative risk control but superior computational practicality. Our results demonstrate that selective conformal risk control offers an effective and efficient path toward compact, reliable uncertainty quantification.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T21:18:28Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Yunpeng Xu</name>
    </author>
    <author>
      <name>Wenge Guo</name>
    </author>
    <author>
      <name>Zhi Wei</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12842v1</id>
    <title>SAGA: Open-World Mobile Manipulation via Structured Affordance Grounding</title>
    <updated>2025-12-14T21:13:56Z</updated>
    <link href="https://arxiv.org/abs/2512.12842v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12842v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We present SAGA, a versatile and adaptive framework for visuomotor control that can generalize across various environments, task objectives, and user specifications. To efficiently learn such capability, our key idea is to disentangle high-level semantic intent from low-level visuomotor control by explicitly grounding task objectives in the observed environment. Using an affordance-based task representation, we express diverse and complex behaviors in a unified, structured form. By leveraging multimodal foundation models, SAGA grounds the proposed task representation to the robot's visual observation as 3D affordance heatmaps, highlighting task-relevant entities while abstracting away spurious appearance variations that would hinder generalization. These grounded affordances enable us to effectively train a conditional policy on multi-task demonstration data for whole-body control. In a unified framework, SAGA can solve tasks specified in different forms, including language instructions, selected points, and example demonstrations, enabling both zero-shot execution and few-shot adaptation. We instantiate SAGA on a quadrupedal manipulator and conduct extensive experiments across eleven real-world tasks. SAGA consistently outperforms end-to-end and modular baselines by substantial margins. Together, these results demonstrate that structured affordance grounding offers a scalable and effective pathway toward generalist mobile manipulation.</summary>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T21:13:56Z</published>
    <arxiv:comment>9 pages, 7 figures</arxiv:comment>
    <arxiv:primary_category term="cs.RO"/>
    <author>
      <name>Kuan Fang</name>
    </author>
    <author>
      <name>Yuxin Chen</name>
    </author>
    <author>
      <name>Xinghao Zhu</name>
    </author>
    <author>
      <name>Farzad Niroui</name>
    </author>
    <author>
      <name>Lingfeng Sun</name>
    </author>
    <author>
      <name>Jiuguang Wang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12840v1</id>
    <title>PRIVEE: Privacy-Preserving Vertical Federated Learning Against Feature Inference Attacks</title>
    <updated>2025-12-14T21:05:19Z</updated>
    <link href="https://arxiv.org/abs/2512.12840v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12840v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Vertical Federated Learning (VFL) enables collaborative model training across organizations that share common user samples but hold disjoint feature spaces. Despite its potential, VFL is susceptible to feature inference attacks, in which adversarial parties exploit shared confidence scores (i.e., prediction probabilities) during inference to reconstruct private input features of other participants. To counter this threat, we propose PRIVEE (PRIvacy-preserving Vertical fEderated lEarning), a novel defense mechanism named after the French word privée, meaning "private." PRIVEE obfuscates confidence scores while preserving critical properties such as relative ranking and inter-score distances. Rather than exposing raw scores, PRIVEE shares only the transformed representations, mitigating the risk of reconstruction attacks without degrading model prediction accuracy. Extensive experiments show that PRIVEE achieves a threefold improvement in privacy protection compared to state-of-the-art defenses, while preserving full predictive performance against advanced feature inference attacks.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T21:05:19Z</published>
    <arxiv:comment>12 pages, 3 figures</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Sindhuja Madabushi</name>
    </author>
    <author>
      <name>Ahmad Faraz Khan</name>
    </author>
    <author>
      <name>Haider Ali</name>
    </author>
    <author>
      <name>Ananthram Swami</name>
    </author>
    <author>
      <name>Rui Ning</name>
    </author>
    <author>
      <name>Hongyi Wu</name>
    </author>
    <author>
      <name>Jin-Hee Cho</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12839v1</id>
    <title>What Matters in Evaluating Book-Length Stories? A Systematic Study of Long Story Evaluation</title>
    <updated>2025-12-14T20:53:29Z</updated>
    <link href="https://arxiv.org/abs/2512.12839v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12839v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>In this work, we conduct systematic research in a challenging area: the automatic evaluation of book-length stories (&gt;100K tokens). Our study focuses on two key questions: (1) understanding which evaluation aspects matter most to readers, and (2) exploring effective methods for evaluating lengthy stories. We introduce the first large-scale benchmark, LongStoryEval, comprising 600 newly published books with an average length of 121K tokens (maximum 397K). Each book includes its average rating and multiple reader reviews, presented as critiques organized by evaluation aspects. By analyzing all user-mentioned aspects, we propose an evaluation criteria structure and conduct experiments to identify the most significant aspects among the 8 top-level criteria. For evaluation methods, we compare the effectiveness of three types: aggregation-based, incremental-updated, and summary-based evaluations. Our findings reveal that aggregation- and summary-based evaluations perform better, with the former excelling in detail assessment and the latter offering greater efficiency. Building on these insights, we further propose NovelCritique, an 8B model that leverages the efficient summary-based framework to review and score stories across specified aspects. NovelCritique outperforms commercial models like GPT-4o in aligning with human evaluations. Our datasets and codes are available at https://github.com/DingyiYang/LongStoryEval.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T20:53:29Z</published>
    <arxiv:comment>24 pages, 7 figures, Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Dingyi Yang</name>
    </author>
    <author>
      <name>Qin Jin</name>
    </author>
    <arxiv:doi>10.18653/v1/2025.acl-long.799</arxiv:doi>
    <link rel="related" href="https://doi.org/10.18653/v1/2025.acl-long.799" title="doi"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12837v1</id>
    <title>Algorithmic Criminal Liability in Greenwashing: Comparing India, United States, and European Union</title>
    <updated>2025-12-14T20:49:41Z</updated>
    <link href="https://arxiv.org/abs/2512.12837v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12837v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>AI-powered greenwashing has emerged as an insidious challenge within corporate sustainability governance, exacerbating the opacity of environmental disclosures and subverting regulatory oversight. This study conducts a comparative legal analysis of criminal liability for AI-mediated greenwashing across India, the US, and the EU, exposing doctrinal lacunae in attributing culpability when deceptive claims originate from algorithmic systems. Existing statutes exhibit anthropocentric biases by predicating liability on demonstrable human intent, rendering them ill-equipped to address algorithmic deception. The research identifies a critical gap in jurisprudential adaptation, as prevailing fraud statutes remain antiquated vis-à-vis AI-generated misrepresentation. Utilising a doctrinal legal methodology, this study systematically dissects judicial precedents and statutory instruments, yielding results regarding the potential expansion of corporate criminal liability. Findings underscore the viability of strict liability models, recalibrated governance frameworks for AI accountability, and algorithmic due diligence mandates under ESG regimes. Comparative insights reveal jurisdictional disparities, with the EU Corporate Sustainability Due Diligence Directive (CSDDD) offering a potential transnational model. This study contributes to AI ethics and environmental jurisprudence by advocating for a hybrid liability framework integrating algorithmic risk assessment with legal personhood constructs, ensuring algorithmic opacity does not preclude liability enforcement.</summary>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T20:49:41Z</published>
    <arxiv:comment>Published in HPNLU Journal of Law, Business and Economics, Vol. 3, 2024, pp. 51-68. ISSN: 2584-0436</arxiv:comment>
    <arxiv:primary_category term="cs.CY"/>
    <arxiv:journal_ref>HPNLU Journal of Law, Business and Economics, Vol. 3, pp. 51-68, 2024</arxiv:journal_ref>
    <author>
      <name>Sahibpreet Singh</name>
    </author>
    <author>
      <name>Manjit Singh</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12832v1</id>
    <title>Network Level Evaluation of Hangup Susceptibility of HRGCs using Deep Learning and Sensing Techniques: A Goal Towards Safer Future</title>
    <updated>2025-12-14T20:25:42Z</updated>
    <link href="https://arxiv.org/abs/2512.12832v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12832v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Steep profiled Highway Railway Grade Crossings (HRGCs) pose safety hazards to vehicles with low ground clearance, which may become stranded on the tracks, creating risks of train vehicle collisions. This research develops a framework for network level evaluation of hangup susceptibility of HRGCs. Profile data from different crossings in Oklahoma were collected using both a walking profiler and the Pave3D8K Laser Imaging System. A hybrid deep learning model, combining Long Short Term Memory (LSTM) and Transformer architectures, was developed to reconstruct accurate HRGC profiles from Pave3D8K Laser Imaging System data. Vehicle dimension data from around 350 specialty vehicles were collected at various locations across Oklahoma to enable up to date statistical design dimensions. Hangup susceptibility was analyzed using three vehicle dimension scenarios (a) median dimension (median wheelbase and ground clearance), (b) 75 25 percentile dimension (75 percentile wheelbase, 25 percentile ground clearance), and (c) worst case dimension (maximum wheelbase and minimum ground clearance). Results indicate 36, 62, and 67 crossings at the highest hangup risk levels under these scenarios, respectively. An ArcGIS database and a software interface were developed to support transportation agencies in mitigating crossing hazards. This framework advances safety evaluation by integrating next generation sensing, deep learning, and infrastructure datasets into practical decision support tools.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T20:25:42Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Kaustav Chatterjee</name>
    </author>
    <author>
      <name>Joshua Li</name>
    </author>
    <author>
      <name>Kundan Parajulee</name>
    </author>
    <author>
      <name>Jared Schwennesen</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12829v1</id>
    <title>Towards a Systematic Taxonomy of Attacks against Space Infrastructures</title>
    <updated>2025-12-14T20:20:36Z</updated>
    <link href="https://arxiv.org/abs/2512.12829v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12829v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Space infrastructures represent an emerging domain that is critical to the global economy and society. However, this domain is vulnerable to attacks. To enhance the resilience of this domain, we must understand the attacks that can be waged against it. The status quo is that there is no systematic understanding of attacks against space infrastructures, despite their importance in guiding systematic analysis of space cybersecurity and future research. In this paper, we fill the void by proposing the first systematic taxonomy of attacks against space infrastructures. We hope this paper will inspire a community effort at refining the taxonomy towards a widely used taxonomy.</summary>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T20:20:36Z</published>
    <arxiv:primary_category term="cs.CR"/>
    <author>
      <name>Jose Luis Castanon Remy</name>
    </author>
    <author>
      <name>Shouhuai Xu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12827v1</id>
    <title>GradID: Adversarial Detection via Intrinsic Dimensionality of Gradients</title>
    <updated>2025-12-14T20:16:03Z</updated>
    <link href="https://arxiv.org/abs/2512.12827v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12827v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Despite their remarkable performance, deep neural networks exhibit a critical vulnerability: small, often imperceptible, adversarial perturbations can lead to drastically altered model predictions. Given the stringent reliability demands of applications such as medical diagnosis and autonomous driving, robust detection of such adversarial attacks is paramount. In this paper, we investigate the geometric properties of a model's input loss landscape. We analyze the Intrinsic Dimensionality (ID) of the model's gradient parameters, which quantifies the minimal number of coordinates required to describe the data points on their underlying manifold. We reveal a distinct and consistent difference in the ID for natural and adversarial data, which forms the basis of our proposed detection method. We validate our approach across two distinct operational scenarios. First, in a batch-wise context for identifying malicious data groups, our method demonstrates high efficacy on datasets like MNIST and SVHN. Second, in the critical individual-sample setting, we establish new state-of-the-art results on challenging benchmarks such as CIFAR-10 and MS COCO. Our detector significantly surpasses existing methods against a wide array of attacks, including CW and AutoAttack, achieving detection rates consistently above 92\% on CIFAR-10. The results underscore the robustness of our geometric approach, highlighting that intrinsic dimensionality is a powerful fingerprint for adversarial detection across diverse datasets and attack strategies.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T20:16:03Z</published>
    <arxiv:comment>16 pages, 8 figures</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Mohammad Mahdi Razmjoo</name>
    </author>
    <author>
      <name>Mohammad Mahdi Sharifian</name>
    </author>
    <author>
      <name>Saeed Bagheri Shouraki</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12824v1</id>
    <title>Adapting Multimodal Foundation Models for Few-Shot Learning: A Comprehensive Study on Contrastive Captioners</title>
    <updated>2025-12-14T20:13:21Z</updated>
    <link href="https://arxiv.org/abs/2512.12824v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12824v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Large-scale multimodal foundation models, particularly Contrastive Captioners (CoCa), have achieved state-of-the-art results by unifying contrastive alignment with generative captioning. While zero-shot transfer capabilities are well-documented, the adaptation of these generative-contrastive hybrids to downstream tasks with extreme data scarcity (few-shot learning) remains under-explored. Existing literature predominantly focuses on dual-encoder architectures like CLIP, leaving a gap in understanding how CoCa's distinct latent space responds to parameter-efficient fine-tuning (PEFT). This paper presents a comprehensive empirical study on adapting the CoCa visual backbone for few-shot image classification. We systematically evaluate a hierarchy of strategies, ranging from training-free hybrid prototyping to deep parameter adaptation via Low-Rank Adaptation (LoRA). First, we identify an "augmentation divergence": while strong data augmentation degrades the performance of linear probing in low-shot settings, it is essential for stabilizing LoRA fine-tuning. We also demonstrate that hybrid objectives incorporating Supervised Contrastive (SupCon) loss yield consistent performance improvements over standard Cross-Entropy across varying shot counts. Crucially, we characterize the sensitivity of training configurations to data scarcity, providing empirical reference settings for scaling regularization, rank, and sampling strategies to facilitate the efficient adaptation of generative-contrastive foundation models.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T20:13:21Z</published>
    <arxiv:comment>9 pages, 3 figures. Accepted to VISAPP 2026</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>N. K. B. M. P. K. B. Narasinghe</name>
    </author>
    <author>
      <name>Uthayasanker Thayasivam</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12822v1</id>
    <title>Lemon: A Unified and Scalable 3D Multimodal Model for Universal Spatial Understanding</title>
    <updated>2025-12-14T20:02:43Z</updated>
    <link href="https://arxiv.org/abs/2512.12822v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12822v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Scaling large multimodal models (LMMs) to 3D understanding poses unique challenges: point cloud data is sparse and irregular, existing models rely on fragmented architectures with modality-specific encoders, and training pipelines often suffer from instability and poor scalability. We introduce Lemon, a unified transformer architecture that addresses these challenges by jointly processing 3D point cloud patches and language tokens as a single sequence. Unlike prior work that relies on modality-specific encoders and cross-modal alignment modules, this design enables early spatial-linguistic fusion, eliminates redundant encoders, improves parameter efficiency, and supports more effective model scaling. To handle the complexity of 3D data, we develop a structured patchification and tokenization scheme that preserves spatial context, and a three-stage training curriculum that progressively builds capabilities from object-level recognition to scene-level spatial reasoning. Lemon establishes new state-of-the-art performance across comprehensive 3D understanding and reasoning tasks, from object recognition and captioning to spatial reasoning in 3D scenes, while demonstrating robust scaling properties as model size and training data increase. Our work provides a unified foundation for advancing 3D spatial intelligence in real-world applications.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T20:02:43Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Yongyuan Liang</name>
    </author>
    <author>
      <name>Xiyao Wang</name>
    </author>
    <author>
      <name>Yuanchen Ju</name>
    </author>
    <author>
      <name>Jianwei Yang</name>
    </author>
    <author>
      <name>Furong Huang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12821v1</id>
    <title>On the continuity of flows</title>
    <updated>2025-12-14T20:00:39Z</updated>
    <link href="https://arxiv.org/abs/2512.12821v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12821v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Flow matching has emerged as a powerful framework for generative modeling through continuous normalizing flows. We investigate a potential topological constraint: when the prior distribution and target distribution have mismatched topology (e.g., unimodal to multimodal), the optimal velocity field under standard flow matching objectives may exhibit spatial discontinuities. We suggest that this discontinuity arises from the requirement that continuous flows must bifurcate to map a single mode to multiple modes, forcing particles to make discrete routing decisions at intermediate times. Through theoretical analysis on bimodal Gaussian mixtures, we demonstrate that the optimal velocity field exhibits jump discontinuities along decision boundaries, with magnitude approaching infinity as time approaches the target distribution. Our analysis suggests that this phenomenon is not specific to $L^2$ loss, but rather may be a consequence of topological mismatch between distributions. We validate our theory empirically and discuss potential implications for flow matching on manifolds, connecting our findings to recent work on Riemannian flow matching and the challenge of learning discontinuous representations in neural networks.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T20:00:39Z</published>
    <arxiv:comment>9 pages, 2 figures</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Congzhou M Sha</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12818v1</id>
    <title>Hindsight is 20/20: Building Agent Memory that Retains, Recalls, and Reflects</title>
    <updated>2025-12-14T19:47:23Z</updated>
    <link href="https://arxiv.org/abs/2512.12818v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12818v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Agent memory has been touted as a dimension of growth for LLM-based applications, enabling agents that can accumulate experience, adapt across sessions, and move beyond single-shot question answering. The current generation of agent memory systems treats memory as an external layer that extracts salient snippets from conversations, stores them in vector or graph-based stores, and retrieves top-k items into the prompt of an otherwise stateless model. While these systems improve personalization and context carry-over, they still blur the line between evidence and inference, struggle to organize information over long horizons, and offer limited support for agents that must explain their reasoning. We present Hindsight, a memory architecture that treats agent memory as a structured, first-class substrate for reasoning by organizing it into four logical networks that distinguish world facts, agent experiences, synthesized entity summaries, and evolving beliefs. This framework supports three core operations -- retain, recall, and reflect -- that govern how information is added, accessed, and updated. Under this abstraction, a temporal, entity aware memory layer incrementally turns conversational streams into a structured, queryable memory bank, while a reflection layer reasons over this bank to produce answers and to update information in a traceable way. On key long-horizon conversational memory benchmarks like LongMemEval and LoCoMo, Hindsight with an open-source 20B model lifts overall accuracy from 39% to 83.6% over a full-context baseline with the same backbone and outperforms full context GPT-4o. Scaling the backbone further pushes Hindsight to 91.4% on LongMemEval and up to 89.61% on LoCoMo (vs. 75.78% for the strongest prior open system), consistently outperforming existing memory architectures on multi-session and open-domain questions.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T19:47:23Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Chris Latimer</name>
    </author>
    <author>
      <name>Nicoló Boschi</name>
    </author>
    <author>
      <name>Andrew Neeser</name>
    </author>
    <author>
      <name>Chris Bartholomew</name>
    </author>
    <author>
      <name>Gaurav Srivastava</name>
    </author>
    <author>
      <name>Xuan Wang</name>
    </author>
    <author>
      <name>Naren Ramakrishnan</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12817v1</id>
    <title>Decoding Human and AI Persuasion in National College Debate: Analyzing Prepared Arguments Through Aristotle's Rhetorical Principles</title>
    <updated>2025-12-14T19:46:16Z</updated>
    <link href="https://arxiv.org/abs/2512.12817v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12817v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Debate has been widely adopted as a strategy to enhance critical thinking skills in English Language Arts (ELA). One important skill in debate is forming effective argumentation, which requires debaters to select supportive evidence from literature and construct compelling claims. However, the training of this skill largely depends on human coaching, which is labor-intensive and difficult to scale. To better support students in preparing for debates, this study explores the potential of leveraging artificial intelligence to generate effective arguments. Specifically, we prompted GPT-4 to create an evidence card and compared it to those produced by human debaters. The evidence cards outline the arguments students will present and how those arguments will be delivered, including components such as literature-based evidence quotations, summaries of core ideas, verbatim reading scripts, and tags (i.e., titles of the arguments). We compared the quality of the arguments in the evidence cards created by GPT and student debaters using Aristotle's rhetorical principles: ethos (credibility), pathos (emotional appeal), and logos (logical reasoning). Through a systematic qualitative and quantitative analysis, grounded in the rhetorical principles, we identify the strengths and limitations of human and GPT in debate reasoning, outlining areas where AI's focus and justifications align with or diverge from human reasoning. Our findings contribute to the evolving role of AI-assisted learning interventions, offering insights into how student debaters can develop strategies that enhance their argumentation and reasoning skills.</summary>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T19:46:16Z</published>
    <arxiv:primary_category term="cs.HC"/>
    <author>
      <name>Mengqian Wu</name>
    </author>
    <author>
      <name>Jiayi Zhang</name>
    </author>
    <author>
      <name>Raymond Z. Zhang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12816v1</id>
    <title>Optimal Resource Allocation for ML Model Training and Deployment under Concept Drift</title>
    <updated>2025-12-14T19:42:04Z</updated>
    <link href="https://arxiv.org/abs/2512.12816v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12816v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We study how to allocate resources for training and deployment of machine learning (ML) models under concept drift and limited budgets. We consider a setting in which a model provider distributes trained models to multiple clients whose devices support local inference but lack the ability to retrain those models, placing the burden of performance maintenance on the provider. We introduce a model-agnostic framework that captures the interaction between resource allocation, concept drift dynamics, and deployment timing. We show that optimal training policies depend critically on the aging properties of concept durations. Under sudden concept changes, we derive optimal training policies subject to budget constraints when concept durations follow distributions with Decreasing Mean Residual Life (DMRL), and show that intuitive heuristics are provably suboptimal under Increasing Mean Residual Life (IMRL). We further study model deployment under communication constraints, prove that the associated optimization problem is quasi-convex under mild conditions, and propose a randomized scheduling strategy that achieves near-optimal client-side performance. These results offer theoretical and algorithmic foundations for cost-efficient ML model management under concept drift, with implications for continual learning, distributed inference, and adaptive ML systems.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T19:42:04Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Hasan Burhan Beytur</name>
    </author>
    <author>
      <name>Gustavo de Veciana</name>
    </author>
    <author>
      <name>Haris Vikalo</name>
    </author>
    <author>
      <name>Kevin S Chan</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12812v1</id>
    <title>Does Tone Change the Answer? Evaluating Prompt Politeness Effects on Modern LLMs: GPT, Gemini, LLaMA</title>
    <updated>2025-12-14T19:25:20Z</updated>
    <link href="https://arxiv.org/abs/2512.12812v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12812v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Prompt engineering has emerged as a critical factor influencing large language model (LLM) performance, yet the impact of pragmatic elements such as linguistic tone and politeness remains underexplored, particularly across different model families. In this work, we propose a systematic evaluation framework to examine how interaction tone affects model accuracy and apply it to three recently released and widely available LLMs: GPT-4o mini (OpenAI), Gemini 2.0 Flash (Google DeepMind), and Llama 4 Scout (Meta). Using the MMMLU benchmark, we evaluate model performance under Very Friendly, Neutral, and Very Rude prompt variants across six tasks spanning STEM and Humanities domains, and analyze pairwise accuracy differences with statistical significance testing.
  Our results show that tone sensitivity is both model-dependent and domain-specific. Neutral or Very Friendly prompts generally yield higher accuracy than Very Rude prompts, but statistically significant effects appear only in a subset of Humanities tasks, where rude tone reduces accuracy for GPT and Llama, while Gemini remains comparatively tone-insensitive. When performance is aggregated across tasks within each domain, tone effects diminish and largely lose statistical significance. Compared with earlier researches, these findings suggest that dataset scale and coverage materially influence the detection of tone effects. Overall, our study indicates that while interaction tone can matter in specific interpretive settings, modern LLMs are broadly robust to tonal variation in typical mixed-domain use, providing practical guidance for prompt design and model selection in real-world deployments.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T19:25:20Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Hanyu Cai</name>
    </author>
    <author>
      <name>Binqi Shen</name>
    </author>
    <author>
      <name>Lier Jin</name>
    </author>
    <author>
      <name>Lan Hu</name>
    </author>
    <author>
      <name>Xiaojing Fan</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12809v1</id>
    <title>OPAL: Operator-Programmed Algorithms for Landscape-Aware Black-Box Optimization</title>
    <updated>2025-12-14T19:16:49Z</updated>
    <link href="https://arxiv.org/abs/2512.12809v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12809v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Black-box optimization often relies on evolutionary and swarm algorithms whose performance is highly problem dependent. We view an optimizer as a short program over a small vocabulary of search operators and learn this operator program separately for each problem instance. We instantiate this idea in Operator-Programmed Algorithms (OPAL), a landscape-aware framework for continuous black-box optimization that uses a small design budget with a standard differential evolution baseline to probe the landscape, builds a $k$-nearest neighbor graph over sampled points, and encodes this trajectory with a graph neural network. A meta-learner then maps the resulting representation to a phase-wise schedule of exploration, restart, and local search operators. On the CEC~2017 test suite, a single meta-trained OPAL policy is statistically competitive with state-of-the-art adaptive differential evolution variants and achieves significant improvements over simpler baselines under nonparametric tests. Ablation studies on CEC~2017 justify the choices for the design phase, the trajectory graph, and the operator-program representation, while the meta-components add only modest wall-clock overhead. Overall, the results indicate that operator-programmed, landscape-aware per-instance design is a practical way forward beyond ad hoc metaphor-based algorithms in black-box optimization.</summary>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T19:16:49Z</published>
    <arxiv:comment>Source code, experiment scripts, and results are publicly available at https://github.com/junbolian/OPAL. The real-world application part hasn't been done yet</arxiv:comment>
    <arxiv:primary_category term="cs.NE"/>
    <author>
      <name>Junbo Jacob Lian</name>
    </author>
    <author>
      <name>Mingyang Yu</name>
    </author>
    <author>
      <name>Kaichen Ouyang</name>
    </author>
    <author>
      <name>Shengwei Fu</name>
    </author>
    <author>
      <name>Rui Zhong</name>
    </author>
    <author>
      <name>Yujun Zhang</name>
    </author>
    <author>
      <name>Jun Zhang</name>
    </author>
    <author>
      <name>Huiling Chen</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12806v1</id>
    <title>Fault-Tolerant Sandboxing for AI Coding Agents: A Transactional Approach to Safe Autonomous Execution</title>
    <updated>2025-12-14T19:03:59Z</updated>
    <link href="https://arxiv.org/abs/2512.12806v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12806v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The transition of Large Language Models (LLMs) from passive code generators to autonomous agents introduces significant safety risks, specifically regarding destructive commands and inconsistent system states. Existing commercial solutions often prioritize interactive user safety, enforcing authentication barriers that break the headless loops required for true autonomy. This paper presents a Fault-Tolerant Sandboxing framework designed to mitigate these risks through a policy-based interception layer and a transactional filesystem snapshot mechanism. We hypothesize that wrapping agent actions in atomic transactions can guarantee safety with acceptable latency, outperforming the heavy initialization overhead of containers or the interactive friction of commercial CLIs. We validated this approach by deploying the Minimind-MoE LLM served via nano-vllm on a custom Proxmox-based testbed utilizing EVPN/VXLAN isolation. Experimental results demonstrate a 100\% interception rate for high-risk commands and a 100\% success rate in rolling back failed states. Crucially, our prototype incurs only a 14.5\% performance overhead (approx. 1.8s) per transaction. In contrast, benchmarking against the Gemini CLI sandbox revealed that it requires interactive authentication ("Sign in"), rendering it unusable for headless, autonomous agent workflows.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T19:03:59Z</published>
    <arxiv:comment>7 pages</arxiv:comment>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Boyang Yan</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12805v1</id>
    <title>From Small to Large: Generalization Bounds for Transformers on Variable-Size Inputs</title>
    <updated>2025-12-14T19:02:16Z</updated>
    <link href="https://arxiv.org/abs/2512.12805v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12805v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Transformers exhibit a notable property of \emph{size generalization}, demonstrating an ability to extrapolate from smaller token sets to significantly longer ones. This behavior has been documented across diverse applications, including point clouds, graphs, and natural language. Despite its empirical success, this capability still lacks some rigorous theoretical characterizations. In this paper, we develop a theoretical framework to analyze this phenomenon for geometric data, which we represent as discrete samples from a continuous source (e.g., point clouds from manifolds, graphs from graphons). Our core contribution is a bound on the error between the Transformer's output for a discrete sample and its continuous-domain equivalent. We prove that for Transformers with stable positional encodings, this bound is determined by the sampling density and the intrinsic dimensionality of the data manifold. Experiments on graphs and point clouds of various sizes confirm the tightness of our theoretical bound.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T19:02:16Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Anastasiia Alokhina</name>
    </author>
    <author>
      <name>Pan Li</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12804v1</id>
    <title>Causal Counterfactuals Reconsidered</title>
    <updated>2025-12-14T18:59:01Z</updated>
    <link href="https://arxiv.org/abs/2512.12804v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12804v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>I develop a novel semantics for probabilities of counterfactuals that generalizes the standard Pearlian semantics: it applies to probabilistic causal models that cannot be extended into realistic structural causal models and are therefore beyond the scope of Pearl's semantics. This generalization is needed because, as I show, such probabilistic causal models arise even in simple settings. My semantics offer a natural compromize in the long-standing debate between Pearl and Dawid over counterfactuals: I agree with Dawid that universal causal determinism and unrealistic variables should be rejected, but I agree with Pearl that a general semantics of counterfactuals is nonetheless possible. I restrict attention to causal models that satisfy the Markov condition, only contain realistic variables, and are causally complete. Although I formulate my proposal using structural causal models, as does Pearl, I refrain from using so-called response variables. Moreover, I prove that my semantics is equivalent to two other recent proposals that do not involve structural causal models, and that it is in line with various comments on stochastic counterfactuals that have appeared in the literature more broadly. Throughout I also reflect on the universality of the Markov condition and explore a novel generalization of causal abstractions</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T18:59:01Z</published>
    <arxiv:comment>Preprint: currently under review</arxiv:comment>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Sander Beckers</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.12802v1</id>
    <title>A Disproof of Large Language Model Consciousness: The Necessity of Continual Learning for Consciousness</title>
    <updated>2025-12-14T18:51:15Z</updated>
    <link href="https://arxiv.org/abs/2512.12802v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.12802v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The requirements for a falsifiable and non-trivial theory of consciousness significantly constrain such theories. Specifically, recent research on the Unfolding Argument and the Substitution Argument has given us formal tools to analyze requirements for a theory of consciousness. I show via a new Proximity Argument that these requirements especially constrain the potential consciousness of contemporary Large Language Models (LLMs) because of their proximity to systems that are equivalent to LLMs in terms of input/output function; yet, for these functionally equivalent systems, there cannot be any non-trivial theory of consciousness that judges them conscious. This forms the basis of a disproof of contemporary LLM consciousness. I then show a positive result, which is that theories of consciousness based on (or requiring) continual learning do satisfy the stringent formal constraints for a theory of consciousness in humans. Intriguingly, this work supports a hypothesis: If continual learning is linked to consciousness in humans, the current limitations of LLMs (which do not continually learn) are intimately tied to their lack of consciousness.</summary>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-14T18:51:15Z</published>
    <arxiv:comment>28 pages, 3 figures</arxiv:comment>
    <arxiv:primary_category term="q-bio.NC"/>
    <author>
      <name>Erik Hoel</name>
    </author>
  </entry>
</feed>
