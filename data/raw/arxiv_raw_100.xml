<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/" xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns="http://www.w3.org/2005/Atom">
  <id>https://arxiv.org/api/QCAx6c6G3ao/klcKP+hlBSw7oio</id>
  <title>arXiv Query: search_query=cat:cs.AI OR cat:cs.LG OR cat:cs.CL OR cat:cs.CV OR cat:cs.NE OR cat:cs.SE OR cat:cs.CR OR cat:cs.DC&amp;id_list=&amp;start=100&amp;max_results=100</title>
  <updated>2025-12-16T13:50:26Z</updated>
  <link href="https://arxiv.org/api/query?search_query=cat:cs.AI+OR+(cat:cs.LG+OR+(cat:cs.CL+OR+(cat:cs.CV+OR+(cat:cs.NE+OR+(cat:cs.SE+OR+(cat:cs.CR+OR+cat:cs.DC))))))&amp;start=100&amp;max_results=100&amp;id_list=" type="application/atom+xml"/>
  <opensearch:itemsPerPage>100</opensearch:itemsPerPage>
  <opensearch:totalResults>566020</opensearch:totalResults>
  <opensearch:startIndex>100</opensearch:startIndex>
  <entry>
    <id>http://arxiv.org/abs/2512.13410v1</id>
    <title>Multiclass Graph-Based Large Margin Classifiers: Unified Approach for Support Vectors and Neural Networks</title>
    <updated>2025-12-15T15:00:13Z</updated>
    <link href="https://arxiv.org/abs/2512.13410v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13410v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>While large margin classifiers are originally an outcome of an optimization framework, support vectors (SVs) can be obtained from geometric approaches. This article presents advances in the use of Gabriel graphs (GGs) in binary and multiclass classification problems. For Chipclass, a hyperparameter-less and optimization-less GG-based binary classifier, we discuss how activation functions and support edge (SE)-centered neurons affect the classification, proposing smoother functions and structural SV (SSV)-centered neurons to achieve margins with low probabilities and smoother classification contours. We extend the neural network architecture, which can be trained with backpropagation with a softmax function and a cross-entropy loss, or by solving a system of linear equations. A new subgraph-/distance-based membership function for graph regularization is also proposed, along with a new GG recomputation algorithm that is less computationally expensive than the standard approach. Experimental results with the Friedman test show that our method was better than previous GG-based classifiers and statistically equivalent to tree-based models.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T15:00:13Z</published>
    <arxiv:comment>Accepted to the IEEE Transactions on Neural Networks and Learning Systems (TNNLS)</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <arxiv:journal_ref>IEEE Transactions on Neural Networks and Learning Systems (Volume: 36, Issue: 5, May 2025, Pages: 8307 - 8316)</arxiv:journal_ref>
    <author>
      <name>Vítor M. Hanriot</name>
    </author>
    <author>
      <name>Luiz C. B. Torres</name>
    </author>
    <author>
      <name>Antônio P. Braga</name>
    </author>
    <arxiv:doi>10.1109/TNNLS.2024.3420227</arxiv:doi>
    <link rel="related" href="https://doi.org/10.1109/TNNLS.2024.3420227" title="doi"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13402v1</id>
    <title>End2Reg: Learning Task-Specific Segmentation for Markerless Registration in Spine Surgery</title>
    <updated>2025-12-15T14:53:20Z</updated>
    <link href="https://arxiv.org/abs/2512.13402v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13402v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Purpose: Intraoperative navigation in spine surgery demands millimeter-level accuracy. Current systems based on intraoperative radiographic imaging and bone-anchored markers are invasive, radiation-intensive and workflow disruptive. Recent markerless RGB-D registration methods offer a promising alternative, but existing approaches rely on weak segmentation labels to isolate relevant anatomical structures, which can propagate errors throughout registration. Methods: We present End2Reg an end-to-end deep learning framework that jointly optimizes segmentation and registration, eliminating the need for weak segmentation labels and manual steps. The network learns segmentation masks specifically optimized for registration, guided solely by the registration objective without direct segmentation supervision. Results: The proposed framework achieves state-of-the-art performance on ex- and in-vivo benchmarks, reducing median Target Registration Error by 32% to 1.83mm and mean Root Mean Square Error by 45% to 3.95mm, respectively. An ablation study confirms that end-to-end optimization significantly improves registration accuracy. Conclusion: The presented end-to-end RGB-D registration pipeline removes dependency on weak labels and manual steps, advancing towards fully automatic, markerless intraoperative navigation. Code and interactive visualizations are available at: https://lorenzopettinari.github.io/end-2-reg/.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T14:53:20Z</published>
    <arxiv:comment>Code and interactive visualizations: https://lorenzopettinari.github.io/end-2-reg/</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Lorenzo Pettinari</name>
    </author>
    <author>
      <name>Sidaty El Hadramy</name>
    </author>
    <author>
      <name>Michael Wehrli</name>
    </author>
    <author>
      <name>Philippe C. Cattin</name>
    </author>
    <author>
      <name>Daniel Studer</name>
    </author>
    <author>
      <name>Carol C. Hasler</name>
    </author>
    <author>
      <name>Maria Licci</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13399v1</id>
    <title>Differentiable Evolutionary Reinforcement Learning</title>
    <updated>2025-12-15T14:50:08Z</updated>
    <link href="https://arxiv.org/abs/2512.13399v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13399v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The design of effective reward functions presents a central and often arduous challenge in reinforcement learning (RL), particularly when developing autonomous agents for complex reasoning tasks. While automated reward optimization approaches exist, they typically rely on derivative-free evolutionary heuristics that treat the reward function as a black box, failing to capture the causal relationship between reward structure and task performance. To bridge this gap, we propose Differentiable Evolutionary Reinforcement Learning (DERL), a bilevel framework that enables the autonomous discovery of optimal reward signals. In DERL, a Meta-Optimizer evolves a reward function (i.e., Meta-Reward) by composing structured atomic primitives, guiding the training of an inner-loop policy. Crucially, unlike previous evolution, DERL is differentiable in its metaoptimization: it treats the inner-loop validation performance as a signal to update the Meta-Optimizer via reinforcement learning. This allows DERL to approximate the "meta-gradient" of task success, progressively learning to generate denser and more actionable feedback. We validate DERL across three distinct domains: robotic agent (ALFWorld), scientific simulation (ScienceWorld), and mathematical reasoning (GSM8k, MATH). Experimental results show that DERL achieves state-of-the-art performance on ALFWorld and ScienceWorld, significantly outperforming methods relying on heuristic rewards, especially in out-of-distribution scenarios. Analysis of the evolutionary trajectory demonstrates that DERL successfully captures the intrinsic structure of tasks, enabling selfimproving agent alignment without human intervention.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T14:50:08Z</published>
    <arxiv:comment>Work in Progress. We release our code and model at https://github.com/sitaocheng/DERL</arxiv:comment>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Sitao Cheng</name>
    </author>
    <author>
      <name>Tianle Li</name>
    </author>
    <author>
      <name>Xuhan Huang</name>
    </author>
    <author>
      <name>Xunjian Yin</name>
    </author>
    <author>
      <name>Difan Zou</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13397v1</id>
    <title>rNCA: Self-Repairing Segmentation Masks</title>
    <updated>2025-12-15T14:49:55Z</updated>
    <link href="https://arxiv.org/abs/2512.13397v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13397v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Accurately predicting topologically correct masks remains a difficult task for general segmentation models, which often produce fragmented or disconnected outputs. Fixing these artifacts typically requires hand-crafted refinement rules or architectures specialized to a particular task. Here, we show that Neural Cellular Automata (NCA) can be directly re-purposed as an effective refinement mechanism, using local, iterative updates guided by image context to repair segmentation masks. By training on imperfect masks and ground truths, the automaton learns the structural properties of the target shape while relying solely on local information. When applied to coarse, globally predicted masks, the learned dynamics progressively reconnect broken regions, prune loose fragments and converge towards stable, topologically consistent results. We show how refinement NCA (rNCA) can be easily applied to repair common topological errors produced by different base segmentation models and tasks: for fragmented retinal vessels, it yields 2-3% gains in Dice/clDice and improves Betti errors, reducing $β_0$ errors by 60% and $β_1$ by 20%; for myocardium, it repairs 61.5% of broken cases in a zero-shot setting while lowering ASSD and HD by 19% and 16%, respectively. This showcases NCA as effective and broadly applicable refiners.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T14:49:55Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Malte Silbernagel</name>
    </author>
    <author>
      <name>Albert Alonso</name>
    </author>
    <author>
      <name>Jens Petersen</name>
    </author>
    <author>
      <name>Bulat Ibragimov</name>
    </author>
    <author>
      <name>Marleen de Bruijne</name>
    </author>
    <author>
      <name>Madeleine K. Wyburd</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13392v1</id>
    <title>Beyond the Visible: Disocclusion-Aware Editing via Proxy Dynamic Graphs</title>
    <updated>2025-12-15T14:45:05Z</updated>
    <link href="https://arxiv.org/abs/2512.13392v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13392v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We address image-to-video generation with explicit user control over the final frame's disoccluded regions. Current image-to-video pipelines produce plausible motion but struggle to generate predictable, articulated motions while enforcing user-specified content in newly revealed areas. Our key idea is to separate motion specification from appearance synthesis: we introduce a lightweight, user-editable Proxy Dynamic Graph (PDG) that deterministically yet approximately drives part motion, while a frozen diffusion prior is used to synthesize plausible appearance that follows that motion. In our training-free pipeline, the user loosely annotates and reposes a PDG, from which we compute a dense motion flow to leverage diffusion as a motion-guided shader. We then let the user edit appearance in the disoccluded areas of the image, and exploit the visibility information encoded by the PDG to perform a latent-space composite that reconciles motion with user intent in these areas. This design yields controllable articulation and user control over disocclusions without fine-tuning. We demonstrate clear advantages against state-of-the-art alternatives towards images turned into short videos of articulated objects, furniture, vehicles, and deformables. Our method mixes generative control, in the form of loose pose and structure, with predictable controls, in the form of appearance specification in the final frame in the disoccluded regions, unlocking a new image-to-video workflow. Code will be released on acceptance. Project page: https://anranqi.github.io/beyondvisible.github.io/</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T14:45:05Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Anran Qi</name>
    </author>
    <author>
      <name>Changjian Li</name>
    </author>
    <author>
      <name>Adrien Bousseau</name>
    </author>
    <author>
      <name>Niloy J. Mitra</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13381v1</id>
    <title>Dual-Phase Federated Deep Unlearning via Weight-Aware Rollback and Reconstruction</title>
    <updated>2025-12-15T14:32:47Z</updated>
    <link href="https://arxiv.org/abs/2512.13381v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13381v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Federated Unlearning (FUL) focuses on client data and computing power to offer a privacy-preserving solution. However, high computational demands, complex incentive mechanisms, and disparities in client-side computing power often lead to long times and higher costs. To address these challenges, many existing methods rely on server-side knowledge distillation that solely removes the updates of the target client, overlooking the privacy embedded in the contributions of other clients, which can lead to privacy leakage. In this work, we introduce DPUL, a novel server-side unlearning method that deeply unlearns all influential weights to prevent privacy pitfalls. Our approach comprises three components: (i) identifying high-weight parameters by filtering client update magnitudes, and rolling them back to ensure deep removal. (ii) leveraging the variational autoencoder (VAE) to reconstruct and eliminate low-weight parameters. (iii) utilizing a projection-based technique to recover the model. Experimental results on four datasets demonstrate that DPUL surpasses state-of-the-art baselines, providing a 1%-5% improvement in accuracy and up to 12x reduction in time cost.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T14:32:47Z</published>
    <arxiv:comment>10 pages, submitted to INFOCOM 2026</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Changjun Zhou</name>
    </author>
    <author>
      <name>Jintao Zheng</name>
    </author>
    <author>
      <name>Leyou Yang</name>
    </author>
    <author>
      <name>Pengfei Wang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13376v1</id>
    <title>Unlocking Generalization in Polyp Segmentation with DINO Self-Attention "keys"</title>
    <updated>2025-12-15T14:29:47Z</updated>
    <link href="https://arxiv.org/abs/2512.13376v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13376v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Automatic polyp segmentation is crucial for improving the clinical identification of colorectal cancer (CRC). While Deep Learning (DL) techniques have been extensively researched for this problem, current methods frequently struggle with generalization, particularly in data-constrained or challenging settings. Moreover, many existing polyp segmentation methods rely on complex, task-specific architectures. To address these limitations, we present a framework that leverages the intrinsic robustness of DINO self-attention "key" features for robust segmentation. Unlike traditional methods that extract tokens from the deepest layers of the Vision Transformer (ViT), our approach leverages the key features of the self-attention module with a simple convolutional decoder to predict polyp masks, resulting in enhanced performance and better generalizability. We validate our approach using a multi-center dataset under two rigorous protocols: Domain Generalization (DG) and Extreme Single Domain Generalization (ESDG). Our results, supported by a comprehensive statistical analysis, demonstrate that this pipeline achieves state-of-the-art (SOTA) performance, significantly enhancing generalization, particularly in data-scarce and challenging scenarios. While avoiding a polyp-specific architecture, we surpass well-established models like nnU-Net and UM-Net. Additionally, we provide a systematic benchmark of the DINO framework's evolution, quantifying the specific impact of architectural advancements on downstream polyp segmentation performance.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T14:29:47Z</published>
    <arxiv:comment>29 pages, 10 figures, 8 tables, under review at MIDL 2026</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Carla Monteiro</name>
    </author>
    <author>
      <name>Valentina Corbetta</name>
    </author>
    <author>
      <name>Regina Beets-Tan</name>
    </author>
    <author>
      <name>Luís F. Teixeira</name>
    </author>
    <author>
      <name>Wilson Silva</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13374v1</id>
    <title>Behavior and Representation in Large Language Models for Combinatorial Optimization: From Feature Extraction to Algorithm Selection</title>
    <updated>2025-12-15T14:28:35Z</updated>
    <link href="https://arxiv.org/abs/2512.13374v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13374v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Recent advances in Large Language Models (LLMs) have opened new perspectives for automation in optimization. While several studies have explored how LLMs can generate or solve optimization models, far less is understood about what these models actually learn regarding problem structure or algorithmic behavior. This study investigates how LLMs internally represent combinatorial optimization problems and whether such representations can support downstream decision tasks. We adopt a twofold methodology combining direct querying, which assesses LLM capacity to explicitly extract instance features, with probing analyses that examine whether such information is implicitly encoded within their hidden layers. The probing framework is further extended to a per-instance algorithm selection task, evaluating whether LLM-derived representations can predict the best-performing solver. Experiments span four benchmark problems and three instance representations. Results show that LLMs exhibit moderate ability to recover feature information from problem instances, either through direct querying or probing. Notably, the predictive power of LLM hidden-layer representations proves comparable to that achieved through traditional feature extraction, suggesting that LLMs capture meaningful structural information relevant to optimization performance.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T14:28:35Z</published>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Francesca Da Ros</name>
    </author>
    <author>
      <name>Luca Di Gaspero</name>
    </author>
    <author>
      <name>Kevin Roitero</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13363v1</id>
    <title>Detecting Emotion Drift in Mental Health Text Using Pre-Trained Transformers</title>
    <updated>2025-12-15T14:18:12Z</updated>
    <link href="https://arxiv.org/abs/2512.13363v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13363v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>This study investigates emotion drift: the change in emotional state across a single text, within mental health-related messages. While sentiment analysis typically classifies an entire message as positive, negative, or neutral, the nuanced shift of emotions over the course of a message is often overlooked. This study detects sentence-level emotions and measures emotion drift scores using pre-trained transformer models such as DistilBERT and RoBERTa. The results provide insights into patterns of emotional escalation or relief in mental health conversations. This methodology can be applied to better understand emotional dynamics in content.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T14:18:12Z</published>
    <arxiv:comment>14 pages, 12 figures</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Shibani Sankpal</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13361v1</id>
    <title>Automated User Identification from Facial Thermograms with Siamese Networks</title>
    <updated>2025-12-15T14:13:49Z</updated>
    <link href="https://arxiv.org/abs/2512.13361v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13361v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The article analyzes the use of thermal imaging technologies for biometric identification based on facial thermograms. It presents a comparative analysis of infrared spectral ranges (NIR, SWIR, MWIR, and LWIR). The paper also defines key requirements for thermal cameras used in biometric systems, including sensor resolution, thermal sensitivity, and a frame rate of at least 30 Hz. Siamese neural networks are proposed as an effective approach for automating the identification process. In experiments conducted on a proprietary dataset, the proposed method achieved an accuracy of approximately 80%. The study also examines the potential of hybrid systems that combine visible and infrared spectra to overcome the limitations of individual modalities. The results indicate that thermal imaging is a promising technology for developing reliable security systems.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T14:13:49Z</published>
    <arxiv:comment>5 pages, 2 figures, reported on 21st International Scientific and Practical Conference 'Electronic Means and Control Systems', dedicated to the 80th anniversary of radio engineering education beyond the Urals, Tomsk, 24 November 2025</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Elizaveta Prozorova</name>
    </author>
    <author>
      <name>Anton Konev</name>
    </author>
    <author>
      <name>Vladimir Faerman</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13360v1</id>
    <title>UCRBench: Benchmarking LLMs on Use Case Recovery</title>
    <updated>2025-12-15T14:12:57Z</updated>
    <link href="https://arxiv.org/abs/2512.13360v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13360v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Use cases are widely employed to specify functional requirements, yet existing benchmarks are scarce and face the risk of being misaligned with actual system behavior, similarly limiting the rigorous evaluation of large language models (LLMs) in generating use cases from source code. We address this gap by introducing code-aligned use case benchmarks, constructed through manual validation of both user-goal and subfunction use cases across nine real-world software projects. Using this benchmark, we conduct the first systematic study of LLMs and propose a hierarchical evaluation protocol that assesses actor correctness, name accuracy, path fidelity, and behavioral coverage. The results show that while LLMs can partially reconstruct system functionality, their performance varies significantly across projects, with particularly noticeable shortcomings in domain-specific and multi-module systems. The models also exhibit high omission rates and struggle to maintain consistent abstraction when aggregating subfunctions into user-goal use cases, highlighting both the potential and current limitations of LLM-based use case reverse engineering.</summary>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T14:12:57Z</published>
    <arxiv:primary_category term="cs.SE"/>
    <author>
      <name>Shuyuan Xiao</name>
    </author>
    <author>
      <name>Yiran Zhang</name>
    </author>
    <author>
      <name>Weisong Sun</name>
    </author>
    <author>
      <name>Xiaohong Chen</name>
    </author>
    <author>
      <name>Yang Liu</name>
    </author>
    <author>
      <name>Zhi Jin</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13359v1</id>
    <title>Fast Policy Learning for 6-DOF Position Control of Underwater Vehicles</title>
    <updated>2025-12-15T14:12:32Z</updated>
    <link href="https://arxiv.org/abs/2512.13359v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13359v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Autonomous Underwater Vehicles (AUVs) require reliable six-degree-of-freedom (6-DOF) position control to operate effectively in complex and dynamic marine environments. Traditional controllers are effective under nominal conditions but exhibit degraded performance when faced with unmodeled dynamics or environmental disturbances. Reinforcement learning (RL) provides a powerful alternative but training is typically slow and sim-to-real transfer remains challenging. This work introduces a GPU-accelerated RL training pipeline built in JAX and MuJoCo-XLA (MJX). By jointly JIT-compiling large-scale parallel physics simulation and learning updates, we achieve training times of under two minutes.Through systematic evaluation of multiple RL algorithms, we show robust 6-DOF trajectory tracking and effective disturbance rejection in real underwater experiments, with policies transferred zero-shot from simulation. Our results provide the first explicit real-world demonstration of RL-based AUV position control across all six degrees of freedom.</summary>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T14:12:32Z</published>
    <arxiv:primary_category term="cs.RO"/>
    <author>
      <name>Sümer Tunçay</name>
    </author>
    <author>
      <name>Alain Andres</name>
    </author>
    <author>
      <name>Ignacio Carlucho</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13356v1</id>
    <title>Control of a Twin Rotor using Twin Delayed Deep Deterministic Policy Gradient (TD3)</title>
    <updated>2025-12-15T14:10:04Z</updated>
    <link href="https://arxiv.org/abs/2512.13356v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13356v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>This paper proposes a reinforcement learning (RL) framework for controlling and stabilizing the Twin Rotor Aerodynamic System (TRAS) at specific pitch and azimuth angles and tracking a given trajectory. The complex dynamics and non-linear characteristics of the TRAS make it challenging to control using traditional control algorithms. However, recent developments in RL have attracted interest due to their potential applications in the control of multirotors. The Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm was used in this paper to train the RL agent. This algorithm is used for environments with continuous state and action spaces, similar to the TRAS, as it does not require a model of the system. The simulation results illustrated the effectiveness of the RL control method. Next, external disturbances in the form of wind disturbances were used to test the controller's effectiveness compared to conventional PID controllers. Lastly, experiments on a laboratory setup were carried out to confirm the controller's effectiveness in real-world applications.</summary>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T14:10:04Z</published>
    <arxiv:comment>This is the Author Accepted Manuscript version of a paper accepted for publication. The final published version is available via IEEE Xplore</arxiv:comment>
    <arxiv:primary_category term="cs.RO"/>
    <arxiv:journal_ref>2024 28th IEEE International Conference on System Theory, Control and Computing (ICSTCC)</arxiv:journal_ref>
    <author>
      <name>Zeyad Gamal</name>
    </author>
    <author>
      <name>Youssef Mahran</name>
    </author>
    <author>
      <name>Ayman El-Badawy</name>
    </author>
    <arxiv:doi>10.1109/ICSTCC62912.2024.10744717</arxiv:doi>
    <link rel="related" href="https://doi.org/10.1109/ICSTCC62912.2024.10744717" title="doi"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13352v1</id>
    <title>On the Effectiveness of Membership Inference in Targeted Data Extraction from Large Language Models</title>
    <updated>2025-12-15T14:05:49Z</updated>
    <link href="https://arxiv.org/abs/2512.13352v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13352v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Large Language Models (LLMs) are prone to memorizing training data, which poses serious privacy risks. Two of the most prominent concerns are training data extraction and Membership Inference Attacks (MIAs). Prior research has shown that these threats are interconnected: adversaries can extract training data from an LLM by querying the model to generate a large volume of text and subsequently applying MIAs to verify whether a particular data point was included in the training set. In this study, we integrate multiple MIA techniques into the data extraction pipeline to systematically benchmark their effectiveness. We then compare their performance in this integrated setting against results from conventional MIA benchmarks, allowing us to evaluate their practical utility in real-world extraction scenarios.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T14:05:49Z</published>
    <arxiv:comment>Accepted to IEEE Conference on Secure and Trustworthy Machine Learning (SaTML) 2026</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Ali Al Sahili</name>
    </author>
    <author>
      <name>Ali Chehab</name>
    </author>
    <author>
      <name>Razane Tajeddine</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13340v1</id>
    <title>Link-Aware Energy-Frugal Continual Learning for Fault Detection in IoT Networks</title>
    <updated>2025-12-15T13:54:38Z</updated>
    <link href="https://arxiv.org/abs/2512.13340v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13340v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The use of lightweight machine learning (ML) models in internet of things (IoT) networks enables resource constrained IoT devices to perform on-device inference for several critical applications. However, the inference accuracy deteriorates due to the non-stationarity in the IoT environment and limited initial training data. To counteract this, the deployed models can be updated occasionally with new observed data samples. However, this approach consumes additional energy, which is undesirable for energy constrained IoT devices. This letter introduces an event-driven communication framework that strategically integrates continual learning (CL) in IoT networks for energy-efficient fault detection. Our framework enables the IoT device and the edge server (ES) to collaboratively update the lightweight ML model by adapting to the wireless link conditions for communication and the available energy budget. Evaluation on real-world datasets show that the proposed approach can outperform both periodic sampling and non-adaptive CL in terms of inference recall; our proposed approach achieves up to a 42.8% improvement, even under tight energy and bandwidth constraint.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T13:54:38Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Henrik C. M. Frederiksen</name>
    </author>
    <author>
      <name>Junya Shiraishi</name>
    </author>
    <author>
      <name>Cedomir Stefanovic</name>
    </author>
    <author>
      <name>Hei Victor Cheng</name>
    </author>
    <author>
      <name>Shashi Raj Pandey</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13337v1</id>
    <title>FROC: A Unified Framework with Risk-Optimized Control for Machine Unlearning in LLMs</title>
    <updated>2025-12-15T13:53:12Z</updated>
    <link href="https://arxiv.org/abs/2512.13337v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13337v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Machine unlearning (MU) seeks to eliminate the influence of specific training examples from deployed models. As large language models (LLMs) become widely used, managing risks arising from insufficient forgetting or utility loss is increasingly crucial. Current MU techniques lack effective mechanisms for evaluating and controlling these risks, hindering the selection of strategies that appropriately balance safety and utility, and raising trust concerns surrounding the "right to be forgotten." To address these issues, we propose FROC, a unified framework with Risk-Optimized Control for machine unlearning in LLMs. FROC is built around a conformal-style risk-control formulation that expresses a user-specified risk budget on unlearning behavior. This probability-based constraint enables FROC to compare MU strategies, identify feasible operating regions, and guide hyperparameter selection according to desired trade-offs between forgetting sufficiency and utility preservation. To operationalize this constraint, FROC introduces a smoothly varying continuous risk model that aggregates forgetting deficiency and utility degradation into a single configuration-level score. Building on conformal risk analysis, FROC computes (1) the Conformal Unlearning Risk (CUR), a data-driven estimated value on the probability that forgotten samples continue to influence model predictions, and (2) risk-controlled configuration sets, which identify unlearning hyperparameters that are valid under the specified risk budget. Experiments across multiple LLM MU methods demonstrate that FROC produces stable, interpretable risk landscapes and reveals consistent relationships between unlearning configurations, semantic shift, and utility impact. FROC reframes MU as a controllable, risk-aware process and offers a practical foundation for managing unlearning behavior in large-scale LLM deployments.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T13:53:12Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Si Qi Goh</name>
    </author>
    <author>
      <name>Yongsen Zheng</name>
    </author>
    <author>
      <name>Ziyao Liu</name>
    </author>
    <author>
      <name>Sami Hormi</name>
    </author>
    <author>
      <name>Kwok-Yan Lam</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13336v1</id>
    <title>KD-PINN: Knowledge-Distilled PINNs for ultra-low-latency real-time neural PDE solvers</title>
    <updated>2025-12-15T13:51:20Z</updated>
    <link href="https://arxiv.org/abs/2512.13336v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13336v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>This work introduces Knowledge-Distilled Physics-Informed Neural Networks (KD-PINN), a framework that transfers the predictive accuracy of a high-capacity teacher model to a compact student through a continuous adaptation of the Kullback-Leibler divergence. To confirm its generality for various dynamics and dimensionalities, the framework is evaluated on a representative set of partial differential equations (PDEs). In all tested cases, the student model preserved the teacher's physical accuracy, with a mean RMSE increase below 0.64%, and achieved inference speedups ranging from 4.8x (Navier-Stokes) to 6.9x (Burgers). The distillation process also revealed a regularizing effect. With an average inference latency of 5.3 ms on CPU, the distilled models enter the ultra-low-latency real-time regime defined by sub-10 ms performance. Finally, this study examines how knowledge distillation reduces inference latency in PINNs to contribute to the development of accurate ultra-low-latency neural PDE solvers.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T13:51:20Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Karim Bounja</name>
    </author>
    <author>
      <name>Lahcen Laayouni</name>
    </author>
    <author>
      <name>Abdeljalil Sakat</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13333v1</id>
    <title>Quantum Disruption: An SOK of How Post-Quantum Attackers Reshape Blockchain Security and Performance</title>
    <updated>2025-12-15T13:48:14Z</updated>
    <link href="https://arxiv.org/abs/2512.13333v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13333v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>As quantum computing advances toward practical deployment, it threatens a wide range of classical cryptographic mechanisms, including digital signatures, key exchange protocols, public-key encryption, and certain hash-based constructions that underpin modern network infrastructures. These primitives form the security backbone of most blockchain platforms, raising serious concerns about the long-term viability of blockchain systems in a post-quantum world. Although migrating to post-quantum cryptography may appear straightforward, the substantially larger key sizes and higher computational costs of post-quantum primitives can introduce significant challenges and, in some cases, render such transitions impractical for blockchain environments.
  In this paper, we examine the implications of adopting post-quantum cryptography in blockchain systems across four key dimensions. We begin by identifying the cryptographic primitives within blockchain architectures that are most vulnerable to quantum attacks, particularly those used in consensus mechanisms, identity management, and transaction validation. We then survey proposed post-quantum adaptations across existing blockchain designs, analyzing their feasibility within decentralized and resource-constrained settings. Building on this analysis, we evaluate how replacing classical primitives with post-quantum alternatives affects system performance, protocol dynamics, and the incentive and trust structures that sustain blockchain ecosystems. Our study demonstrates that integrating post-quantum signature schemes into blockchain systems is not a simple drop-in replacement; instead, it requires careful architectural redesign, as naive substitutions risk undermining both security guarantees and operational efficiency.</summary>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T13:48:14Z</published>
    <arxiv:primary_category term="cs.CR"/>
    <author>
      <name>Tushin Mallick</name>
    </author>
    <author>
      <name>Maya Zeldin</name>
    </author>
    <author>
      <name>Murat Cenk</name>
    </author>
    <author>
      <name>Cristina Nita-Rotaru</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13330v1</id>
    <title>FIN-bench-v2: A Unified and Robust Benchmark Suite for Evaluating Finnish Large Language Models</title>
    <updated>2025-12-15T13:41:41Z</updated>
    <link href="https://arxiv.org/abs/2512.13330v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13330v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We introduce FIN-bench-v2, a unified benchmark suite for evaluating large language models in Finnish. FIN-bench-v2 consolidates Finnish versions of widely used benchmarks together with an updated and expanded version of the original FIN-bench into a single, consistently formatted collection, covering multiple-choice and generative tasks across reading comprehension, commonsense reasoning, sentiment analysis, world knowledge, and alignment. All datasets are converted to HuggingFace Datasets, which include both cloze and multiple-choice prompt formulations with five variants per task, and we incorporate human annotation or review for machine-translated resources such as GoldenSwag and XED. To select robust tasks, we pretrain a set of 2.15B-parameter decoder-only models and use their learning curves to compute monotonicity, signal-to-noise, non-random performance, and model ordering consistency, retaining only tasks that satisfy all criteria. We further evaluate a set of larger instruction-tuned models to characterize performance across tasks and prompt formulations. All datasets, prompts, and evaluation configurations are publicly available via our fork of the Language Model Evaluation Harness at https://github.com/LumiOpen/lm-evaluation-harness. Supplementary resources are released in a separate repository at https://github.com/TurkuNLP/FIN-bench-v2.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T13:41:41Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Joona Kytöniemi</name>
    </author>
    <author>
      <name>Jousia Piha</name>
    </author>
    <author>
      <name>Akseli Reunamo</name>
    </author>
    <author>
      <name>Fedor Vitiugin</name>
    </author>
    <author>
      <name>Farrokh Mehryary</name>
    </author>
    <author>
      <name>Sampo Pyysalo</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13325v1</id>
    <title>Security and Detectability Analysis of Unicode Text Watermarking Methods Against Large Language Models</title>
    <updated>2025-12-15T13:40:00Z</updated>
    <link href="https://arxiv.org/abs/2512.13325v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13325v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Securing digital text is becoming increasingly relevant due to the widespread use of large language models. Individuals' fear of losing control over data when it is being used to train such machine learning models or when distinguishing model-generated output from text written by humans. Digital watermarking provides additional protection by embedding an invisible watermark within the data that requires protection. However, little work has been taken to analyze and verify if existing digital text watermarking methods are secure and undetectable by large language models. In this paper, we investigate the security-related area of watermarking and machine learning models for text data. In a controlled testbed of three experiments, ten existing Unicode text watermarking methods were implemented and analyzed across six large language models: GPT-5, GPT-4o, Teuken 7B, Llama 3.3, Claude Sonnet 4, and Gemini 2.5 Pro. The findings of our experiments indicate that, especially the latest reasoning models, can detect a watermarked text. Nevertheless, all models fail to extract the watermark unless implementation details in the form of source code are provided. We discuss the implications for security researchers and practitioners and outline future research opportunities to address security concerns.</summary>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T13:40:00Z</published>
    <arxiv:comment>Accepted for publication at the ICISSP 2026</arxiv:comment>
    <arxiv:primary_category term="cs.CR"/>
    <author>
      <name>Malte Hellmeier</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13323v1</id>
    <title>Error-Driven Prompt Optimization for Arithmetic Reasoning</title>
    <updated>2025-12-15T13:39:14Z</updated>
    <link href="https://arxiv.org/abs/2512.13323v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13323v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Recent advancements in artificial intelligence have sparked interest in industrial agents capable of supporting analysts in regulated sectors, such as finance and healthcare, within tabular data workflows. A key capability for such systems is performing accurate arithmetic operations on structured data while ensuring sensitive information never leaves secure, on-premises environments. Here, we introduce an error-driven optimization framework for arithmetic reasoning that enhances a Code Generation Agent (CGA), specifically applied to on-premises small language models (SLMs). Through a systematic evaluation of a leading SLM (Qwen3 4B), we find that while the base model exhibits fundamental limitations in arithmetic tasks, our proposed error-driven method, which clusters erroneous predictions to refine prompt-rules iteratively, dramatically improves performance, elevating the model's accuracy to 70.8\%. Our results suggest that developing reliable, interpretable, and industrially deployable AI assistants can be achieved not only through costly fine-tuning but also via systematic, error-driven prompt optimization, enabling small models to surpass larger language models (GPT-3.5 Turbo) in a privacy-compliant manner.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T13:39:14Z</published>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Árpád Pándy</name>
    </author>
    <author>
      <name>Róbert Lakatos</name>
    </author>
    <author>
      <name>András Hajdu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13319v1</id>
    <title>Temporal parallelisation of continuous-time maximum-a-posteriori trajectory estimation</title>
    <updated>2025-12-15T13:37:14Z</updated>
    <link href="https://arxiv.org/abs/2512.13319v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13319v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>This paper proposes a parallel-in-time method for computing continuous-time maximum-a-posteriori (MAP) trajectory estimates of the states of partially observed stochastic differential equations (SDEs), with the goal of improving computational speed on parallel architectures. The MAP estimation problem is reformulated as a continuous-time optimal control problem based on the Onsager-Machlup functional. This reformulation enables the use of a previously proposed parallel-in-time solution for optimal control problems, which we adapt to the current problem. The structure of the resulting optimal control problem admits a parallel solution based on parallel associative scan algorithms. In the linear Gaussian special case, it yields a parallel Kalman-Bucy filter and a parallel continuous-time Rauch-Tung-Striebel smoother. These linear computational methods are further extended to nonlinear continuous-time state-space models through Taylor expansions. We also present the corresponding parallel two-filter smoother. The graphics processing unit (GPU) experiments on linear and nonlinear models demonstrate that the proposed framework achieves a significant speedup in computations while maintaining the accuracy of sequential algorithms.</summary>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T13:37:14Z</published>
    <arxiv:primary_category term="cs.DC"/>
    <author>
      <name>Hassan Razavi</name>
    </author>
    <author>
      <name>Ángel F. García-Fernández</name>
    </author>
    <author>
      <name>Simo Särkkä</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13317v1</id>
    <title>Face Identity Unlearning for Retrieval via Embedding Dispersion</title>
    <updated>2025-12-15T13:35:28Z</updated>
    <link href="https://arxiv.org/abs/2512.13317v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13317v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Face recognition systems rely on learning highly discriminative and compact identity clusters to enable accurate retrieval. However, as with other surveillance-oriented technologies, such systems raise serious privacy concerns due to their potential for unauthorized identity tracking. While several works have explored machine unlearning as a means of privacy protection, their applicability to face retrieval - especially for modern embedding-based recognition models - remains largely unexplored. In this work, we study the problem of face identity unlearning for retrieval systems and present its inherent challenges. The goal is to make selected identities unretrievable by dispersing their embeddings on the hypersphere and preventing the formation of compact identity clusters that enable re-identification in the gallery. The primary challenge is to achieve this forgetting effect while preserving the discriminative structure of the embedding space and the retrieval performance of the model for the remaining identities. To address this, we evaluate several existing approximate class unlearning methods (e.g., Random Labeling, Gradient Ascent, Boundary Unlearning, and other recent approaches) in the context of face retrieval and propose a simple yet effective dispersion-based unlearning approach. Extensive experiments on standard benchmarks (VGGFace2, CelebA) demonstrate that our method achieves superior forgetting behavior while preserving retrieval utility.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T13:35:28Z</published>
    <arxiv:comment>12 pages, 1 figure, 5 tables, 10 equations. Preprint</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Mikhail Zakharov</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13316v1</id>
    <title>ALIGN-FL: Architecture-independent Learning through Invariant Generative component sharing in Federated Learning</title>
    <updated>2025-12-15T13:35:27Z</updated>
    <link href="https://arxiv.org/abs/2512.13316v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13316v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We present ALIGN-FL, a novel approach to distributed learning that addresses the challenge of learning from highly disjoint data distributions through selective sharing of generative components. Instead of exchanging full model parameters, our framework enables privacy-preserving learning by transferring only generative capabilities across clients, while the server performs global training using synthetic samples. Through complementary privacy mechanisms: DP-SGD with adaptive clipping and Lipschitz regularized VAE decoders and a stateful architecture supporting heterogeneous clients, we experimentally validate our approach on MNIST and Fashion-MNIST datasets with cross-domain outliers. Our analysis demonstrates that both privacy mechanisms effectively map sensitive outliers to typical data points while maintaining utility in extreme Non-IID scenarios typical of cross-silo collaborations.
  Index Terms: Client-invariant Learning, Federated Learning (FL), Privacy-preserving Generative Models, Non-Independent and Identically Distributed (Non-IID), Heterogeneous Architectures</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T13:35:27Z</published>
    <arxiv:comment>Accepted at 2025 International Conference on Cyber-Enabled Distributed Computing and Knowledge Discovery (CyberC)</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Mayank Gulati</name>
    </author>
    <author>
      <name>Benedikt Groß</name>
    </author>
    <author>
      <name>Gerhard Wunder</name>
    </author>
    <arxiv:doi>10.1109/CyberC66434.2025.00025</arxiv:doi>
    <link rel="related" href="https://doi.org/10.1109/CyberC66434.2025.00025" title="doi"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13313v1</id>
    <title>KlingAvatar 2.0 Technical Report</title>
    <updated>2025-12-15T13:30:51Z</updated>
    <link href="https://arxiv.org/abs/2512.13313v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13313v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Avatar video generation models have achieved remarkable progress in recent years. However, prior work exhibits limited efficiency in generating long-duration high-resolution videos, suffering from temporal drifting, quality degradation, and weak prompt following as video length increases. To address these challenges, we propose KlingAvatar 2.0, a spatio-temporal cascade framework that performs upscaling in both spatial resolution and temporal dimension. The framework first generates low-resolution blueprint video keyframes that capture global semantics and motion, and then refines them into high-resolution, temporally coherent sub-clips using a first-last frame strategy, while retaining smooth temporal transitions in long-form videos. To enhance cross-modal instruction fusion and alignment in extended videos, we introduce a Co-Reasoning Director composed of three modality-specific large language model (LLM) experts. These experts reason about modality priorities and infer underlying user intent, converting inputs into detailed storylines through multi-turn dialogue. A Negative Director further refines negative prompts to improve instruction alignment. Building on these components, we extend the framework to support ID-specific multi-character control. Extensive experiments demonstrate that our model effectively addresses the challenges of efficient, multimodally aligned long-form high-resolution video generation, delivering enhanced visual clarity, realistic lip-teeth rendering with accurate lip synchronization, strong identity preservation, and coherent multimodal instruction following.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T13:30:51Z</published>
    <arxiv:comment>14 pages, 7 figures</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name> Kling Team</name>
    </author>
    <author>
      <name>Jialu Chen</name>
    </author>
    <author>
      <name>Yikang Ding</name>
    </author>
    <author>
      <name>Zhixue Fang</name>
    </author>
    <author>
      <name>Kun Gai</name>
    </author>
    <author>
      <name>Yuan Gao</name>
    </author>
    <author>
      <name>Kang He</name>
    </author>
    <author>
      <name>Jingyun Hua</name>
    </author>
    <author>
      <name>Boyuan Jiang</name>
    </author>
    <author>
      <name>Mingming Lao</name>
    </author>
    <author>
      <name>Xiaohan Li</name>
    </author>
    <author>
      <name>Hui Liu</name>
    </author>
    <author>
      <name>Jiwen Liu</name>
    </author>
    <author>
      <name>Xiaoqiang Liu</name>
    </author>
    <author>
      <name>Yuan Liu</name>
    </author>
    <author>
      <name>Shun Lu</name>
    </author>
    <author>
      <name>Yongsen Mao</name>
    </author>
    <author>
      <name>Yingchao Shao</name>
    </author>
    <author>
      <name>Huafeng Shi</name>
    </author>
    <author>
      <name>Xiaoyu Shi</name>
    </author>
    <author>
      <name>Peiqin Sun</name>
    </author>
    <author>
      <name>Songlin Tang</name>
    </author>
    <author>
      <name>Pengfei Wan</name>
    </author>
    <author>
      <name>Chao Wang</name>
    </author>
    <author>
      <name>Xuebo Wang</name>
    </author>
    <author>
      <name>Haoxian Zhang</name>
    </author>
    <author>
      <name>Yuanxing Zhang</name>
    </author>
    <author>
      <name>Yan Zhou</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13303v1</id>
    <title>ShowTable: Unlocking Creative Table Visualization with Collaborative Reflection and Refinement</title>
    <updated>2025-12-15T13:21:50Z</updated>
    <link href="https://arxiv.org/abs/2512.13303v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13303v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>While existing generation and unified models excel at general image generation, they struggle with tasks requiring deep reasoning, planning, and precise data-to-visual mapping abilities beyond general scenarios. To push beyond the existing limitations, we introduce a new and challenging task: creative table visualization, requiring the model to generate an infographic that faithfully and aesthetically visualizes the data from a given table. To address this challenge, we propose ShowTable, a pipeline that synergizes MLLMs with diffusion models via a progressive self-correcting process. The MLLM acts as the central orchestrator for reasoning the visual plan and judging visual errors to provide refined instructions, the diffusion execute the commands from MLLM, achieving high-fidelity results. To support this task and our pipeline, we introduce three automated data construction pipelines for training different modules. Furthermore, we introduce TableVisBench, a new benchmark with 800 challenging instances across 5 evaluation dimensions, to assess performance on this task. Experiments demonstrate that our pipeline, instantiated with different models, significantly outperforms baselines, highlighting its effective multi-modal reasoning, generation, and error correction capabilities.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T13:21:50Z</published>
    <arxiv:comment>project page: https://lntzm.github.io/showtable-page/</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Zhihang Liu</name>
    </author>
    <author>
      <name>Xiaoyi Bao</name>
    </author>
    <author>
      <name>Pandeng Li</name>
    </author>
    <author>
      <name>Junjie Zhou</name>
    </author>
    <author>
      <name>Zhaohe Liao</name>
    </author>
    <author>
      <name>Yefei He</name>
    </author>
    <author>
      <name>Kaixun Jiang</name>
    </author>
    <author>
      <name>Chen-Wei Xie</name>
    </author>
    <author>
      <name>Yun Zheng</name>
    </author>
    <author>
      <name>Hongtao Xie</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13300v1</id>
    <title>No One Left Behind: How to Exploit the Incomplete and Skewed Multi-Label Data for Conversion Rate Prediction</title>
    <updated>2025-12-15T13:14:20Z</updated>
    <link href="https://arxiv.org/abs/2512.13300v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13300v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>In most real-world online advertising systems, advertisers typically have diverse customer acquisition goals. A common solution is to use multi-task learning (MTL) to train a unified model on post-click data to estimate the conversion rate (CVR) for these diverse targets. In practice, CVR prediction often encounters missing conversion data as many advertisers submit only a subset of user conversion actions due to privacy or other constraints, making the labels of multi-task data incomplete. If the model is trained on all available samples where advertisers submit user conversion actions, it may struggle when deployed to serve a subset of advertisers targeting specific conversion actions, as the training and deployment data distributions are mismatched. While considerable MTL efforts have been made, a long-standing challenge is how to effectively train a unified model with the incomplete and skewed multi-label data. In this paper, we propose a fine-grained Knowledge transfer framework for Asymmetric Multi-Label data (KAML). We introduce an attribution-driven masking strategy (ADM) to better utilize data with asymmetric multi-label data in training. However, the more relaxed masking in ADM is a double-edged sword: it provides additional training signals but also introduces noise due to skewed data. To address this, we propose a hierarchical knowledge extraction mechanism (HKE) to model the sample discrepancy within the target task tower. Finally, to maximize the utility of unlabeled samples, we incorporate ranking loss strategy to further enhance our model. The effectiveness of KAML has been demonstrated through comprehensive evaluations on offline industry datasets and online A/B tests, which show significant performance improvements over existing MTL baselines.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T13:14:20Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Qinglin Jia</name>
    </author>
    <author>
      <name>Zhaocheng Du</name>
    </author>
    <author>
      <name>Chuhan Wu</name>
    </author>
    <author>
      <name>Huifeng Guo</name>
    </author>
    <author>
      <name>Ruiming Tang</name>
    </author>
    <author>
      <name>Shuting Shi</name>
    </author>
    <author>
      <name>Muyu Zhang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13298v1</id>
    <title>MiniLingua: A Small Open-Source LLM for European Languages</title>
    <updated>2025-12-15T13:12:42Z</updated>
    <link href="https://arxiv.org/abs/2512.13298v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13298v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Large language models are powerful but often limited by high computational cost, privacy concerns, and English-centric training. Recent progress demonstrates that small, efficient models with around one billion parameters can deliver strong results and enable on-device use. This paper introduces MiniLingua, a multilingual open-source LLM of one billion parameters trained from scratch for 13 European languages, designed to balance coverage and instruction-following capabilities. Based on evaluation results, the instruction-tuned version of MiniLingua outperforms EuroLLM, a model with a similar training approach but a larger training budget, on summarization, classification and both open- and closed-book question answering. Moreover, it remains competitive with more advanced state-of-the-art models on open-ended generation tasks. We release model weights, tokenizer and source code used for data processing and model training.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T13:12:42Z</published>
    <arxiv:comment>9+6 pages, 6 figures and 3 tables in the main text. Code at https://github.com/MiniLingua-ai/training_artifacts</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Anna Aksenova</name>
    </author>
    <author>
      <name>Boris Zverkov</name>
    </author>
    <author>
      <name>Nicola Dainese</name>
    </author>
    <author>
      <name>Alexander Nikitin</name>
    </author>
    <author>
      <name>Pekka Marttinen</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13297v1</id>
    <title>MedInsightBench: Evaluating Medical Analytics Agents Through Multi-Step Insight Discovery in Multimodal Medical Data</title>
    <updated>2025-12-15T13:10:42Z</updated>
    <link href="https://arxiv.org/abs/2512.13297v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13297v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>In medical data analysis, extracting deep insights from complex, multi-modal datasets is essential for improving patient care, increasing diagnostic accuracy, and optimizing healthcare operations. However, there is currently a lack of high-quality datasets specifically designed to evaluate the ability of large multi-modal models (LMMs) to discover medical insights. In this paper, we introduce MedInsightBench, the first benchmark that comprises 332 carefully curated medical cases, each annotated with thoughtfully designed insights. This benchmark is intended to evaluate the ability of LMMs and agent frameworks to analyze multi-modal medical image data, including posing relevant questions, interpreting complex findings, and synthesizing actionable insights and recommendations. Our analysis indicates that existing LMMs exhibit limited performance on MedInsightBench, which is primarily attributed to their challenges in extracting multi-step, deep insights and the absence of medical expertise. Therefore, we propose MedInsightAgent, an automated agent framework for medical data analysis, composed of three modules: Visual Root Finder, Analytical Insight Agent, and Follow-up Question Composer. Experiments on MedInsightBench highlight pervasive challenges and demonstrate that MedInsightAgent can improve the performance of general LMMs in medical data insight discovery.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T13:10:42Z</published>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Zhenghao Zhu</name>
    </author>
    <author>
      <name>Chuxue Cao</name>
    </author>
    <author>
      <name>Sirui Han</name>
    </author>
    <author>
      <name>Yuanfeng Song</name>
    </author>
    <author>
      <name>Xing Chen</name>
    </author>
    <author>
      <name>Caleb Chen Cao</name>
    </author>
    <author>
      <name>Yike Guo</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13293v1</id>
    <title>Intrinsic-Motivation Multi-Robot Social Formation Navigation with Coordinated Exploration</title>
    <updated>2025-12-15T13:03:08Z</updated>
    <link href="https://arxiv.org/abs/2512.13293v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13293v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>This paper investigates the application of reinforcement learning (RL) to multi-robot social formation navigation, a critical capability for enabling seamless human-robot coexistence. While RL offers a promising paradigm, the inherent unpredictability and often uncooperative dynamics of pedestrian behavior pose substantial challenges, particularly concerning the efficiency of coordinated exploration among robots. To address this, we propose a novel coordinated-exploration multi-robot RL algorithm introducing an intrinsic motivation exploration. Its core component is a self-learning intrinsic reward mechanism designed to collectively alleviate policy conservatism. Moreover, this algorithm incorporates a dual-sampling mode within the centralized training and decentralized execution framework to enhance the representation of both the navigation policy and the intrinsic reward, leveraging a two-time-scale update rule to decouple parameter updates. Empirical results on social formation navigation benchmarks demonstrate the proposed algorithm's superior performance over existing state-of-the-art methods across crucial metrics. Our code and video demos are available at: https://github.com/czxhunzi/CEMRRL.</summary>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T13:03:08Z</published>
    <arxiv:primary_category term="cs.RO"/>
    <author>
      <name>Hao Fua</name>
    </author>
    <author>
      <name>Wei Liu</name>
    </author>
    <author>
      <name>Shuai Zhoua</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13290v1</id>
    <title>LINA: Learning INterventions Adaptively for Physical Alignment and Generalization in Diffusion Models</title>
    <updated>2025-12-15T12:59:59Z</updated>
    <link href="https://arxiv.org/abs/2512.13290v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13290v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Diffusion models (DMs) have achieved remarkable success in image and video generation. However, they still struggle with (1) physical alignment and (2) out-of-distribution (OOD) instruction following. We argue that these issues stem from the models' failure to learn causal directions and to disentangle causal factors for novel recombination. We introduce the Causal Scene Graph (CSG) and the Physical Alignment Probe (PAP) dataset to enable diagnostic interventions. This analysis yields three key insights. First, DMs struggle with multi-hop reasoning for elements not explicitly determined in the prompt. Second, the prompt embedding contains disentangled representations for texture and physics. Third, visual causal structure is disproportionately established during the initial, computationally limited denoising steps. Based on these findings, we introduce LINA (Learning INterventions Adaptively), a novel framework that learns to predict prompt-specific interventions, which employs (1) targeted guidance in the prompt and visual latent spaces, and (2) a reallocated, causality-aware denoising schedule. Our approach enforces both physical alignment and OOD instruction following in image and video DMs, achieving state-of-the-art performance on challenging causal generation tasks and the Winoground dataset. Our project page is at https://opencausalab.github.io/LINA.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T12:59:59Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Shu Yu</name>
    </author>
    <author>
      <name>Chaochao Lu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13286v1</id>
    <title>Integrating Causal Reasoning into Automated Fact-Checking</title>
    <updated>2025-12-15T12:56:00Z</updated>
    <link href="https://arxiv.org/abs/2512.13286v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13286v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>In fact-checking applications, a common reason to reject a claim is to detect the presence of erroneous cause-effect relationships between the events at play. However, current automated fact-checking methods lack dedicated causal-based reasoning, potentially missing a valuable opportunity for semantically rich explainability. To address this gap, we propose a methodology that combines event relation extraction, semantic similarity computation, and rule-based reasoning to detect logical inconsistencies between chains of events mentioned in a claim and in an evidence. Evaluated on two fact-checking datasets, this method establishes the first baseline for integrating fine-grained causal event relationships into fact-checking and enhance explainability of verdict prediction.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T12:56:00Z</published>
    <arxiv:comment>Extended version of the accepted ACM SAC paper</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <arxiv:journal_ref>The 41st ACM SIGAPP Symposium on Applied Computing (SAC 26), March 23-27, 2026, Thessaloniki, Greece. ACM, New York, NY, USA</arxiv:journal_ref>
    <author>
      <name>Youssra Rebboud</name>
    </author>
    <author>
      <name>Pasquale Lisena</name>
    </author>
    <author>
      <name>Raphael Troncy</name>
    </author>
    <arxiv:doi>10.1145/3748522.3779831</arxiv:doi>
    <link rel="related" href="https://doi.org/10.1145/3748522.3779831" title="doi"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13285v1</id>
    <title>CausalCLIP: Causally-Informed Feature Disentanglement and Filtering for Generalizable Detection of Generated Images</title>
    <updated>2025-12-15T12:48:27Z</updated>
    <link href="https://arxiv.org/abs/2512.13285v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13285v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The rapid advancement of generative models has increased the demand for generated image detectors capable of generalizing across diverse and evolving generation techniques. However, existing methods, including those leveraging pre-trained vision-language models, often produce highly entangled representations, mixing task-relevant forensic cues (causal features) with spurious or irrelevant patterns (non-causal features), thus limiting generalization. To address this issue, we propose CausalCLIP, a framework that explicitly disentangles causal from non-causal features and employs targeted filtering guided by causal inference principles to retain only the most transferable and discriminative forensic cues. By modeling the generation process with a structural causal model and enforcing statistical independence through Gumbel-Softmax-based feature masking and Hilbert-Schmidt Independence Criterion (HSIC) constraints, CausalCLIP isolates stable causal features robust to distribution shifts. When tested on unseen generative models from different series, CausalCLIP demonstrates strong generalization ability, achieving improvements of 6.83% in accuracy and 4.06% in average precision over state-of-the-art methods.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T12:48:27Z</published>
    <arxiv:comment>9 pages Accepted to AAAI 2026</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Bo Liu</name>
    </author>
    <author>
      <name>Qiao Qin</name>
    </author>
    <author>
      <name>Qinghui He</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13281v1</id>
    <title>Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?</title>
    <updated>2025-12-15T12:41:23Z</updated>
    <link href="https://arxiv.org/abs/2512.13281v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13281v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Recent advances in video generation have produced vivid content that are often indistinguishable from real videos, making AI-generated video detection an emerging societal challenge. Prior AIGC detection benchmarks mostly evaluate video without audio, target broad narrative domains, and focus on classification solely. Yet it remains unclear whether state-of-the-art video generation models can produce immersive, audio-paired videos that reliably deceive humans and VLMs. To this end, we introduce Video Reality Test, an ASMR-sourced video benchmark suite for testing perceptual realism under tight audio-visual coupling, featuring the following dimensions: \textbf{(i) Immersive ASMR video-audio sources.} Built on carefully curated real ASMR videos, the benchmark targets fine-grained action-object interactions with diversity across objects, actions, and backgrounds. \textbf{(ii) Peer-Review evaluation.} An adversarial creator-reviewer protocol where video generation models act as creators aiming to fool reviewers, while VLMs serve as reviewers seeking to identify fakeness. Our experimental findings show: The best creator Veo3.1-Fast even fools most VLMs: the strongest reviewer (Gemini 2.5-Pro) achieves only 56\% accuracy (random 50\%), far below that of human experts (81.25\%). Adding audio improves real-fake discrimination, yet superficial cues such as watermarks can still significantly mislead models. These findings delineate the current boundary of video generation realism and expose limitations of VLMs in perceptual fidelity and audio-visual consistency. Our code is available at https://github.com/video-reality-test/video-reality-test.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T12:41:23Z</published>
    <arxiv:comment>Code is at https://github.com/video-reality-test/video-reality-tes}, page is at https://video-reality-test.github.io/</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Jiaqi Wang</name>
    </author>
    <author>
      <name>Weijia Wu</name>
    </author>
    <author>
      <name>Yi Zhan</name>
    </author>
    <author>
      <name>Rui Zhao</name>
    </author>
    <author>
      <name>Ming Hu</name>
    </author>
    <author>
      <name>James Cheng</name>
    </author>
    <author>
      <name>Wei Liu</name>
    </author>
    <author>
      <name>Philip Torr</name>
    </author>
    <author>
      <name>Kevin Qinghong Lin</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13279v1</id>
    <title>AIR: Post-training Data Selection for Reasoning via Attention Head Influence</title>
    <updated>2025-12-15T12:38:24Z</updated>
    <link href="https://arxiv.org/abs/2512.13279v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13279v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>LLMs achieve remarkable multi-step reasoning capabilities, yet effectively transferring these skills via post-training distillation remains challenging. Existing data selection methods, ranging from manual curation to heuristics based on length, entropy, or overall loss, fail to capture the causal importance of individual reasoning steps, limiting distillation efficiency. To address this, we propose Attention Influence for Reasoning (AIR), a principled, unsupervised and training-free framework that leverages mechanistic insights of the retrieval head to select high-value post-training data. AIR first identifies reasoning-critical attention heads of an off-the-shelf model, then constructs a weakened reference model with disabled head influence, and finally quantifies the resulting loss divergence as the Attention Influence Score. This score enables fine-grained assessment at both the step and sample levels, supporting step-level weighted fine-tuning and global sample selection. Experiments across multiple reasoning benchmarks show that AIR consistently improves reasoning accuracy, surpassing heuristic baselines and effectively isolating the most critical steps and samples. Our work establishes a mechanism-driven, data-efficient approach for reasoning distillation in LLMs.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T12:38:24Z</published>
    <arxiv:comment>19 pages</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Jinrui Liu</name>
    </author>
    <author>
      <name>Jeff Wu</name>
    </author>
    <author>
      <name>Xuanguang Pan</name>
    </author>
    <author>
      <name>Gavin Cheung</name>
    </author>
    <author>
      <name>Shuai Ma</name>
    </author>
    <author>
      <name>Chongyang Tao</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13278v1</id>
    <title>AutoTool: Dynamic Tool Selection and Integration for Agentic Reasoning</title>
    <updated>2025-12-15T12:38:04Z</updated>
    <link href="https://arxiv.org/abs/2512.13278v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13278v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Agentic reinforcement learning has advanced large language models (LLMs) to reason through long chain-of-thought trajectories while interleaving external tool use. Existing approaches assume a fixed inventory of tools, limiting LLM agents' adaptability to new or evolving toolsets. We present AutoTool, a framework that equips LLM agents with dynamic tool-selection capabilities throughout their reasoning trajectories. We first construct a 200k dataset with explicit tool-selection rationales across 1,000+ tools and 100+ tasks spanning mathematics, science, code generation, and multimodal reasoning. Building on this data foundation, AutoTool employs a dual-phase optimization pipeline: (i) supervised and RL-based trajectory stabilization for coherent reasoning, and (ii) KL-regularized Plackett-Luce ranking to refine consistent multi-step tool selection. Across ten diverse benchmarks, we train two base models, Qwen3-8B and Qwen2.5-VL-7B, with AutoTool. With fewer parameters, AutoTool consistently outperforms advanced LLM agents and tool-integration methods, yielding average gains of 6.4% in math &amp; science reasoning, 4.5% in search-based QA, 7.7% in code generation, and 6.9% in multimodal understanding. In addition, AutoTool exhibits stronger generalization by dynamically leveraging unseen tools from evolving toolsets during inference.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T12:38:04Z</published>
    <arxiv:comment>Best Paper Award at ICCV 2025 Workshop on Multi-Modal Reasoning for Agentic Intelligence</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Jiaru Zou</name>
    </author>
    <author>
      <name>Ling Yang</name>
    </author>
    <author>
      <name>Yunzhe Qi</name>
    </author>
    <author>
      <name>Sirui Chen</name>
    </author>
    <author>
      <name>Mengting Ai</name>
    </author>
    <author>
      <name>Ke Shen</name>
    </author>
    <author>
      <name>Jingrui He</name>
    </author>
    <author>
      <name>Mengdi Wang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13276v1</id>
    <title>CogniEdit: Dense Gradient Flow Optimization for Fine-Grained Image Editing</title>
    <updated>2025-12-15T12:36:50Z</updated>
    <link href="https://arxiv.org/abs/2512.13276v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13276v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Instruction-based image editing with diffusion models has achieved impressive results, yet existing methods struggle with fine-grained instructions specifying precise attributes such as colors, positions, and quantities. While recent approaches employ Group Relative Policy Optimization (GRPO) for alignment, they optimize only at individual sampling steps, providing sparse feedback that limits trajectory-level control. We propose a unified framework CogniEdit, combining multi-modal reasoning with dense reward optimization that propagates gradients across consecutive denoising steps, enabling trajectory-level gradient flow through the sampling process. Our method comprises three components: (1) Multi-modal Large Language Models for decomposing complex instructions into actionable directives, (2) Dynamic Token Focus Relocation that adaptively emphasizes fine-grained attributes, and (3) Dense GRPO-based optimization that propagates gradients across consecutive steps for trajectory-level supervision. Extensive experiments on benchmark datasets demonstrate that our CogniEdit achieves state-of-the-art performance in balancing fine-grained instruction following with visual quality and editability preservation</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T12:36:50Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Yan Li</name>
    </author>
    <author>
      <name>Lin Liu</name>
    </author>
    <author>
      <name>Xiaopeng Zhang</name>
    </author>
    <author>
      <name>Wei Xue</name>
    </author>
    <author>
      <name>Wenhan Luo</name>
    </author>
    <author>
      <name>Yike Guo</name>
    </author>
    <author>
      <name>Qi Tian</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13268v1</id>
    <title>SPARS: A Reinforcement Learning-Enabled Simulator for Power Management in HPC Job Scheduling</title>
    <updated>2025-12-15T12:28:08Z</updated>
    <link href="https://arxiv.org/abs/2512.13268v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13268v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>High-performance computing (HPC) clusters consume enormous amounts of energy, with idle nodes as a major source of waste. Powering down unused nodes can mitigate this problem, but poorly timed transitions introduce long delays and reduce overall performance. To address this trade-off, we present SPARS, a reinforcement learning-enabled simulator for power management in HPC job scheduling. SPARS integrates job scheduling and node power state management within a discrete-event simulation framework. It supports traditional scheduling policies such as First Come First Served and EASY Backfilling, along with enhanced variants that employ reinforcement learning agents to dynamically decide when nodes should be powered on or off. Users can configure workloads and platforms in JSON format, specifying job arrivals, execution times, node power models, and transition delays. The simulator records comprehensive metrics-including energy usage, wasted power, job waiting times, and node utilization-and provides Gantt chart visualizations to analyze scheduling dynamics and power transitions. Unlike widely used Batsim-based frameworks that rely on heavy inter-process communication, SPARS provides lightweight event handling and consistent simulation results, making experiments easier to reproduce and extend. Its modular design allows new scheduling heuristics or learning algorithms to be integrated with minimal effort. By providing a flexible, reproducible, and extensible platform, SPARS enables researchers and practitioners to systematically evaluate power-aware scheduling strategies, explore the trade-offs between energy efficiency and performance, and accelerate the development of sustainable HPC operations.</summary>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T12:28:08Z</published>
    <arxiv:comment>12 pages, 4 figures, 5 tables</arxiv:comment>
    <arxiv:primary_category term="cs.DC"/>
    <author>
      <name>Muhammad Alfian Amrizal</name>
    </author>
    <author>
      <name>Raka Satya Prasasta</name>
    </author>
    <author>
      <name>Santana Yuda Pradata</name>
    </author>
    <author>
      <name>Kadek Gemilang Santiyuda</name>
    </author>
    <author>
      <name>Reza Pulungan</name>
    </author>
    <author>
      <name>Hiroyuki Takizawa</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13262v1</id>
    <title>Post-Training and Test-Time Scaling of Generative Agent Behavior Models for Interactive Autonomous Driving</title>
    <updated>2025-12-15T12:18:50Z</updated>
    <link href="https://arxiv.org/abs/2512.13262v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13262v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Learning interactive motion behaviors among multiple agents is a core challenge in autonomous driving. While imitation learning models generate realistic trajectories, they often inherit biases from datasets dominated by safe demonstrations, limiting robustness in safety-critical cases. Moreover, most studies rely on open-loop evaluation, overlooking compounding errors in closed-loop execution. We address these limitations with two complementary strategies. First, we propose Group Relative Behavior Optimization (GRBO), a reinforcement learning post-training method that fine-tunes pretrained behavior models via group relative advantage maximization with human regularization. Using only 10% of the training dataset, GRBO improves safety performance by over 40% while preserving behavioral realism. Second, we introduce Warm-K, a warm-started Top-K sampling strategy that balances consistency and diversity in motion selection. Our Warm-K method-based test-time scaling enhances behavioral consistency and reactivity at test time without retraining, mitigating covariate shift and reducing performance discrepancies. Demo videos are available in the supplementary material.</summary>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T12:18:50Z</published>
    <arxiv:comment>11 pages, 5 figures</arxiv:comment>
    <arxiv:primary_category term="cs.RO"/>
    <author>
      <name>Hyunki Seong</name>
    </author>
    <author>
      <name>Jeong-Kyun Lee</name>
    </author>
    <author>
      <name>Heesoo Myeong</name>
    </author>
    <author>
      <name>Yongho Shin</name>
    </author>
    <author>
      <name>Hyun-Mook Cho</name>
    </author>
    <author>
      <name>Duck Hoon Kim</name>
    </author>
    <author>
      <name>Pranav Desai</name>
    </author>
    <author>
      <name>Monu Surana</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13255v1</id>
    <title>BézierFlow: Bézier Stochastic Interpolant Schedulers for Few-Step Generation</title>
    <updated>2025-12-15T12:09:32Z</updated>
    <link href="https://arxiv.org/abs/2512.13255v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13255v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We introduce BézierFlow, a lightweight training approach for few-step generation with pretrained diffusion and flow models. BézierFlow achieves a 2-3x performance improvement for sampling with $\leq$ 10 NFEs while requiring only 15 minutes of training. Recent lightweight training approaches have shown promise by learning optimal timesteps, but their scope remains restricted to ODE discretizations. To broaden this scope, we propose learning the optimal transformation of the sampling trajectory by parameterizing stochastic interpolant (SI) schedulers. The main challenge lies in designing a parameterization that satisfies critical desiderata, including boundary conditions, differentiability, and monotonicity of the SNR. To effectively meet these requirements, we represent scheduler functions as Bézier functions, where control points naturally enforce these properties. This reduces the problem to learning an ordered set of points in the time range, while the interpretation of the points changes from ODE timesteps to Bézier control points. Across a range of pretrained diffusion and flow models, BézierFlow consistently outperforms prior timestep-learning methods, demonstrating the effectiveness of expanding the search space from discrete timesteps to Bézier-based trajectory transformations.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T12:09:32Z</published>
    <arxiv:comment>Project page: https://bezierflow.github.io</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Yunhong Min</name>
    </author>
    <author>
      <name>Juil Koo</name>
    </author>
    <author>
      <name>Seungwoo Yoo</name>
    </author>
    <author>
      <name>Minhyuk Sung</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13250v1</id>
    <title>Toward Ambulatory Vision: Learning Visually-Grounded Active View Selection</title>
    <updated>2025-12-15T12:04:26Z</updated>
    <link href="https://arxiv.org/abs/2512.13250v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13250v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Vision Language Models (VLMs) excel at visual question answering (VQA) but remain limited to snapshot vision, reasoning from static images. In contrast, embodied agents require ambulatory vision, actively moving to obtain more informative views. We introduce Visually Grounded Active View Selection (VG-AVS), a task that selects the most informative next viewpoint using only the visual information in the current image, without relying on scene memory or external knowledge. To support this task, we construct a synthetic dataset with automatically generated paired query-target views and question-answer prompts. We also propose a framework that fine-tunes pretrained VLMs through supervised fine-tuning (SFT) followed by RL-based policy optimization. Our approach achieves strong question answering performance based on viewpoint selection and generalizes robustly to unseen synthetic and real scenes. Furthermore, incorporating our learned VG-AVS framework into existing scene-exploration-based EQA systems improves downstream question-answering accuracy.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T12:04:26Z</published>
    <arxiv:comment>Project page: https://active-view-selection.github.io/</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Juil Koo</name>
    </author>
    <author>
      <name>Daehyeon Choi</name>
    </author>
    <author>
      <name>Sangwoo Youn</name>
    </author>
    <author>
      <name>Phillip Y. Lee</name>
    </author>
    <author>
      <name>Minhyuk Sung</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13247v1</id>
    <title>STARCaster: Spatio-Temporal AutoRegressive Video Diffusion for Identity- and View-Aware Talking Portraits</title>
    <updated>2025-12-15T11:59:01Z</updated>
    <link href="https://arxiv.org/abs/2512.13247v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13247v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>This paper presents STARCaster, an identity-aware spatio-temporal video diffusion model that addresses both speech-driven portrait animation and free-viewpoint talking portrait synthesis, given an identity embedding or reference image, within a unified framework. Existing 2D speech-to-video diffusion models depend heavily on reference guidance, leading to limited motion diversity. At the same time, 3D-aware animation typically relies on inversion through pre-trained tri-plane generators, which often leads to imperfect reconstructions and identity drift. We rethink reference- and geometry-based paradigms in two ways. First, we deviate from strict reference conditioning at pre-training by introducing softer identity constraints. Second, we address 3D awareness implicitly within the 2D video domain by leveraging the inherent multi-view nature of video data. STARCaster adopts a compositional approach progressing from ID-aware motion modeling, to audio-visual synchronization via lip reading-based supervision, and finally to novel view animation through temporal-to-spatial adaptation. To overcome the scarcity of 4D audio-visual data, we propose a decoupled learning approach in which view consistency and temporal coherence are trained independently. A self-forcing training scheme enables the model to learn from longer temporal contexts than those generated at inference, mitigating the overly static animations common in existing autoregressive approaches. Comprehensive evaluations demonstrate that STARCaster generalizes effectively across tasks and identities, consistently surpassing prior approaches in different benchmarks.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T11:59:01Z</published>
    <arxiv:comment>Project page: https://foivospar.github.io/STARCaster/</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Foivos Paraperas Papantoniou</name>
    </author>
    <author>
      <name>Stathis Galanakis</name>
    </author>
    <author>
      <name>Rolandos Alexandros Potamias</name>
    </author>
    <author>
      <name>Bernhard Kainz</name>
    </author>
    <author>
      <name>Stefanos Zafeiriou</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13240v1</id>
    <title>Reflective Preference Optimization (RPO): Enhancing On-Policy Alignment via Hint-Guided Reflection</title>
    <updated>2025-12-15T11:55:55Z</updated>
    <link href="https://arxiv.org/abs/2512.13240v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13240v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Direct Preference Optimization (DPO) has emerged as a lightweight and effective alternative to Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning with AI Feedback (RLAIF) for aligning large language and vision-language models. However, the standard DPO formulation, in which both the chosen and rejected responses are generated by the same policy, suffers from a weak learning signal because the two responses often share similar errors and exhibit small Kullback-Leibler (KL) divergence. This leads to slow and unstable convergence. To address this limitation, we introduce Reflective Preference Optimization (RPO), a new framework that incorporates hint-guided reflection into the DPO paradigm. RPO uses external models to identify hallucination sources and generate concise reflective hints, enabling the construction of on-policy preference pairs with stronger contrastiveness and clearer preference signals. We theoretically show that conditioning on hints increases the expected preference margin through mutual information and improves sample efficiency while remaining within the policy distribution family. Empirically, RPO achieves superior alignment with fewer training samples and iterations, substantially reducing hallucination rates and delivering state-of-the-art performance across multimodal benchmarks.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T11:55:55Z</published>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Zihui Zhao</name>
    </author>
    <author>
      <name>Zechang Li</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13239v1</id>
    <title>A Decision Support Framework for Blockchain Pattern Selection Based on Soft Goals</title>
    <updated>2025-12-15T11:54:00Z</updated>
    <link href="https://arxiv.org/abs/2512.13239v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13239v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Blockchain technology is gaining momentum across many sectors. Whereas blockchain solutions have important positive effects on the business domain, they also introduce constraints and may cause delayed or unforeseen negative effects, undermining business strategies. The diversity of blockchain patterns and lack of standardized frameworks linking business goals to technical design decisions make pattern selection a complex task for system architects. To address this challenge, we propose Blockchain--Technology-Aware Enterprise Modeling (BC-TEAEM), a decision support framework that combines ontologies of blockchain patterns and domain-independent soft goals with a multi-criteria decision-making approach. The framework focuses on the interplay between a domain expert and a technical expert to ensure alignment and traceability. By iteratively capturing and refining preferences, BC-TEAEM supports systematic selection of blockchain patterns. We develop a prototype decision support tool implementing our method and validate it through a case study of a pharmaceutical company's supply chain traceability system, demonstrating the framework's applicability. %a supply chain traceability case study.</summary>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T11:54:00Z</published>
    <arxiv:primary_category term="cs.SE"/>
    <arxiv:journal_ref>Blockchain Confluence (satellite conference of the 1st IEEE International Conference on Distributed Ledger Technologies), Nov 2025, Lisboa, Portugal</arxiv:journal_ref>
    <author>
      <name>Eddy Kiomba Kambilo</name>
      <arxiv:affiliation>CRI</arxiv:affiliation>
    </author>
    <author>
      <name>Nicolas Herbaut</name>
      <arxiv:affiliation>CRI, LaBRI</arxiv:affiliation>
    </author>
    <author>
      <name>Irina Rychkova</name>
      <arxiv:affiliation>CRI</arxiv:affiliation>
    </author>
    <author>
      <name>Carine Souveyet</name>
      <arxiv:affiliation>CRI</arxiv:affiliation>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13238v1</id>
    <title>Ego-EXTRA: video-language Egocentric Dataset for EXpert-TRAinee assistance</title>
    <updated>2025-12-15T11:53:35Z</updated>
    <link href="https://arxiv.org/abs/2512.13238v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13238v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We present Ego-EXTRA, a video-language Egocentric Dataset for EXpert-TRAinee assistance. Ego-EXTRA features 50 hours of unscripted egocentric videos of subjects performing procedural activities (the trainees) while guided by real-world experts who provide guidance and answer specific questions using natural language. Following a ``Wizard of OZ'' data collection paradigm, the expert enacts a wearable intelligent assistant, looking at the activities performed by the trainee exclusively from their egocentric point of view, answering questions when asked by the trainee, or proactively interacting with suggestions during the procedures. This unique data collection protocol enables Ego-EXTRA to capture a high-quality dialogue in which expert-level feedback is provided to the trainee. Two-way dialogues between experts and trainees are recorded, transcribed, and used to create a novel benchmark comprising more than 15k high-quality Visual Question Answer sets, which we use to evaluate Multimodal Large Language Models. The results show that Ego-EXTRA is challenging and highlight the limitations of current models when used to provide expert-level assistance to the user. The Ego-EXTRA dataset is publicly available to support the benchmark of egocentric video-language assistants: https://fpv-iplab.github.io/Ego-EXTRA/.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T11:53:35Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Francesco Ragusa</name>
    </author>
    <author>
      <name>Michele Mazzamuto</name>
    </author>
    <author>
      <name>Rosario Forte</name>
    </author>
    <author>
      <name>Irene D'Ambra</name>
    </author>
    <author>
      <name>James Fort</name>
    </author>
    <author>
      <name>Jakob Engel</name>
    </author>
    <author>
      <name>Antonino Furnari</name>
    </author>
    <author>
      <name>Giovanni Maria Farinella</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13237v1</id>
    <title>Learning to Retrieve with Weakened Labels: Robust Training under Label Noise</title>
    <updated>2025-12-15T11:52:13Z</updated>
    <link href="https://arxiv.org/abs/2512.13237v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13237v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Neural Encoders are frequently used in the NLP domain to perform dense retrieval tasks, for instance, to generate the candidate documents for a given query in question-answering tasks. However, sparse annotation and label noise in the training data make it challenging to train or fine-tune such retrieval models. Although existing works have attempted to mitigate these problems by incorporating modified loss functions or data cleaning, these approaches either require some hyperparameters to tune during training or add substantial complexity to the training setup. In this work, we consider a label weakening approach to generate robust retrieval models in the presence of label noise. Instead of enforcing a single, potentially erroneous label for each query document pair, we allow for a set of plausible labels derived from both the observed supervision and the model's confidence scores. We perform an extensive evaluation considering two retrieval models, one re-ranking model, considering four diverse ranking datasets. To this end, we also consider a realistic noisy setting by using a semantic-aware noise generation technique to generate different ratios of noise. Our initial results show that label weakening can improve the performance of the retrieval tasks in comparison to 10 different state-of-the-art loss functions.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T11:52:13Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Arnab Sharma</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13235v1</id>
    <title>CORE: Contrastive Masked Feature Reconstruction on Graphs</title>
    <updated>2025-12-15T11:48:48Z</updated>
    <link href="https://arxiv.org/abs/2512.13235v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13235v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>In the rapidly evolving field of self-supervised learning on graphs, generative and contrastive methodologies have emerged as two dominant approaches. Our study focuses on masked feature reconstruction (MFR), a generative technique where a model learns to restore the raw features of masked nodes in a self-supervised manner. We observe that both MFR and graph contrastive learning (GCL) aim to maximize agreement between similar elements. Building on this observation, we reveal a novel theoretical insight: under specific conditions, the objectives of MFR and node-level GCL converge, despite their distinct operational mechanisms. This theoretical connection suggests these approaches are complementary rather than fundamentally different, prompting us to explore their integration to enhance self-supervised learning on graphs. Our research presents Contrastive Masked Feature Reconstruction (CORE), a novel graph self-supervised learning framework that integrates contrastive learning into MFR. Specifically, we form positive pairs exclusively between the original and reconstructed features of masked nodes, encouraging the encoder to prioritize contextual information over the node's own features. Additionally, we leverage the masked nodes themselves as negative samples, combining MFR's reconstructive power with GCL's discriminative ability to better capture intrinsic graph structures. Empirically, our proposed framework CORE significantly outperforms MFR across node and graph classification tasks, demonstrating state-of-the-art results. In particular, CORE surpasses GraphMAE and GraphMAE2 by up to 2.80% and 3.72% on node classification tasks, and by up to 3.82% and 3.76% on graph classification tasks.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T11:48:48Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Jianyuan Bo</name>
    </author>
    <author>
      <name>Yuan Fang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13228v1</id>
    <title>ModSSC: A Modular Framework for Semi-Supervised Classification on Heterogeneous Data</title>
    <updated>2025-12-15T11:43:14Z</updated>
    <link href="https://arxiv.org/abs/2512.13228v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13228v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Semi-supervised classification leverages both labeled and unlabeled data to improve predictive performance, but existing software support is fragmented across methods and modalities. We introduce ModSSC, an open source Python framework that unifies inductive and transductive semi-supervised classification in a modular code base. ModSSC implements a broad range of classical and recent algorithms, provides loaders for tabular, image, text, audio and graph datasets, and exposes a single configuration interface for specifying datasets, models and evaluation protocols. It supports both lightweight classical methods on small datasets running on CPU and recent deep approaches that can exploit multiple GPUs within the same experimental framework. Experiments are described declaratively in YAML, which facilitates reproducing existing work and running large comparative studies. ModSSC 1.0.0 is released under the MIT license with extensive documentation and tests, and is available at https://github.com/ModSSC/ModSSC.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T11:43:14Z</published>
    <arxiv:comment>Preprint describing the open source ModSSC framework for inductive and transductive semi-supervised classification on heterogeneous data</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Melvin Barbaux</name>
      <arxiv:affiliation>IMB</arxiv:affiliation>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13227v1</id>
    <title>Better LMO-based Momentum Methods with Second-Order Information</title>
    <updated>2025-12-15T11:43:09Z</updated>
    <link href="https://arxiv.org/abs/2512.13227v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13227v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The use of momentum in stochastic optimization algorithms has shown empirical success across a range of machine learning tasks. Recently, a new class of stochastic momentum algorithms has emerged within the Linear Minimization Oracle (LMO) framework--leading to state-of-the-art methods, such as Muon, Scion, and Gluon, that effectively solve deep neural network training problems. However, traditional stochastic momentum methods offer convergence guarantees no better than the ${O}(1/K^{1/4})$ rate. While several approaches--such as Hessian-Corrected Momentum (HCM)--have aimed to improve this rate, their theoretical results are generally restricted to the Euclidean norm setting. This limitation hinders their applicability in problems, where arbitrary norms are often required. In this paper, we extend the LMO-based framework by integrating HCM, and provide convergence guarantees under relaxed smoothness and arbitrary norm settings. We establish improved convergence rates of ${O}(1/K^{1/3})$ for HCM, which can adapt to the geometry of the problem and achieve a faster rate than traditional momentum. Experimental results on training Multi-Layer Perceptrons (MLPs) and Long Short-Term Memory (LSTM) networks verify our theoretical observations.</summary>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T11:43:09Z</published>
    <arxiv:primary_category term="math.OC"/>
    <author>
      <name>Sarit Khirirat</name>
    </author>
    <author>
      <name>Abdurakhmon Sadiev</name>
    </author>
    <author>
      <name>Yury Demidovich</name>
    </author>
    <author>
      <name>Peter Richtárik</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13217v1</id>
    <title>Rethinking Physics-Informed Regression Beyond Training Loops and Bespoke Architectures</title>
    <updated>2025-12-15T11:31:41Z</updated>
    <link href="https://arxiv.org/abs/2512.13217v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13217v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We revisit the problem of physics-informed regression, and propose a method that directly computes the state at the prediction point, simultaneously with the derivative and curvature information of the existing samples. We frame each prediction as a constrained optimisation problem, leveraging multivariate Taylor series expansions and explicitly enforcing physical laws. Each individual query can be processed with low computational cost without any pre- or re-training, in contrast to global function approximator-based solutions such as neural networks. Our comparative benchmarks on a reaction-diffusion system show competitive predictive accuracy relative to a neural network-based solution, while completely eliminating the need for long training loops, and remaining robust to changes in the sampling layout.</summary>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T11:31:41Z</published>
    <arxiv:primary_category term="math.OC"/>
    <author>
      <name>Lorenzo Sabug</name>
    </author>
    <author>
      <name>Eric Kerrigan</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13213v1</id>
    <title>Towards Secure Decentralized Applications and Consensus Protocols in Blockchains (on Selfish Mining, Undercutting Attacks, DAG-Based Blockchains, E-Voting, Cryptocurrency Wallets, Secure-Logging, and CBDC)</title>
    <updated>2025-12-15T11:26:43Z</updated>
    <link href="https://arxiv.org/abs/2512.13213v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13213v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>With the rise of cryptocurrencies, many new applications built on decentralized blockchains have emerged. Blockchains are full-stack distributed systems where multiple sub-systems interact. While many deployed blockchains and decentralized applications need better scalability and performance, security is also critical. Due to their complexity, assessing blockchain and DAPP security requires a more holistic view than for traditional distributed or centralized systems.
  In this thesis, we summarize our contributions to blockchain and decentralized application security. We propose a security reference architecture to support standardized vulnerability and threat analysis. We study consensus security in single-chain Proof-of-Work blockchains, including resistance to selfish mining, undercutting, and greedy transaction selection, as well as related issues in DAG-based systems. We contribute to wallet security with a new classification of authentication schemes and a two-factor method based on One-Time Passwords. We advance e-voting with a practical boardroom voting protocol, extend it to a scalable version for millions of participants while preserving security and privacy, and introduce a repetitive voting framework that enables vote changes between elections while avoiding peak-end effects. Finally, we improve secure logging using blockchains and trusted computing through a centralized ledger that guarantees non-equivocation, integrity, and censorship evidence, then build on it to propose an interoperability protocol for central bank digital currencies that ensures atomic transfers.</summary>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T11:26:43Z</published>
    <arxiv:primary_category term="cs.CR"/>
    <author>
      <name>Ivan Homoliak</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13207v1</id>
    <title>Evaluating Adversarial Attacks on Federated Learning for Temperature Forecasting</title>
    <updated>2025-12-15T11:22:24Z</updated>
    <link href="https://arxiv.org/abs/2512.13207v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13207v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Deep learning and federated learning (FL) are becoming powerful partners for next-generation weather forecasting. Deep learning enables high-resolution spatiotemporal forecasts that can surpass traditional numerical models, while FL allows institutions in different locations to collaboratively train models without sharing raw data, addressing efficiency and security concerns. While FL has shown promise across heterogeneous regions, its distributed nature introduces new vulnerabilities. In particular, data poisoning attacks, in which compromised clients inject manipulated training data, can degrade performance or introduce systematic biases. These threats are amplified by spatial dependencies in meteorological data, allowing localized perturbations to influence broader regions through global model aggregation. In this study, we investigate how adversarial clients distort federated surface temperature forecasts trained on the Copernicus European Regional ReAnalysis (CERRA) dataset. We simulate geographically distributed clients and evaluate patch-based and global biasing attacks on regional temperature forecasts. Our results show that even a small fraction of poisoned clients can mislead predictions across large, spatially connected areas. A global temperature bias attack from a single compromised client shifts predictions by up to -1.7 K, while coordinated patch attacks more than triple the mean squared error and produce persistent regional anomalies exceeding +3.5 K. Finally, we assess trimmed mean aggregation as a defense mechanism, showing that it successfully defends against global bias attacks (2-13\% degradation) but fails against patch attacks (281-603\% amplification), exposing limitations of outlier-based defenses for spatially correlated data.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T11:22:24Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Karina Chichifoi</name>
    </author>
    <author>
      <name>Fabio Merizzi</name>
    </author>
    <author>
      <name>Michele Colajanni</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13199v1</id>
    <title>Investigation of a Bit-Sequence Reconciliation Protocol Based on Neural TPM Networks in Secure Quantum Communications</title>
    <updated>2025-12-15T11:14:00Z</updated>
    <link href="https://arxiv.org/abs/2512.13199v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13199v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The article discusses a key reconciliation protocol for quantum key distribution (QKD) systems based on Tree Parity Machines (TPM). The idea of transforming key material into neural network weights is presented. Two experiments were conducted to study how the number of synchronization iterations and the amount of leaked information depend on the quantum bit error rate (QBER) and the range of neural network weights. The results show a direct relationship between the average number of synchronization iterations and QBER, an increase in iterations when the weight range is expanded, and a reduction in leaked information as the weight range increases. Based on these results, conclusions are drawn regarding the applicability of the protocol and the prospects for further research on neural cryptographic methods in the context of key reconciliation.</summary>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T11:14:00Z</published>
    <arxiv:comment>4 pages; 6 figures; reported on 21st International Scientific and Practical Conference 'Electronic Means and Control Systems', dedicated to the 80th anniversary of radio engineering education beyond the Urals, November 23, Tomsk</arxiv:comment>
    <arxiv:primary_category term="quant-ph"/>
    <author>
      <name>Matvey Yorkhov</name>
    </author>
    <author>
      <name>Vladimir Faerman</name>
    </author>
    <author>
      <name>Anton Konev</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13197v1</id>
    <title>MicroPhaseNO: Adapting an Earthquake-Trained Phase Neural Operator for Microseismic Phase Picking</title>
    <updated>2025-12-15T11:13:21Z</updated>
    <link href="https://arxiv.org/abs/2512.13197v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13197v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Seismic phase picking is very often used for microseismic monitoring and subsurface imaging. Traditional manual processing is not feasible for either real-time applications or large arrays. Deep learning-based pickers trained on large earthquake catalogs offer an automated alternative. However, they are typically optimized for high signal-to-noise, long-duration networks and struggle with the challenges presented by microseismic datasets, which are purpose-built for limited time without previously detected seismicity. In this study, we demonstrate how a network-wide earthquake phase picker, the Phase Neural Operator (PhaseNO), can be adapted to microseismic monitoring using transfer learning. Starting from a PhaseNO model pre-trained on more than 57,000 three-component earthquake and noise records, we fine-tune the model using only 200 labeled and noise seismograms from induced events in hydraulic-fracturing settings. The fine-tuned model thus preserves the rich spatio-temporal representation learned from abundant earthquake data, while adapting to the characteristics and labeling conventions of microseismic phases, which are often picked on peaks or troughs rather than onsets. We evaluate performance on three distinct real-world microseismic datasets with different network geometries and acquisition parameters. Compared to the original PhaseNO and a conventional workflow, the adapted model increases F1 score and accuracy by up to 30%, and strongly reduces systematic timing bias and pick uncertainty. Because the adaptation relies on a small, campaign-specific calibration set, the approach is readily transferable to other microseismic tasks where public earthquake data and pre-trained models are accessible. The associated code will be released openly at https://github.com/ayratabd/MicroPhaseNO.</summary>
    <category term="physics.geo-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T11:13:21Z</published>
    <arxiv:comment>Submitted to Pure and Applied Geophysics</arxiv:comment>
    <arxiv:primary_category term="physics.geo-ph"/>
    <author>
      <name>Ayrat Abdullin</name>
    </author>
    <author>
      <name>Umair bin Waheed</name>
    </author>
    <author>
      <name>Leo Eisner</name>
    </author>
    <author>
      <name>Naveed Iqbal</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13196v1</id>
    <title>Noise-Resilient Quantum Aggregation on NISQ for Federated ADAS Learning</title>
    <updated>2025-12-15T11:10:40Z</updated>
    <link href="https://arxiv.org/abs/2512.13196v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13196v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Advanced Driver Assistance Systems (ADAS) increasingly employ Federated Learning (FL) to collaboratively train models across distributed vehicular nodes while preserving data privacy. Yet, conventional FL aggregation remains susceptible to noise, latency, and security constraints inherent to real-time vehicular networks. This paper introduces Noise-Resilient Quantum Federated Learning (NR-QFL), a hybrid quantum-classical framework that enables secure, low-latency aggregation through variational quantum circuits (VQCs) operating under Noisy Intermediate-Scale Quantum (NISQ) conditions. The framework encodes model parameters as quantum states with adaptive gate reparameterization, ensuring bounded-error convergence and provable resilience under Completely Positive Trace-Preserving (CPTP) dynamics. NR-QFL employs quantum entropy-based client selection and multi-server coordination for fairness and stability. Empirical validation shows consistent convergence with reduced gradient variance, lower communication overhead, and enhanced noise tolerance under constrained edge conditions. The framework establishes a scalable foundation for quantum-enhanced federated learning, enabling secure, efficient, and dynamically stable ADAS intelligence at the vehicular edge.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T11:10:40Z</published>
    <arxiv:comment>This paper was accepted and presented at WinTechCon 2025, Bangalore, India, and is published in IEEE Xplore</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <arxiv:journal_ref>Proc. 2025 IEEE 6th International Women in Technology Conference (WINTECHCON), IEEE, 2025</arxiv:journal_ref>
    <author>
      <name>Chethana Prasad Kabgere</name>
    </author>
    <author>
      <name>Sudarshan T S B</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13194v1</id>
    <title>Efficient Adaptive Rejection Sampling for Accelerating Speculative Decoding in Large Language Models</title>
    <updated>2025-12-15T11:08:56Z</updated>
    <link href="https://arxiv.org/abs/2512.13194v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13194v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Speculative Decoding is a prominent technique for accelerating the autoregressive inference of large language models (LLMs) by employing a fast draft model to propose candidate token sequences and a large target model to verify them in parallel. However, its core component -- the rejection sampling mechanism -- relies on a fixed, context-independent random threshold. This leads to a significant "random rejection" problem in high-uncertainty generation scenarios, where plausible candidate tokens are frequently rejected due to random chance, undermining inference efficiency. This paper introduces Efficient Adaptive Rejection Sampling (EARS), a novel method that dynamically adjusts the acceptance threshold by incorporating the target model's own predictive uncertainty, measured as \(1 - \max(P_{\mathrm{target}})\). By introducing a tolerance term proportional to this uncertainty, EARS intelligently relaxes the acceptance criterion when the model is uncertain, effectively reducing random rejections while maintaining strict standards when the model is confident. Experiments on creative writing and open-domain QA tasks demonstrate that EARS significantly enhances the efficiency of speculative decoding, achieving up to an 18.12% increase in throughput with a negligible 0.84% accuracy drop on the GSM8K benchmark. The method requires no modifications to model architectures and can be seamlessly integrated into existing speculative decoding frameworks.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T11:08:56Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Chendong Sun</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13192v1</id>
    <title>POLAR: A Portrait OLAT Dataset and Generative Framework for Illumination-Aware Face Modeling</title>
    <updated>2025-12-15T11:04:09Z</updated>
    <link href="https://arxiv.org/abs/2512.13192v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13192v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Face relighting aims to synthesize realistic portraits under novel illumination while preserving identity and geometry. However, progress remains constrained by the limited availability of large-scale, physically consistent illumination data. To address this, we introduce POLAR, a large-scale and physically calibrated One-Light-at-a-Time (OLAT) dataset containing over 200 subjects captured under 156 lighting directions, multiple views, and diverse expressions. Building upon POLAR, we develop a flow-based generative model POLARNet that predicts per-light OLAT responses from a single portrait, capturing fine-grained and direction-aware illumination effects while preserving facial identity. Unlike diffusion or background-conditioned methods that rely on statistical or contextual cues, our formulation models illumination as a continuous, physically interpretable transformation between lighting states, enabling scalable and controllable relighting. Together, POLAR and POLARNet form a unified illumination learning framework that links real data, generative synthesis, and physically grounded relighting, establishing a self-sustaining "chicken-and-egg" cycle for scalable and reproducible portrait illumination.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T11:04:09Z</published>
    <arxiv:comment>19 pages, 19 figures</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Zhuo Chen</name>
    </author>
    <author>
      <name>Chengqun Yang</name>
    </author>
    <author>
      <name>Zhuo Su</name>
    </author>
    <author>
      <name>Zheng Lv</name>
    </author>
    <author>
      <name>Jingnan Gao</name>
    </author>
    <author>
      <name>Xiaoyuan Zhang</name>
    </author>
    <author>
      <name>Xiaokang Yang</name>
    </author>
    <author>
      <name>Yichao Yan</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13191v1</id>
    <title>CoRA: A Collaborative Robust Architecture with Hybrid Fusion for Efficient Perception</title>
    <updated>2025-12-15T11:00:38Z</updated>
    <link href="https://arxiv.org/abs/2512.13191v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13191v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Collaborative perception has garnered significant attention as a crucial technology to overcome the perceptual limitations of single-agent systems. Many state-of-the-art (SOTA) methods have achieved communication efficiency and high performance via intermediate fusion. However, they share a critical vulnerability: their performance degrades under adverse communication conditions due to the misalignment induced by data transmission, which severely hampers their practical deployment. To bridge this gap, we re-examine different fusion paradigms, and recover that the strengths of intermediate and late fusion are not a trade-off, but a complementary pairing. Based on this key insight, we propose CoRA, a novel collaborative robust architecture with a hybrid approach to decouple performance from robustness with low communication. It is composed of two components: a feature-level fusion branch and an object-level correction branch. Its first branch selects critical features and fuses them efficiently to ensure both performance and scalability. The second branch leverages semantic relevance to correct spatial displacements, guaranteeing resilience against pose errors. Experiments demonstrate the superiority of CoRA. Under extreme scenarios, CoRA improves upon its baseline performance by approximately 19% in AP@0.7 with more than 5x less communication volume, which makes it a promising solution for robust collaborative perception.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T11:00:38Z</published>
    <arxiv:comment>Accepted by AAAI2026</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Gong Chen</name>
    </author>
    <author>
      <name>Chaokun Zhang</name>
    </author>
    <author>
      <name>Pengcheng Lv</name>
    </author>
    <author>
      <name>Xiaohui Xie</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13190v1</id>
    <title>WAY: Estimation of Vessel Destination in Worldwide AIS Trajectory</title>
    <updated>2025-12-15T10:55:20Z</updated>
    <link href="https://arxiv.org/abs/2512.13190v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13190v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The Automatic Identification System (AIS) enables data-driven maritime surveillance but suffers from reliability issues and irregular intervals. We address vessel destination estimation using global-scope AIS data by proposing a differentiated approach that recasts long port-to-port trajectories as a nested sequence structure. Using spatial grids, this method mitigates spatio-temporal bias while preserving detailed resolution. We introduce a novel deep learning architecture, WAY, designed to process these reformulated trajectories for long-term destination estimation days to weeks in advance. WAY comprises a trajectory representation layer and Channel-Aggregative Sequential Processing (CASP) blocks. The representation layer generates multi-channel vector sequences from kinematic and non-kinematic features. CASP blocks utilize multi-headed channel- and self-attention for aggregation and sequential information delivery. Additionally, we propose a task-specialized Gradient Dropout (GD) technique to enable many-to-many training on single labels, preventing biased feedback surges by stochastically blocking gradient flow based on sample length. Experiments on 5-year AIS data demonstrate WAY's superiority over conventional spatial grid-based approaches regardless of trajectory progression. Results further confirm that adopting GD leads to performance gains. Finally, we explore WAY's potential for real-world application through multitask learning for ETA estimation.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T10:55:20Z</published>
    <arxiv:comment>Accepted to IEEE Transactions on Aerospace and Electronic Systems (TAES)</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <arxiv:journal_ref>IEEE Transactions on Aerospace and Electronic Systems, vol. 59, no. 5, Oct. 2023</arxiv:journal_ref>
    <author>
      <name>Jin Sob Kim</name>
    </author>
    <author>
      <name>Hyun Joon Park</name>
    </author>
    <author>
      <name>Wooseok Shin</name>
    </author>
    <author>
      <name>Dongil Park</name>
    </author>
    <author>
      <name>Sung Won Han</name>
    </author>
    <arxiv:doi>10.1109/TAES.2023.3269729</arxiv:doi>
    <link rel="related" href="https://doi.org/10.1109/TAES.2023.3269729" title="doi"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13186v1</id>
    <title>PolySet: Restoring the Statistical Ensemble Nature of Polymers for Machine Learning</title>
    <updated>2025-12-15T10:50:48Z</updated>
    <link href="https://arxiv.org/abs/2512.13186v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13186v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Machine-learning (ML) models in polymer science typically treat a polymer as a single, perfectly defined molecular graph, even though real materials consist of stochastic ensembles of chains with distributed lengths. This mismatch between physical reality and digital representation limits the ability of current models to capture polymer behaviour. Here we introduce PolySet, a framework that represents a polymer as a finite, weighted ensemble of chains sampled from an assumed molar-mass distribution. This ensemble-based encoding is independent of chemical detail, compatible with any molecular representation and illustrated here in the homopolymer case using a minimal language model. We show that PolySet retains higher-order distributional moments (such as Mz, Mz+1), enabling ML models to learn tail-sensitive properties with greatly improved stability and accuracy. By explicitly acknowledging the statistical nature of polymer matter, PolySet establishes a physically grounded foundation for future polymer machine learning, naturally extensible to copolymers, block architectures, and other complex topologies.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.mtrl-sci" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T10:50:48Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Khalid Ferji</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13177v1</id>
    <title>MMDrive: Interactive Scene Understanding Beyond Vision with Multi-representational Fusion</title>
    <updated>2025-12-15T10:37:59Z</updated>
    <link href="https://arxiv.org/abs/2512.13177v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13177v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Vision-language models enable the understanding and reasoning of complex traffic scenarios through multi-source information fusion, establishing it as a core technology for autonomous driving. However, existing vision-language models are constrained by the image understanding paradigm in 2D plane, which restricts their capability to perceive 3D spatial information and perform deep semantic fusion, resulting in suboptimal performance in complex autonomous driving environments. This study proposes MMDrive, an multimodal vision-language model framework that extends traditional image understanding to a generalized 3D scene understanding framework. MMDrive incorporates three complementary modalities, including occupancy maps, LiDAR point clouds, and textual scene descriptions. To this end, it introduces two novel components for adaptive cross-modal fusion and key information extraction. Specifically, the Text-oriented Multimodal Modulator dynamically weights the contributions of each modality based on the semantic cues in the question, guiding context-aware feature integration. The Cross-Modal Abstractor employs learnable abstract tokens to generate compact, cross-modal summaries that highlight key regions and essential semantics. Comprehensive evaluations on the DriveLM and NuScenes-QA benchmarks demonstrate that MMDrive achieves significant performance gains over existing vision-language models for autonomous driving, with a BLEU-4 score of 54.56 and METEOR of 41.78 on DriveLM, and an accuracy score of 62.7% on NuScenes-QA. MMDrive effectively breaks the traditional image-only understanding barrier, enabling robust multimodal reasoning in complex driving environments and providing a new foundation for interpretable autonomous driving scene understanding.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T10:37:59Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Minghui Hou</name>
    </author>
    <author>
      <name>Wei-Hsing Huang</name>
    </author>
    <author>
      <name>Shaofeng Liang</name>
    </author>
    <author>
      <name>Daizong Liu</name>
    </author>
    <author>
      <name>Tai-Hao Wen</name>
    </author>
    <author>
      <name>Gang Wang</name>
    </author>
    <author>
      <name>Runwei Guan</name>
    </author>
    <author>
      <name>Weiping Ding</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13175v1</id>
    <title>Seeing the Whole Picture: Distribution-Guided Data-Free Distillation for Semantic Segmentation</title>
    <updated>2025-12-15T10:37:05Z</updated>
    <link href="https://arxiv.org/abs/2512.13175v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13175v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Semantic segmentation requires a holistic understanding of the physical world, as it assigns semantic labels to spatially continuous and structurally coherent objects rather than to isolated pixels. However, existing data-free knowledge distillation (DFKD) methods-primarily designed for classification-often disregard this continuity, resulting in significant performance degradation when applied directly to segmentation tasks. In this paper, we introduce DFSS, a novel data-free distillation framework tailored for semantic segmentation. Unlike prior approaches that treat pixels independently, DFSS respects the structural and contextual continuity of real-world scenes. Our key insight is to leverage Batch Normalization (BN) statistics from a teacher model to guide Approximate Distribution Sampling (ADS), enabling the selection of data that better reflects the original training distribution-without relying on potentially misleading teacher predictions. Additionally, we propose Weighted Distribution Progressive Distillation (WDPD), which dynamically prioritizes reliable samples that are more closely aligned with the original data distribution early in training and gradually incorporates more challenging cases, mirroring the natural progression of learning in human perception. Extensive experiments on standard benchmarks demonstrate that DFSS consistently outperforms existing data-free distillation methods for semantic segmentation, achieving state-of-the-art results with significantly reduced reliance on auxiliary data.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T10:37:05Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Hongxuan Sun</name>
    </author>
    <author>
      <name>Tao Wu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13174v1</id>
    <title>Carrot, stick, or both? Price incentives for sustainable food choice in competitive environments</title>
    <updated>2025-12-15T10:35:44Z</updated>
    <link href="https://arxiv.org/abs/2512.13174v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13174v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Meat consumption is a major driver of global greenhouse gas emissions. While pricing interventions have shown potential to reduce meat intake, previous studies have focused on highly constrained environments with limited consumer choice. Here, we present the first large-scale field experiment to evaluate multiple pricing interventions in a real-world, competitive setting. Using a sequential crossover design with matched menus in a Swiss university campus, we systematically compared vegetarian-meal discounts (-2.5 CHF), meat surcharges (+2.5 CHF), and a combined scheme (-1.2 CHF=+1.2 CHF) across four campus cafeterias. Only the surcharge and combined interventions led to significant increases in vegetarian meal uptake--by 26.4% and 16.6%, respectively--and reduced CO2 emissions per meal by 7.4% and 11.3%, respectively. The surcharge, while effective, triggered a 12.3% drop in sales at intervention sites and a corresponding 14.9% increase in non-treated locations, hence causing a spillover effect that completely offset environmental gains. In contrast, the combined approach achieved meaningful emission reductions without significant effects on overall sales or revenue, making it both effective and economically viable. Notably, pricing interventions were equally effective for both vegetarian-leaning customers and habitual meat-eaters, stimulating change even within entrenched dietary habits. Our results show that balanced pricing strategies can reduce the carbon footprint of realistic food environments, but require coordinated implementation to maximize climate benefits and avoid unintended spillover effects.</summary>
    <category term="econ.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T10:35:44Z</published>
    <arxiv:comment>10 pages, 3 figures</arxiv:comment>
    <arxiv:primary_category term="econ.GN"/>
    <author>
      <name>Francesco Salvi</name>
    </author>
    <author>
      <name>Giuseppe Russo</name>
    </author>
    <author>
      <name>Adam Barla</name>
    </author>
    <author>
      <name>Vincent Moreau</name>
    </author>
    <author>
      <name>Robert West</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13170v1</id>
    <title>Iterative Tuning of Nonlinear Model Predictive Control for Robotic Manufacturing Tasks</title>
    <updated>2025-12-15T10:30:40Z</updated>
    <link href="https://arxiv.org/abs/2512.13170v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13170v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Manufacturing processes are often perturbed by drifts in the environment and wear in the system, requiring control re-tuning even in the presence of repetitive operations. This paper presents an iterative learning framework for automatic tuning of Nonlinear Model Predictive Control (NMPC) weighting matrices based on task-level performance feedback. Inspired by norm-optimal Iterative Learning Control (ILC), the proposed method adaptively adjusts NMPC weights Q and R across task repetitions to minimize key performance indicators (KPIs) related to tracking accuracy, control effort, and saturation. Unlike gradient-based approaches that require differentiating through the NMPC solver, we construct an empirical sensitivity matrix, enabling structured weight updates without analytic derivatives. The framework is validated through simulation on a UR10e robot performing carbon fiber winding on a tetrahedral core. Results demonstrate that the proposed approach converges to near-optimal tracking performance (RMSE within 0.3% of offline Bayesian Optimization (BO)) in just 4 online repetitions, compared to 100 offline evaluations required by BO algorithm. The method offers a practical solution for adaptive NMPC tuning in repetitive robotic tasks, combining the precision of carefully optimized controllers with the flexibility of online adaptation.</summary>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T10:30:40Z</published>
    <arxiv:primary_category term="cs.RO"/>
    <author>
      <name>Deepak Ingole</name>
    </author>
    <author>
      <name>Valentin Bhend</name>
    </author>
    <author>
      <name>Shiva Ganesh Murali</name>
    </author>
    <author>
      <name>Oliver Dobrich</name>
    </author>
    <author>
      <name>Alisa Rupenayan</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13168v1</id>
    <title>Finch: Benchmarking Finance &amp; Accounting across Spreadsheet-Centric Enterprise Workflows</title>
    <updated>2025-12-15T10:28:45Z</updated>
    <link href="https://arxiv.org/abs/2512.13168v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13168v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We introduce a finance &amp; accounting benchmark (Finch) for evaluating AI agents on real-world, enterprise-grade professional workflows -- interleaving data entry, structuring, formatting, web search, cross-file retrieval, calculation, modeling, validation, translation, visualization, and reporting. Finch is sourced from authentic enterprise workspaces at Enron (15,000 spreadsheets and 500,000 emails from 150 employees) and other financial institutions, preserving in-the-wild messiness across multimodal artifacts (text, tables, formulas, charts, code, and images) and spanning diverse domains such as budgeting, trading, and asset management.
  We propose a workflow construction process that combines LLM-assisted discovery with expert annotation: (1) LLM-assisted, expert-verified derivation of workflows from real-world email threads and version histories of spreadsheet files, and (2) meticulous expert annotation for workflows, requiring over 700 hours of domain-expert effort. This yields 172 composite workflows with 384 tasks, involving 1,710 spreadsheets with 27 million cells, along with PDFs and other artifacts, capturing the intrinsically messy, long-horizon, knowledge-intensive, and collaborative nature of real-world enterprise work.
  We conduct both human and automated evaluations of frontier AI systems including GPT 5.1, Claude Sonnet 4.5, Gemini 3 Pro, Grok 4, and Qwen 3 Max, and GPT 5.1 Pro spends 48 hours in total yet passes only 38.4% of workflows, while Claude Sonnet 4.5 passes just 25.0%. Comprehensive case studies further surface the challenges that real-world enterprise workflows pose for AI agents.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T10:28:45Z</published>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Haoyu Dong</name>
    </author>
    <author>
      <name>Pengkun Zhang</name>
    </author>
    <author>
      <name>Yan Gao</name>
    </author>
    <author>
      <name>Xuanyu Dong</name>
    </author>
    <author>
      <name>Yilin Cheng</name>
    </author>
    <author>
      <name>Mingzhe Lu</name>
    </author>
    <author>
      <name>Adina Yakefu</name>
    </author>
    <author>
      <name>Shuxin Zheng</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13165v1</id>
    <title>SACn: Soft Actor-Critic with n-step Returns</title>
    <updated>2025-12-15T10:23:13Z</updated>
    <link href="https://arxiv.org/abs/2512.13165v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13165v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Soft Actor-Critic (SAC) is widely used in practical applications and is now one of the most relevant off-policy online model-free reinforcement learning (RL) methods. The technique of n-step returns is known to increase the convergence speed of RL algorithms compared to their 1-step returns-based versions. However, SAC is notoriously difficult to combine with n-step returns, since their usual combination introduces bias in off-policy algorithms due to the changes in action distribution. While this problem is solved by importance sampling, a method for estimating expected values of one distribution using samples from another distribution, importance sampling may result in numerical instability. In this work, we combine SAC with n-step returns in a way that overcomes this issue. We present an approach to applying numerically stable importance sampling with simplified hyperparameter selection. Furthermore, we analyze the entropy estimation approach of Soft Actor-Critic in the context of the n-step maximum entropy framework and formulate the $τ$-sampled entropy estimation to reduce the variance of the learning target. Finally, we formulate the Soft Actor-Critic with n-step returns (SAC$n$) algorithm that we experimentally verify on MuJoCo simulated environments.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T10:23:13Z</published>
    <arxiv:comment>Accepted at ICAART 2026</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Jakub Łyskawa</name>
    </author>
    <author>
      <name>Jakub Lewandowski</name>
    </author>
    <author>
      <name>Paweł Wawrzyński</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13164v1</id>
    <title>A Semantically Enhanced Generative Foundation Model Improves Pathological Image Synthesis</title>
    <updated>2025-12-15T10:22:43Z</updated>
    <link href="https://arxiv.org/abs/2512.13164v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13164v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The development of clinical-grade artificial intelligence in pathology is limited by the scarcity of diverse, high-quality annotated datasets. Generative models offer a potential solution but suffer from semantic instability and morphological hallucinations that compromise diagnostic reliability. To address this challenge, we introduce a Correlation-Regulated Alignment Framework for Tissue Synthesis (CRAFTS), the first generative foundation model for pathology-specific text-to-image synthesis. By leveraging a dual-stage training strategy on approximately 2.8 million image-caption pairs, CRAFTS incorporates a novel alignment mechanism that suppresses semantic drift to ensure biological accuracy. This model generates diverse pathological images spanning 30 cancer types, with quality rigorously validated by objective metrics and pathologist evaluations. Furthermore, CRAFTS-augmented datasets enhance the performance across various clinical tasks, including classification, cross-modal retrieval, self-supervised learning, and visual question answering. In addition, coupling CRAFTS with ControlNet enables precise control over tissue architecture from inputs such as nuclear segmentation masks and fluorescence images. By overcoming the critical barriers of data scarcity and privacy concerns, CRAFTS provides a limitless source of diverse, annotated histology data, effectively unlocking the creation of robust diagnostic tools for rare and complex cancer phenotypes.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T10:22:43Z</published>
    <arxiv:comment>67 pages, 9 figures, 16 tables</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Xianchao Guan</name>
    </author>
    <author>
      <name>Zhiyuan Fan</name>
    </author>
    <author>
      <name>Yifeng Wang</name>
    </author>
    <author>
      <name>Fuqiang Chen</name>
    </author>
    <author>
      <name>Yanjiang Zhou</name>
    </author>
    <author>
      <name>Zengyang Che</name>
    </author>
    <author>
      <name>Hongxue Meng</name>
    </author>
    <author>
      <name>Xin Li</name>
    </author>
    <author>
      <name>Yaowei Wang</name>
    </author>
    <author>
      <name>Hongpeng Wang</name>
    </author>
    <author>
      <name>Min Zhang</name>
    </author>
    <author>
      <name>Heng Tao Shen</name>
    </author>
    <author>
      <name>Zheng Zhang</name>
    </author>
    <author>
      <name>Yongbing Zhang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13159v1</id>
    <title>SpeakRL: Synergizing Reasoning, Speaking, and Acting in Language Models with Reinforcement Learning</title>
    <updated>2025-12-15T10:08:53Z</updated>
    <link href="https://arxiv.org/abs/2512.13159v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13159v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Effective human-agent collaboration is increasingly prevalent in real-world applications. Current trends in such collaborations are predominantly unidirectional, with users providing instructions or posing questions to agents, where agents respond directly without seeking necessary clarifications or confirmations. However, the evolving capabilities of these agents require more proactive engagement, where agents should dynamically participate in conversations to clarify user intents, resolve ambiguities, and adapt to changing circumstances. Existing prior work under-utilize the conversational capabilities of language models (LMs), thereby optimizing agents as better followers rather than effective speakers. In this work, we introduce SpeakRL, a reinforcement learning (RL) method that enhances agents' conversational capabilities by rewarding proactive interactions with users, such as asking right clarification questions when necessary. To support this, we curate SpeakER, a synthetic dataset that includes diverse scenarios from task-oriented dialogues, where tasks are resolved through interactive clarification questions. We present a systematic analysis of reward design for conversational proactivity and propose a principled reward formulation for teaching agents to balance asking with acting. Empirical evaluations demonstrate that our approach achieves a 20.14% absolute improvement in task completion over base models without increasing conversation turns even surpassing even much larger proprietary models, demonstrating the promise of clarification-centric user-agent interactions.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T10:08:53Z</published>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Emre Can Acikgoz</name>
    </author>
    <author>
      <name>Jinoh Oh</name>
    </author>
    <author>
      <name>Jie Hao</name>
    </author>
    <author>
      <name>Joo Hyuk Jeon</name>
    </author>
    <author>
      <name>Heng Ji</name>
    </author>
    <author>
      <name>Dilek Hakkani-Tür</name>
    </author>
    <author>
      <name>Gokhan Tur</name>
    </author>
    <author>
      <name>Xiang Li</name>
    </author>
    <author>
      <name>Chengyuan Ma</name>
    </author>
    <author>
      <name>Xing Fan</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13157v1</id>
    <title>Intrinsic Image Fusion for Multi-View 3D Material Reconstruction</title>
    <updated>2025-12-15T10:05:59Z</updated>
    <link href="https://arxiv.org/abs/2512.13157v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13157v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We introduce Intrinsic Image Fusion, a method that reconstructs high-quality physically based materials from multi-view images. Material reconstruction is highly underconstrained and typically relies on analysis-by-synthesis, which requires expensive and noisy path tracing. To better constrain the optimization, we incorporate single-view priors into the reconstruction process. We leverage a diffusion-based material estimator that produces multiple, but often inconsistent, candidate decompositions per view. To reduce the inconsistency, we fit an explicit low-dimensional parametric function to the predictions. We then propose a robust optimization framework using soft per-view prediction selection together with confidence-based soft multi-view inlier set to fuse the most consistent predictions of the most confident views into a consistent parametric material space. Finally, we use inverse path tracing to optimize for the low-dimensional parameters. Our results outperform state-of-the-art methods in material disentanglement on both synthetic and real scenes, producing sharp and clean reconstructions suitable for high-quality relighting.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T10:05:59Z</published>
    <arxiv:comment>Project page: https://peter-kocsis.github.io/IntrinsicImageFusion/ Video: https://www.youtube.com/watch?v=-Vs3tR1Xl7k</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Peter Kocsis</name>
      <arxiv:affiliation>Technical University of Munich</arxiv:affiliation>
    </author>
    <author>
      <name>Lukas Höllein</name>
      <arxiv:affiliation>Technical University of Munich</arxiv:affiliation>
    </author>
    <author>
      <name>Matthias Nießner</name>
      <arxiv:affiliation>Technical University of Munich</arxiv:affiliation>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13154v1</id>
    <title>MAC: A Multi-Agent Framework for Interactive User Clarification in Multi-turn Conversations</title>
    <updated>2025-12-15T10:02:50Z</updated>
    <link href="https://arxiv.org/abs/2512.13154v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13154v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Conversational agents often encounter ambiguous user requests, requiring an effective clarification to successfully complete tasks. While recent advancements in real-world applications favor multi-agent architectures to manage complex conversational scenarios efficiently, ambiguity resolution remains a critical and underexplored challenge--particularly due to the difficulty of determining which agent should initiate a clarification and how agents should coordinate their actions when faced with uncertain or incomplete user input. The fundamental questions of when to interrupt a user and how to formulate the optimal clarification query within the most optimal multi-agent settings remain open. In this paper, we propose MAC (Multi-Agent Clarification), an interactive multi-agent framework specifically optimized to resolve user ambiguities by strategically managing clarification dialogues. We first introduce a novel taxonomy categorizing user ambiguities to systematically guide clarification strategies. Then, we present MAC that autonomously coordinates multiple agents to interact synergistically with users. Empirical evaluations on MultiWOZ 2.4 demonstrate that enabling clarification at both levels increases task success rate 7.8\% (54.5 to 62.3) and reduces the average number of dialogue turns (6.53 to 4.86) by eliciting all required user information up front and minimizing repetition. Our findings highlight the importance of active user interaction and role-aware clarification for more reliable human-agent communication.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T10:02:50Z</published>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Emre Can Acikgoz</name>
    </author>
    <author>
      <name>Jinoh Oh</name>
    </author>
    <author>
      <name>Joo Hyuk Jeon</name>
    </author>
    <author>
      <name>Jie Hao</name>
    </author>
    <author>
      <name>Heng Ji</name>
    </author>
    <author>
      <name>Dilek Hakkani-Tür</name>
    </author>
    <author>
      <name>Gokhan Tur</name>
    </author>
    <author>
      <name>Xiang Li</name>
    </author>
    <author>
      <name>Chengyuan Ma</name>
    </author>
    <author>
      <name>Xing Fan</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13149v1</id>
    <title>Enhancing Node-Level Graph Domain Adaptation by Alleviating Local Dependency</title>
    <updated>2025-12-15T10:00:25Z</updated>
    <link href="https://arxiv.org/abs/2512.13149v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13149v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Recent years have witnessed significant advancements in machine learning methods on graphs. However, transferring knowledge effectively from one graph to another remains a critical challenge. This highlights the need for algorithms capable of applying information extracted from a source graph to an unlabeled target graph, a task known as unsupervised graph domain adaptation (GDA). One key difficulty in unsupervised GDA is conditional shift, which hinders transferability. In this paper, we show that conditional shift can be observed only if there exists local dependencies among node features. To support this claim, we perform a rigorous analysis and also further provide generalization bounds of GDA when dependent node features are modeled using markov chains. Guided by the theoretical findings, we propose to improve GDA by decorrelating node features, which can be specifically implemented through decorrelated GCN layers and graph transformer layers. Our experimental results demonstrate the effectiveness of this approach, showing not only substantial performance enhancements over baseline GDA methods but also clear visualizations of small intra-class distances in the learned representations. Our code is available at https://github.com/TechnologyAiGroup/DFT</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T10:00:25Z</published>
    <arxiv:comment>Accepted to KDD 2026</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Xinwei Tai</name>
    </author>
    <author>
      <name>Dongmian Zou</name>
    </author>
    <author>
      <name>Hongfei Wang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13147v1</id>
    <title>StarryGazer: Leveraging Monocular Depth Estimation Models for Domain-Agnostic Single Depth Image Completion</title>
    <updated>2025-12-15T09:56:09Z</updated>
    <link href="https://arxiv.org/abs/2512.13147v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13147v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The problem of depth completion involves predicting a dense depth image from a single sparse depth map and an RGB image. Unsupervised depth completion methods have been proposed for various datasets where ground truth depth data is unavailable and supervised methods cannot be applied. However, these models require auxiliary data to estimate depth values, which is far from real scenarios. Monocular depth estimation (MDE) models can produce a plausible relative depth map from a single image, but there is no work to properly combine the sparse depth map with MDE for depth completion; a simple affine transformation to the depth map will yield a high error since MDE are inaccurate at estimating depth difference between objects. We introduce StarryGazer, a domain-agnostic framework that predicts dense depth images from a single sparse depth image and an RGB image without relying on ground-truth depth by leveraging the power of large MDE models. First, we employ a pre-trained MDE model to produce relative depth images. These images are segmented and randomly rescaled to form synthetic pairs for dense pseudo-ground truth and corresponding sparse depths. A refinement network is trained with the synthetic pairs, incorporating the relative depth maps and RGB images to improve the model's accuracy and robustness. StarryGazer shows superior results over existing unsupervised methods and transformed MDE results on various datasets, demonstrating that our framework exploits the power of MDE models while appropriately fixing errors using sparse depth information.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T09:56:09Z</published>
    <arxiv:comment>11 pages</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Sangmin Hong</name>
    </author>
    <author>
      <name>Suyoung Lee</name>
    </author>
    <author>
      <name>Kyoung Mu Lee</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13144v1</id>
    <title>Weight Space Correlation Analysis: Quantifying Feature Utilization in Deep Learning Models</title>
    <updated>2025-12-15T09:52:46Z</updated>
    <link href="https://arxiv.org/abs/2512.13144v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13144v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Deep learning models in medical imaging are susceptible to shortcut learning, relying on confounding metadata (e.g., scanner model) that is often encoded in image embeddings. The crucial question is whether the model actively utilizes this encoded information for its final prediction. We introduce Weight Space Correlation Analysis, an interpretable methodology that quantifies feature utilization by measuring the alignment between the classification heads of a primary clinical task and auxiliary metadata tasks. We first validate our method by successfully detecting artificially induced shortcut learning. We then apply it to probe the feature utilization of an SA-SonoNet model trained for Spontaneous Preterm Birth (sPTB) prediction. Our analysis confirmed that while the embeddings contain substantial metadata, the sPTB classifier's weight vectors were highly correlated with clinically relevant factors (e.g., birth weight) but decoupled from clinically irrelevant acquisition factors (e.g. scanner). Our methodology provides a tool to verify model trustworthiness, demonstrating that, in the absence of induced bias, the clinical model selectively utilizes features related to the genuine clinical signal.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T09:52:46Z</published>
    <arxiv:comment>46 pages</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Chun Kit Wong</name>
    </author>
    <author>
      <name>Paraskevas Pegios</name>
    </author>
    <author>
      <name>Nina Weng</name>
    </author>
    <author>
      <name>Emilie Pi Fogtmann Sejer</name>
    </author>
    <author>
      <name>Martin Grønnebæk Tolsgaard</name>
    </author>
    <author>
      <name>Anders Nymark Christensen</name>
    </author>
    <author>
      <name>Aasa Feragen</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13142v1</id>
    <title>Can AI Understand What We Cannot Say? Measuring Multilevel Alignment Through Abortion Stigma Across Cognitive, Interpersonal, and Structural Levels</title>
    <updated>2025-12-15T09:50:00Z</updated>
    <link href="https://arxiv.org/abs/2512.13142v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13142v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>As large language models increasingly mediate stigmatized health decisions, their capacity to genuinely understand complex psychological and physiological phenomena remains poorly evaluated. Can AI understand what we cannot say? We investigate whether LLMs coherently represent abortion stigma across the cognitive, interpersonal, and structural levels where it operates. We systematically tested 627 demographically diverse personas across five leading LLMs using the validated Individual Level Abortion Stigma Scale (ILAS). Our multilevel analysis examined whether models coherently represent stigma at the cognitive level (self-judgment), interpersonal level (anticipated judgment and isolation), and structural level (community condemnation and disclosure patterns), as well as overall stigma. Models fail tests of genuine understanding across all levels. They overestimate interpersonal stigma while underestimating cognitive stigma, assume uniform community condemnation, introduce demographic biases absent from human validation data, miss the empirically validated stigma-secrecy relationship, and contradict themselves within theoretical constructs. These patterns reveal that current alignment approaches ensure appropriate language but not coherent multilevel understanding. This work provides empirical evidence that current LLMs lack coherent multilevel understanding of psychological and physiological constructs. AI safety in high-stakes contexts demands new approaches to design (multilevel coherence), evaluation (continuous auditing), governance and regulation (mandatory audits, accountability, deployment restrictions), and AI literacy in domains where understanding what people cannot say determines whether support helps or harms.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T09:50:00Z</published>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Anika Sharma</name>
    </author>
    <author>
      <name>Malavika Mampally</name>
    </author>
    <author>
      <name>Chidaksh Ravuru</name>
    </author>
    <author>
      <name>Kandyce Brennan</name>
    </author>
    <author>
      <name>Neil Gaikwad</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13131v1</id>
    <title>Towards Unified Co-Speech Gesture Generation via Hierarchical Implicit Periodicity Learning</title>
    <updated>2025-12-15T09:43:08Z</updated>
    <link href="https://arxiv.org/abs/2512.13131v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13131v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Generating 3D-based body movements from speech shows great potential in extensive downstream applications, while it still suffers challenges in imitating realistic human movements. Predominant research efforts focus on end-to-end generation schemes to generate co-speech gestures, spanning GANs, VQ-VAE, and recent diffusion models. As an ill-posed problem, in this paper, we argue that these prevailing learning schemes fail to model crucial inter- and intra-correlations across different motion units, i.e. head, body, and hands, thus leading to unnatural movements and poor coordination. To delve into these intrinsic correlations, we propose a unified Hierarchical Implicit Periodicity (HIP) learning approach for audio-inspired 3D gesture generation. Different from predominant research, our approach models this multi-modal implicit relationship by two explicit technique insights: i) To disentangle the complicated gesture movements, we first explore the gesture motion phase manifolds with periodic autoencoders to imitate human natures from realistic distributions while incorporating non-period ones from current latent states for instance-level diversities. ii) To model the hierarchical relationship of face motions, body gestures, and hand movements, driving the animation with cascaded guidance during learning. We exhibit our proposed approach on 3D avatars and extensive experiments show our method outperforms the state-of-the-art co-speech gesture generation methods by both quantitative and qualitative evaluations. Code and models will be publicly available.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T09:43:08Z</published>
    <arxiv:comment>IEEE Transactions on Image Processing</arxiv:comment>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Xin Guo</name>
    </author>
    <author>
      <name>Yifan Zhao</name>
    </author>
    <author>
      <name>Jia Li</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13130v1</id>
    <title>LeafTrackNet: A Deep Learning Framework for Robust Leaf Tracking in Top-Down Plant Phenotyping</title>
    <updated>2025-12-15T09:43:07Z</updated>
    <link href="https://arxiv.org/abs/2512.13130v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13130v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>High resolution phenotyping at the level of individual leaves offers fine-grained insights into plant development and stress responses. However, the full potential of accurate leaf tracking over time remains largely unexplored due to the absence of robust tracking methods-particularly for structurally complex crops such as canola. Existing plant-specific tracking methods are typically limited to small-scale species or rely on constrained imaging conditions. In contrast, generic multi-object tracking (MOT) methods are not designed for dynamic biological scenes. Progress in the development of accurate leaf tracking models has also been hindered by a lack of large-scale datasets captured under realistic conditions. In this work, we introduce CanolaTrack, a new benchmark dataset comprising 5,704 RGB images with 31,840 annotated leaf instances spanning the early growth stages of 184 canola plants. To enable accurate leaf tracking over time, we introduce LeafTrackNet, an efficient framework that combines a YOLOv10-based leaf detector with a MobileNetV3-based embedding network. During inference, leaf identities are maintained over time through an embedding-based memory association strategy. LeafTrackNet outperforms both plant-specific trackers and state-of-the-art MOT baselines, achieving a 9% HOTA improvement on CanolaTrack. With our work we provide a new standard for leaf-level tracking under realistic conditions and we provide CanolaTrack - the largest dataset for leaf tracking in agriculture crops, which will contribute to future research in plant phenotyping. Our code and dataset are publicly available at https://github.com/shl-shawn/LeafTrackNet.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T09:43:07Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Shanghua Liu</name>
    </author>
    <author>
      <name>Majharulislam Babor</name>
    </author>
    <author>
      <name>Christoph Verduyn</name>
    </author>
    <author>
      <name>Breght Vandenberghe</name>
    </author>
    <author>
      <name>Bruno Betoni Parodi</name>
    </author>
    <author>
      <name>Cornelia Weltzien</name>
    </author>
    <author>
      <name>Marina M. -C. Höhne</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13125v1</id>
    <title>Quanvolutional Neural Networks for Spectrum Peak-Finding</title>
    <updated>2025-12-15T09:33:54Z</updated>
    <link href="https://arxiv.org/abs/2512.13125v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13125v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The analysis of spectra, such as Nuclear Magnetic Resonance (NMR) spectra, for the comprehensive characterization of peaks is a challenging task for both experts and machines, especially with complex molecules. This process, also known as deconvolution, involves identifying and quantifying the peaks in the spectrum. Machine learning techniques have shown promising results in automating this process. With the advent of quantum computing, there is potential to further enhance these techniques. In this work, inspired by the success of classical Convolutional Neural Networks (CNNs), we explore the use of Quanvolutional Neural Networks (QuanvNNs) for the multi-task peak finding problem, involving both peak counting and position estimation. We implement a simple and interpretable QuanvNN architecture that can be directly compared to its classical CNN counterpart, and evaluate its performance on a synthetic NMR-inspired dataset. Our results demonstrate that QuanvNNs outperform classical CNNs on challenging spectra, achieving an 11\% improvement in F1 score and a 30\% reduction in mean absolute error for peak position estimation. Additionally, QuanvNNs appear to exhibit better convergence stability for harder problems.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T09:33:54Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Lukas Bischof</name>
    </author>
    <author>
      <name>Rudolf M. Füchslin</name>
    </author>
    <author>
      <name>Kurt Stockinger</name>
    </author>
    <author>
      <name>Pavel Sulimov</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13123v1</id>
    <title>Stopping Rules for Stochastic Gradient Descent via Anytime-Valid Confidence Sequences</title>
    <updated>2025-12-15T09:26:45Z</updated>
    <link href="https://arxiv.org/abs/2512.13123v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13123v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We study stopping rules for stochastic gradient descent (SGD) for convex optimization from the perspective of anytime-valid confidence sequences. Classical analyses of SGD provide convergence guarantees in expectation or at a fixed horizon, but offer no statistically valid way to assess, at an arbitrary time, how close the current iterate is to the optimum. We develop an anytime-valid, data-dependent upper confidence sequence for the weighted average suboptimality of projected SGD, constructed via nonnegative supermartingales and requiring no smoothness or strong convexity. This confidence sequence yields a simple stopping rule that is provably $\varepsilon$-optimal with probability at least $1-α$ and is almost surely finite under standard stochastic approximation stepsizes. To the best of our knowledge, these are the first rigorous, time-uniform performance guarantees and finite-time $\varepsilon$-optimality certificates for projected SGD with general convex objectives, based solely on observable trajectory quantities.</summary>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T09:26:45Z</published>
    <arxiv:primary_category term="math.OC"/>
    <author>
      <name>Liviu Aolaritei</name>
    </author>
    <author>
      <name>Michael I. Jordan</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13122v1</id>
    <title>DePT3R: Joint Dense Point Tracking and 3D Reconstruction of Dynamic Scenes in a Single Forward Pass</title>
    <updated>2025-12-15T09:21:28Z</updated>
    <link href="https://arxiv.org/abs/2512.13122v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13122v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Current methods for dense 3D point tracking in dynamic scenes typically rely on pairwise processing, require known camera poses, or assume a temporal ordering to input frames, constraining their flexibility and applicability. Additionally, recent advances have successfully enabled efficient 3D reconstruction from large-scale, unposed image collections, underscoring opportunities for unified approaches to dynamic scene understanding. Motivated by this, we propose DePT3R, a novel framework that simultaneously performs dense point tracking and 3D reconstruction of dynamic scenes from multiple images in a single forward pass. This multi-task learning is achieved by extracting deep spatio-temporal features with a powerful backbone and regressing pixel-wise maps with dense prediction heads. Crucially, DePT3R operates without requiring camera poses, substantially enhancing its adaptability and efficiency-especially important in dynamic environments with rapid changes. We validate DePT3R on several challenging benchmarks involving dynamic scenes, demonstrating strong performance and significant improvements in memory efficiency over existing state-of-the-art methods. Data and codes are available via the open repository: https://github.com/StructuresComp/DePT3R</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T09:21:28Z</published>
    <arxiv:comment>This is a work in progress</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Vivek Alumootil</name>
    </author>
    <author>
      <name>Tuan-Anh Vu</name>
    </author>
    <author>
      <name>M. Khalid Jawed</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13120v1</id>
    <title>Towards Practical Large-scale Dynamical Heterogeneous Graph Embedding: Cold-start Resilient Recommendation</title>
    <updated>2025-12-15T09:19:23Z</updated>
    <link href="https://arxiv.org/abs/2512.13120v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13120v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Deploying dynamic heterogeneous graph embeddings in production faces key challenges of scalability, data freshness, and cold-start. This paper introduces a practical, two-stage solution that balances deep graph representation with low-latency incremental updates. Our framework combines HetSGFormer, a scalable graph transformer for static learning, with Incremental Locally Linear Embedding (ILLE), a lightweight, CPU-based algorithm for real-time updates. HetSGFormer captures global structure with linear scalability, while ILLE provides rapid, targeted updates to incorporate new data, thus avoiding costly full retraining. This dual approach is cold-start resilient, leveraging the graph to create meaningful embeddings from sparse data. On billion-scale graphs, A/B tests show HetSGFormer achieved up to a 6.11% lift in Advertiser Value over previous methods, while the ILLE module added another 3.22% lift and improved embedding refresh timeliness by 83.2%. Our work provides a validated framework for deploying dynamic graph learning in production environments.</summary>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T09:19:23Z</published>
    <arxiv:primary_category term="cs.IR"/>
    <author>
      <name>Mabiao Long</name>
    </author>
    <author>
      <name>Jiaxi Liu</name>
    </author>
    <author>
      <name>Yufeng Li</name>
    </author>
    <author>
      <name>Hao Xiong</name>
    </author>
    <author>
      <name>Junchi Yan</name>
    </author>
    <author>
      <name>Kefan Wang</name>
    </author>
    <author>
      <name>Yi Cao</name>
    </author>
    <author>
      <name>Jiandong Ding</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13119v1</id>
    <title>Less Is More: Sparse and Cooperative Perturbation for Point Cloud Attacks</title>
    <updated>2025-12-15T09:18:27Z</updated>
    <link href="https://arxiv.org/abs/2512.13119v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13119v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Most adversarial attacks on point clouds perturb a large number of points, causing widespread geometric changes and limiting applicability in real-world scenarios. While recent works explore sparse attacks by modifying only a few points, such approaches often struggle to maintain effectiveness due to the limited influence of individual perturbations. In this paper, we propose SCP, a sparse and cooperative perturbation framework that selects and leverages a compact subset of points whose joint perturbations produce amplified adversarial effects. Specifically, SCP identifies the subset where the misclassification loss is locally convex with respect to their joint perturbations, determined by checking the positivedefiniteness of the corresponding Hessian block. The selected subset is then optimized to generate high-impact adversarial examples with minimal modifications. Extensive experiments show that SCP achieves 100% attack success rates, surpassing state-of-the-art sparse attacks, and delivers superior imperceptibility to dense attacks with far fewer modifications.</summary>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T09:18:27Z</published>
    <arxiv:comment>Accepted by AAAI'2026 (Oral)</arxiv:comment>
    <arxiv:primary_category term="cs.CR"/>
    <author>
      <name>Keke Tang</name>
    </author>
    <author>
      <name>Tianyu Hao</name>
    </author>
    <author>
      <name>Xiaofei Wang</name>
    </author>
    <author>
      <name>Weilong Peng</name>
    </author>
    <author>
      <name>Denghui Zhang</name>
    </author>
    <author>
      <name>Peican Zhu</name>
    </author>
    <author>
      <name>Zhihong Tian</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13111v1</id>
    <title>From Overfitting to Reliability: Introducing the Hierarchical Approximate Bayesian Neural Network</title>
    <updated>2025-12-15T09:08:42Z</updated>
    <link href="https://arxiv.org/abs/2512.13111v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13111v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>In recent years, neural networks have revolutionized various domains, yet challenges such as hyperparameter tuning and overfitting remain significant hurdles. Bayesian neural networks offer a framework to address these challenges by incorporating uncertainty directly into the model, yielding more reliable predictions, particularly for out-of-distribution data. This paper presents Hierarchical Approximate Bayesian Neural Network, a novel approach that uses a Gaussian-inverse-Wishart distribution as a hyperprior of the network's weights to increase both the robustness and performance of the model. We provide analytical representations for the predictive distribution and weight posterior, which amount to the calculation of the parameters of Student's t-distributions in closed form with linear complexity with respect to the number of weights. Our method demonstrates robust performance, effectively addressing issues of overfitting and providing reliable uncertainty estimates, particularly for out-of-distribution tasks. Experimental results indicate that HABNN not only matches but often outperforms state-of-the-art models, suggesting a promising direction for future applications in safety-critical environments.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T09:08:42Z</published>
    <arxiv:comment>9 pages main body, 1 Figure, 15 pages Appendix</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Hayk Amirkhanian</name>
    </author>
    <author>
      <name>Marco F. Huber</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13109v1</id>
    <title>Uncovering the Role of Initial Saliency in U-Shaped Attention Bias: Scaling Initial Token Weight for Enhanced Long-Text Processing</title>
    <updated>2025-12-15T09:04:06Z</updated>
    <link href="https://arxiv.org/abs/2512.13109v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13109v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Large language models (LLMs) have demonstrated strong performance on a variety of natural language processing (NLP) tasks. However, they often struggle with long-text sequences due to the ``lost in the middle'' phenomenon. This issue has been shown to arise from a U-shaped attention bias, where attention is disproportionately focused on the beginning and end of a text, leaving the middle section underrepresented. While previous studies have attributed this bias to position encoding, our research first identifies an additional factor: initial saliency. It means that in the attention computation for each token, tokens with higher attention weights relative to the initial token tend to receive more attention in the prediction of the next token. We further find that utilizing this property by scaling attention weight between the initial token and others improves the model's ability to process long contexts, achieving a maximum improvement of 3.6\% in MDQA dataset. Moreover, combining this approach with existing methods to reduce position encoding bias further enhances performance, achieving a maximum improvement of 3.4\% in KV-Retrieval tasks.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T09:04:06Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Zewen Qiang</name>
    </author>
    <author>
      <name>Sendong Zhao</name>
    </author>
    <author>
      <name>Haochun Wang</name>
    </author>
    <author>
      <name>Bing Qin</name>
    </author>
    <author>
      <name>Ting Liu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13107v1</id>
    <title>Diffusion-Based Restoration for Multi-Modal 3D Object Detection in Adverse Weather</title>
    <updated>2025-12-15T09:03:46Z</updated>
    <link href="https://arxiv.org/abs/2512.13107v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13107v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Multi-modal 3D object detection is important for reliable perception in robotics and autonomous driving. However, its effectiveness remains limited under adverse weather conditions due to weather-induced distortions and misalignment between different data modalities. In this work, we propose DiffFusion, a novel framework designed to enhance robustness in challenging weather through diffusion-based restoration and adaptive cross-modal fusion. Our key insight is that diffusion models possess strong capabilities for denoising and generating data that can adapt to various weather conditions. Building on this, DiffFusion introduces Diffusion-IR restoring images degraded by weather effects and Point Cloud Restoration (PCR) compensating for corrupted LiDAR data using image object cues. To tackle misalignments between two modalities, we develop Bidirectional Adaptive Fusion and Alignment Module (BAFAM). It enables dynamic multi-modal fusion and bidirectional bird's-eye view (BEV) alignment to maintain consistent spatial correspondence. Extensive experiments on three public datasets show that DiffFusion achieves state-of-the-art robustness under adverse weather while preserving strong clean-data performance. Zero-shot results on the real-world DENSE dataset further validate its generalization. The implementation of our DiffFusion will be released as open-source.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T09:03:46Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Zhijian He</name>
    </author>
    <author>
      <name>Feifei Liu</name>
    </author>
    <author>
      <name>Yuwei Li</name>
    </author>
    <author>
      <name>Zhanpeng Liu</name>
    </author>
    <author>
      <name>Jintao Cheng</name>
    </author>
    <author>
      <name>Xieyuanli Chen</name>
    </author>
    <author>
      <name>Xiaoyu Tang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13106v1</id>
    <title>TraPO: A Semi-Supervised Reinforcement Learning Framework for Boosting LLM Reasoning</title>
    <updated>2025-12-15T09:03:45Z</updated>
    <link href="https://arxiv.org/abs/2512.13106v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13106v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Reinforcement learning with verifiable rewards (RLVR) has proven effective in training large reasoning models (LRMs) by leveraging answer-verifiable signals to guide policy optimization, which, however, suffers from high annotation costs. To alleviate this problem, recent work has explored unsupervised RLVR methods that derive rewards solely from the model's internal consistency, such as through entropy and majority voting. While seemingly promising, these methods often suffer from model collapse in the later stages of training, which may arise from the reinforcement of incorrect reasoning patterns in the absence of external supervision. In this work, we investigate a novel semi-supervised RLVR paradigm that utilizes a small labeled set to guide RLVR training on unlabeled samples. Our key insight is that supervised rewards are essential for stabilizing consistency-based training on unlabeled samples, ensuring that only reasoning patterns verified on labeled instances are incorporated into RL training. Technically, we propose an effective policy optimization algorithm, TraPO, that identifies reliable unlabeled samples by matching their learning trajectory similarity to labeled ones. Building on this, TraPO achieves remarkable data efficiency and strong generalization on six widely used mathematical reasoning benchmarks (AIME24/25, AMC, MATH-500, Minerva, and Olympiad) and three out-of-distribution tasks (ARC-c, GPQA-diamond, and MMLU-pro). With only 1K labeled and 3K unlabeled samples, TraPO reaches 42.6% average accuracy, surpassing the best unsupervised method trained on 45K unlabeled samples (38.3%). Notably, when using 4K labeled and 12K unlabeled samples, TraPO even outperforms the fully supervised model trained on the full 45K labeled samples on all benchmarks, while using only 10% of the labeled data. The code is available via https://github.com/ShenzhiYang2000/TRAPO.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T09:03:45Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Shenzhi Yang</name>
    </author>
    <author>
      <name>Guangcheng Zhu</name>
    </author>
    <author>
      <name>Xing Zheng</name>
    </author>
    <author>
      <name>Yingfan MA</name>
    </author>
    <author>
      <name>Zhongqi Chen</name>
    </author>
    <author>
      <name>Bowen Song</name>
    </author>
    <author>
      <name>Weiqiang Wang</name>
    </author>
    <author>
      <name>Junbo Zhao</name>
    </author>
    <author>
      <name>Gang Chen</name>
    </author>
    <author>
      <name>Haobo Wang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13104v1</id>
    <title>FID-Net: A Feature-Enhanced Deep Learning Network for Forest Infestation Detection</title>
    <updated>2025-12-15T09:01:10Z</updated>
    <link href="https://arxiv.org/abs/2512.13104v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13104v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Forest pests threaten ecosystem stability, requiring efficient monitoring. To overcome the limitations of traditional methods in large-scale, fine-grained detection, this study focuses on accurately identifying infected trees and analyzing infestation patterns. We propose FID-Net, a deep learning model that detects pest-affected trees from UAV visible-light imagery and enables infestation analysis via three spatial metrics. Based on YOLOv8n, FID-Net introduces a lightweight Feature Enhancement Module (FEM) to extract disease-sensitive cues, an Adaptive Multi-scale Feature Fusion Module (AMFM) to align and fuse dual-branch features (RGB and FEM-enhanced), and an Efficient Channel Attention (ECA) mechanism to enhance discriminative information efficiently. From detection results, we construct a pest situation analysis framework using: (1) Kernel Density Estimation to locate infection hotspots; (2) neighborhood evaluation to assess healthy trees' infection risk; (3) DBSCAN clustering to identify high-density healthy clusters as priority protection zones. Experiments on UAV imagery from 32 forest plots in eastern Tianshan, China, show that FID-Net achieves 86.10% precision, 75.44% recall, 82.29% mAP@0.5, and 64.30% mAP@0.5:0.95, outperforming mainstream YOLO models. Analysis confirms infected trees exhibit clear clustering, supporting targeted forest protection. FID-Net enables accurate tree health discrimination and, combined with spatial metrics, provides reliable data for intelligent pest monitoring, early warning, and precise management.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T09:01:10Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Yan Zhang</name>
    </author>
    <author>
      <name>Baoxin Li</name>
    </author>
    <author>
      <name>Han Sun</name>
    </author>
    <author>
      <name>Yuhang Gao</name>
    </author>
    <author>
      <name>Mingtai Zhang</name>
    </author>
    <author>
      <name>Pei Wang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13102v1</id>
    <title>Socratic Students: Teaching Language Models to Learn by Asking Questions</title>
    <updated>2025-12-15T08:59:19Z</updated>
    <link href="https://arxiv.org/abs/2512.13102v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13102v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Large Language Models (LLMs) excel at static interactions, where they answer user queries by retrieving knowledge encoded in their parameters. However, in many real-world settings, such as educational tutoring or medical assistance, relevant information is not directly available and must be actively acquired through dynamic interactions. An interactive agent would recognize its own uncertainty, ask targeted questions, and retain new knowledge efficiently. Prior work has primarily explored effective ways for a teacher to instruct the student, where the teacher identifies student gaps and provides guidance. In this work, we shift the focus to the student and investigate effective strategies to actively query the teacher in seeking useful information. Across math and coding benchmarks, where baseline student models begin with near-zero performance, we show that student-led approaches consistently yield absolute Pass@k improvements of at least 0.5 over static baselines. To improve question quality, we train students using Direct Preference Optimization (DPO) with guidance from either self or stronger students. We find that this guided training enables smaller models to learn how to ask better questions, further enhancing learning efficiency.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T08:59:19Z</published>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Rajeev Bhatt Ambati</name>
    </author>
    <author>
      <name>Tianyi Niu</name>
    </author>
    <author>
      <name>Aashu Singh</name>
    </author>
    <author>
      <name>Shlok Mishra</name>
    </author>
    <author>
      <name>Shashank Srivastava</name>
    </author>
    <author>
      <name>Snigdha Chaturvedi</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13101v1</id>
    <title>Harmonizing Generalization and Specialization: Uncertainty-Informed Collaborative Learning for Semi-supervised Medical Image Segmentation</title>
    <updated>2025-12-15T08:57:49Z</updated>
    <link href="https://arxiv.org/abs/2512.13101v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13101v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Vision foundation models have demonstrated strong generalization in medical image segmentation by leveraging large-scale, heterogeneous pretraining. However, they often struggle to generalize to specialized clinical tasks under limited annotations or rare pathological variations, due to a mismatch between general priors and task-specific requirements. To address this, we propose Uncertainty-informed Collaborative Learning (UnCoL), a dual-teacher framework that harmonizes generalization and specialization in semi-supervised medical image segmentation. Specifically, UnCoL distills both visual and semantic representations from a frozen foundation model to transfer general knowledge, while concurrently maintaining a progressively adapting teacher to capture fine-grained and task-specific representations. To balance guidance from both teachers, pseudo-label learning in UnCoL is adaptively regulated by predictive uncertainty, which selectively suppresses unreliable supervision and stabilizes learning in ambiguous regions. Experiments on diverse 2D and 3D segmentation benchmarks show that UnCoL consistently outperforms state-of-the-art semi-supervised methods and foundation model baselines. Moreover, our model delivers near fully supervised performance with markedly reduced annotation requirements.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T08:57:49Z</published>
    <arxiv:comment>This work has been submitted to the IEEE TMI for possible publication</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Wenjing Lu</name>
    </author>
    <author>
      <name>Yi Hong</name>
    </author>
    <author>
      <name>Yang Yang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13100v1</id>
    <title>OXE-AugE: A Large-Scale Robot Augmentation of OXE for Scaling Cross-Embodiment Policy Learning</title>
    <updated>2025-12-15T08:57:15Z</updated>
    <link href="https://arxiv.org/abs/2512.13100v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13100v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Large and diverse datasets are needed for training generalist robot policies that have potential to control a variety of robot embodiments -- robot arm and gripper combinations -- across diverse tasks and environments. As re-collecting demonstrations and retraining for each new hardware platform are prohibitively costly, we show that existing robot data can be augmented for transfer and generalization. The Open X-Embodiment (OXE) dataset, which aggregates demonstrations from over 60 robot datasets, has been widely used as the foundation for training generalist policies. However, it is highly imbalanced: the top four robot types account for over 85\% of its real data, which risks overfitting to robot-scene combinations. We present AugE-Toolkit, a scalable robot augmentation pipeline, and OXE-AugE, a high-quality open-source dataset that augments OXE with 9 different robot embodiments. OXE-AugE provides over 4.4 million trajectories, more than triple the size of the original OXE. We conduct a systematic study of how scaling robot augmentation impacts cross-embodiment learning. Results suggest that augmenting datasets with diverse arms and grippers improves policy performance not only on the augmented robots, but also on unseen robots and even the original robots under distribution shifts. In physical experiments, we demonstrate that state-of-the-art generalist policies such as OpenVLA and $π_0$ benefit from fine-tuning on OXE-AugE, improving success rates by 24-45% on previously unseen robot-gripper combinations across four real-world manipulation tasks. Project website: https://OXE-AugE.github.io/.</summary>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T08:57:15Z</published>
    <arxiv:primary_category term="cs.RO"/>
    <author>
      <name>Guanhua Ji</name>
    </author>
    <author>
      <name>Harsha Polavaram</name>
    </author>
    <author>
      <name>Lawrence Yunliang Chen</name>
    </author>
    <author>
      <name>Sandeep Bajamahal</name>
    </author>
    <author>
      <name>Zehan Ma</name>
    </author>
    <author>
      <name>Simeon Adebola</name>
    </author>
    <author>
      <name>Chenfeng Xu</name>
    </author>
    <author>
      <name>Ken Goldberg</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13096v1</id>
    <title>Toward Self-Healing Networks-on-Chip: RL-Driven Routing in 2D Torus Architectures</title>
    <updated>2025-12-15T08:54:43Z</updated>
    <link href="https://arxiv.org/abs/2512.13096v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13096v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>We investigate adaptive minimal routing in 2D torus networks on chip NoCs under node fault conditions comparing a reinforcement learning RL based strategy to an adaptive routing baseline A torus topology is used for its low diameter high connectivity properties The RL approach models each router as an agent that learns to forward packets based on network state while the adaptive scheme uses fixed minimal paths with simple rerouting around faults We implement both methods in simulation injecting up to 50 node faults uniformly at random Key metrics are measured 1 throughput vs offered load at fault density 02 2 packet delivery ratio PDR vs fault density and 3 a fault adaptive score FT vs fault density Experimental results show the RL method achieves significantly higher throughput at high load approximately 2030 gain and maintains higher reliability under increasing faults The RL router delivers more packets per cycle and adapts to faults by exploiting path diversity whereas the adaptive scheme degrades sharply as faults accumulate In particular the RL approach preserves end to end connectivity longer PDR remains above 90 until approximately 3040 faults while adaptive PDR drops to approximately 70 at the same point The fault adaptive score likewise favors RL routing Thus RL based adaptive routing demonstrates clear advantages in throughput and fault resilience for torus NoCs</summary>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T08:54:43Z</published>
    <arxiv:primary_category term="cs.DC"/>
    <author>
      <name>Mohammad Walid Charrwi</name>
    </author>
    <author>
      <name>Zaid Hussain</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13095v1</id>
    <title>ADHint: Adaptive Hints with Difficulty Priors for Reinforcement Learning</title>
    <updated>2025-12-15T08:53:47Z</updated>
    <link href="https://arxiv.org/abs/2512.13095v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13095v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>To combine the advantages of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), recent methods have integrated ''hints'' into post-training, which are prefix segments of complete reasoning trajectories, aiming for powerful knowledge expansion and reasoning generalization. However, existing hint-based RL methods typically ignore difficulty when scheduling hint ratios and estimating relative advantages, leading to unstable learning and excessive imitation of off-policy hints. In this work, we propose ADHint, which treats difficulty as a key factor in both hint-ratio schedule and relative-advantage estimation to achieve a better trade-off between exploration and imitation. Specifically, we propose Adaptive Hint with Sample Difficulty Prior, which evaluates each sample's difficulty under the policy model and accordingly schedules an appropriate hint ratio to guide its rollouts. We also introduce Consistency-based Gradient Modulation and Selective Masking for Hint Preservation to modulate token-level gradients within hints, preventing biased and destructive updates. Additionally, we propose Advantage Estimation with Rollout Difficulty Posterior, which leverages the relative difficulty of rollouts with and without hints to estimate their respective advantages, thereby achieving more balanced updates. Extensive experiments across diverse modalities, model scales, and domains demonstrate that ADHint delivers superior reasoning ability and out-of-distribution generalization, consistently surpassing existing methods in both pass@1 and avg@8. Our code and dataset will be made publicly available upon paper acceptance.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T08:53:47Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Feng Zhang</name>
    </author>
    <author>
      <name>Zezhong Tan</name>
    </author>
    <author>
      <name>Xinhong Ma</name>
    </author>
    <author>
      <name>Ziqiang Dong</name>
    </author>
    <author>
      <name>Xi Leng</name>
    </author>
    <author>
      <name>Jianfei Zhao</name>
    </author>
    <author>
      <name>Xin Sun</name>
    </author>
    <author>
      <name>Yang Yang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13094v1</id>
    <title>Sequence of Expert: Boosting Imitation Planners for Autonomous Driving through Temporal Alternation</title>
    <updated>2025-12-15T08:50:23Z</updated>
    <link href="https://arxiv.org/abs/2512.13094v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13094v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Imitation learning (IL) has emerged as a central paradigm in autonomous driving. While IL excels in matching expert behavior in open-loop settings by minimizing per-step prediction errors, its performance degrades unexpectedly in closed-loop due to the gradual accumulation of small, often imperceptible errors over time.Over successive planning cycles, these errors compound, potentially resulting in severe failures.Current research efforts predominantly rely on increasingly sophisticated network architectures or high-fidelity training datasets to enhance the robustness of IL planners against error accumulation, focusing on the state-level robustness at a single time point. However, autonomous driving is inherently a continuous-time process, and leveraging the temporal scale to enhance robustness may provide a new perspective for addressing this issue.To this end, we propose a method termed Sequence of Experts (SoE), a temporal alternation policy that enhances closed-loop performance without increasing model size or data requirements. Our experiments on large-scale autonomous driving benchmarks nuPlan demonstrate that SoE method consistently and significantly improves the performance of all the evaluated models, and achieves state-of-the-art performance.This module may provide a key and widely applicable support for improving the training efficiency of autonomous driving models.</summary>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T08:50:23Z</published>
    <arxiv:primary_category term="cs.RO"/>
    <author>
      <name>Xiang Li</name>
    </author>
    <author>
      <name>Gang Liu</name>
    </author>
    <author>
      <name>Weitao Zhou</name>
    </author>
    <author>
      <name>Hongyi Zhu</name>
    </author>
    <author>
      <name>Zhong Cao</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13093v1</id>
    <title>PvP: Data-Efficient Humanoid Robot Learning with Proprioceptive-Privileged Contrastive Representations</title>
    <updated>2025-12-15T08:50:20Z</updated>
    <link href="https://arxiv.org/abs/2512.13093v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13093v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Achieving efficient and robust whole-body control (WBC) is essential for enabling humanoid robots to perform complex tasks in dynamic environments. Despite the success of reinforcement learning (RL) in this domain, its sample inefficiency remains a significant challenge due to the intricate dynamics and partial observability of humanoid robots. To address this limitation, we propose PvP, a Proprioceptive-Privileged contrastive learning framework that leverages the intrinsic complementarity between proprioceptive and privileged states. PvP learns compact and task-relevant latent representations without requiring hand-crafted data augmentations, enabling faster and more stable policy learning. To support systematic evaluation, we develop SRL4Humanoid, the first unified and modular framework that provides high-quality implementations of representative state representation learning (SRL) methods for humanoid robot learning. Extensive experiments on the LimX Oli robot across velocity tracking and motion imitation tasks demonstrate that PvP significantly improves sample efficiency and final performance compared to baseline SRL methods. Our study further provides practical insights into integrating SRL with RL for humanoid WBC, offering valuable guidance for data-efficient humanoid robot learning.</summary>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T08:50:20Z</published>
    <arxiv:comment>13 pages, 12 figures</arxiv:comment>
    <arxiv:primary_category term="cs.RO"/>
    <author>
      <name>Mingqi Yuan</name>
    </author>
    <author>
      <name>Tao Yu</name>
    </author>
    <author>
      <name>Haolin Song</name>
    </author>
    <author>
      <name>Bo Li</name>
    </author>
    <author>
      <name>Xin Jin</name>
    </author>
    <author>
      <name>Hua Chen</name>
    </author>
    <author>
      <name>Wenjun Zeng</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13089v1</id>
    <title>UniVCD: A New Method for Unsupervised Change Detection in the Open-Vocabulary Era</title>
    <updated>2025-12-15T08:42:23Z</updated>
    <link href="https://arxiv.org/abs/2512.13089v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13089v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Change detection (CD) identifies scene changes from multi-temporal observations and is widely used in urban development and environmental monitoring. Most existing CD methods rely on supervised learning, making performance strongly dataset-dependent and incurring high annotation costs; they typically focus on a few predefined categories and generalize poorly to diverse scenes. With the rise of vision foundation models such as SAM2 and CLIP, new opportunities have emerged to relax these constraints. We propose Unified Open-Vocabulary Change Detection (UniVCD), an unsupervised, open-vocabulary change detection method built on frozen SAM2 and CLIP. UniVCD detects category-agnostic changes across diverse scenes and imaging geometries without any labeled data or paired change images. A lightweight feature alignment module is introduced to bridge the spatially detailed representations from SAM2 and the semantic priors from CLIP, enabling high-resolution, semantically aware change estimation while keeping the number of trainable parameters small. On top of this, a streamlined post-processing pipeline is further introduced to suppress noise and pseudo-changes, improving the detection accuracy for objects with well-defined boundaries. Experiments on several public BCD (Binary Change Detection) and SCD (Semantic Change Detection) benchmarks show that UniVCD achieves consistently strong performance and matches or surpasses existing open-vocabulary CD methods in key metrics such as F1 and IoU. The results demonstrate that unsupervised change detection with frozen vision foundation models and lightweight multi-modal alignment is a practical and effective paradigm for open-vocabulary CD. Code and pretrained models will be released at https://github.com/Die-Xie/UniVCD.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T08:42:23Z</published>
    <arxiv:comment>10 pages, 6 figures</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Ziqiang Zhu</name>
    </author>
    <author>
      <name>Bowei Yang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13083v1</id>
    <title>DiRe: Diversity-promoting Regularization for Dataset Condensation</title>
    <updated>2025-12-15T08:33:44Z</updated>
    <link href="https://arxiv.org/abs/2512.13083v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13083v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>In Dataset Condensation, the goal is to synthesize a small dataset that replicates the training utility of a large original dataset. Existing condensation methods synthesize datasets with significant redundancy, so there is a dire need to reduce redundancy and improve the diversity of the synthesized datasets. To tackle this, we propose an intuitive Diversity Regularizer (DiRe) composed of cosine similarity and Euclidean distance, which can be applied off-the-shelf to various state-of-the-art condensation methods. Through extensive experiments, we demonstrate that the addition of our regularizer improves state-of-the-art condensation methods on various benchmark datasets from CIFAR-10 to ImageNet-1K with respect to generalization and diversity metrics.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T08:33:44Z</published>
    <arxiv:comment>Accepted to WACV 2026</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Saumyaranjan Mohanty</name>
    </author>
    <author>
      <name>Aravind Reddy</name>
    </author>
    <author>
      <name>Konda Reddy Mopuri</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13078v1</id>
    <title>Heart Disease Prediction using Case Based Reasoning (CBR)</title>
    <updated>2025-12-15T08:20:47Z</updated>
    <link href="https://arxiv.org/abs/2512.13078v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13078v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>This study provides an overview of heart disease prediction using an intelligent system. Predicting disease accurately is crucial in the medical field, but traditional methods relying solely on a doctor's experience often lack precision. To address this limitation, intelligent systems are applied as an alternative to traditional approaches. While various intelligent system methods exist, this study focuses on three: Fuzzy Logic, Neural Networks, and Case-Based Reasoning (CBR). A comparison of these techniques in terms of accuracy was conducted, and ultimately, Case-Based Reasoning (CBR) was selected for heart disease prediction. In the prediction phase, the heart disease dataset underwent data pre-processing to clean the data and data splitting to separate it into training and testing sets. The chosen intelligent system was then employed to predict heart disease outcomes based on the processed data. The experiment concluded with Case-Based Reasoning (CBR) achieving a notable accuracy rate of 97.95% in predicting heart disease. The findings also revealed that the probability of heart disease was 57.76% for males and 42.24% for females. Further analysis from related studies suggests that factors such as smoking and alcohol consumption are significant contributors to heart disease, particularly among males.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T08:20:47Z</published>
    <arxiv:comment>Published in Journal of Theoretical and Applied Information Technology on 31st October 2024. Vol.102. No. 20</arxiv:comment>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Mohaiminul Islam Bhuiyan</name>
    </author>
    <author>
      <name>Chan Hue Wah</name>
    </author>
    <author>
      <name>Nur Shazwani Kamarudin</name>
    </author>
    <author>
      <name>Nur Hafieza Ismail</name>
    </author>
    <author>
      <name>Ahmad Fakhri Ab Nasir</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13077v1</id>
    <title>LikeBench: Evaluating Subjective Likability in LLMs for Personalization</title>
    <updated>2025-12-15T08:18:42Z</updated>
    <link href="https://arxiv.org/abs/2512.13077v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13077v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>A personalized LLM should remember user facts, apply them correctly, and adapt over time to provide responses that the user prefers. Existing LLM personalization benchmarks are largely centered on two axes: accurately recalling user information and accurately applying remembered information in downstream tasks. We argue that a third axis, likability, is both subjective and central to user experience, yet under-measured by current benchmarks. To measure likability holistically, we introduce LikeBench, a multi-session, dynamic evaluation framework that measures likability across multiple dimensions by how much an LLM can adapt over time to a user's preferences to provide more likable responses. In LikeBench, the LLMs engage in conversation with a simulated user and learn preferences only from the ongoing dialogue. As the interaction unfolds, models try to adapt to responses, and after each turn, they are evaluated for likability across seven dimensions by the same simulated user. To the best of our knowledge, we are the first to decompose likability into multiple diagnostic metrics: emotional adaptation, formality matching, knowledge adaptation, reference understanding, conversation length fit, humor fit, and callback, which makes it easier to pinpoint where a model falls short. To make the simulated user more realistic and discriminative, LikeBench uses fine-grained, psychologically grounded descriptive personas rather than the coarse high/low trait rating based personas used in prior work. Our benchmark shows that strong memory performance does not guarantee high likability: DeepSeek R1, with lower memory accuracy (86%, 17 facts/profile), outperformed Qwen3 by 28% on likability score despite Qwen3's higher memory accuracy (93%, 43 facts/profile). Even SOTA models like GPT-5 adapt well in short exchanges but show only limited robustness in longer, noisier interactions.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T08:18:42Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Md Awsafur Rahman</name>
    </author>
    <author>
      <name>Adam Gabrys</name>
    </author>
    <author>
      <name>Doug Kang</name>
    </author>
    <author>
      <name>Jingjing Sun</name>
    </author>
    <author>
      <name>Tian Tan</name>
    </author>
    <author>
      <name>Ashwin Chandramouli</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13074v1</id>
    <title>A Simple and Effective Framework for Symmetric Consistent Indexing in Large-Scale Dense Retrieval</title>
    <updated>2025-12-15T08:11:24Z</updated>
    <link href="https://arxiv.org/abs/2512.13074v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13074v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Dense retrieval has become the industry standard in large-scale information retrieval systems due to its high efficiency and competitive accuracy. Its core relies on a coarse-to-fine hierarchical architecture that enables rapid candidate selection and precise semantic matching, achieving millisecond-level response over billion-scale corpora. This capability makes it essential not only in traditional search and recommendation scenarios but also in the emerging paradigm of generative recommendation driven by large language models, where semantic IDs-themselves a form of coarse-to-fine representation-play a foundational role. However, the widely adopted dual-tower encoding architecture introduces inherent challenges, primarily representational space misalignment and retrieval index inconsistency, which degrade matching accuracy, retrieval stability, and performance on long-tail queries. These issues are further magnified in semantic ID generation, ultimately limiting the performance ceiling of downstream generative models.
  To address these challenges, this paper proposes a simple and effective framework named SCI comprising two synergistic modules: a symmetric representation alignment module that employs an innovative input-swapping mechanism to unify the dual-tower representation space without adding parameters, and an consistent indexing with dual-tower synergy module that redesigns retrieval paths using a dual-view indexing strategy to maintain consistency from training to inference. The framework is systematic, lightweight, and engineering-friendly, requiring minimal overhead while fully supporting billion-scale deployment. We provide theoretical guarantees for our approach, with its effectiveness validated by results across public datasets and real-world e-commerce datasets.</summary>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T08:11:24Z</published>
    <arxiv:primary_category term="cs.IR"/>
    <author>
      <name>Huimu Wang</name>
    </author>
    <author>
      <name>Yiming Qiu</name>
    </author>
    <author>
      <name>Xingzhi Yao</name>
    </author>
    <author>
      <name>Zhiguo Chen</name>
    </author>
    <author>
      <name>Guoyu Tang</name>
    </author>
    <author>
      <name>Songlin Wang</name>
    </author>
    <author>
      <name>Sulong Xu</name>
    </author>
    <author>
      <name>Mingming Li</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13072v1</id>
    <title>Forging a Dynamic Memory: Retrieval-Guided Continual Learning for Generalist Medical Foundation Models</title>
    <updated>2025-12-15T08:09:40Z</updated>
    <link href="https://arxiv.org/abs/2512.13072v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13072v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Multimodal biomedical Vision-Language Models (VLMs) exhibit immense potential in the field of Continual Learning (CL). However, they confront a core dilemma: how to preserve fine-grained intra-modality features while bridging the significant domain gap across different modalities. To address this challenge, we propose a comprehensive framework. Leveraging our 18-million multimodal and comprehensive medical retrieval database derived from PubMed scientific papers, we pioneer the integration of Retrieval-Augmented Generation (RAG) into CL. Specifically, we employ a multi-modal, multi-layer RAG system that provides real-time guidance for model fine-tuning through dynamic, on-demand knowledge retrieval. Building upon this, we introduce a dynamic knowledge distillation framework. This framework precisely resolves the aforementioned core dilemma by dynamically modulating the importance of the parameter space, the granularity of the distilled knowledge, and the data distribution of the reference dataset in accordance with the required level of detail. To thoroughly validate the clinical value of our strategy, we have designed a more rigorous \textbf{M}edical Generalist Task Incremental Learning (MGTIL) benchmark. This benchmark is engineered to simultaneously evaluate the model's capacity for adaptation to significant domain shifts, retention of subtle intra-domain features, and real-time learning of novel and complex medical tasks. Extensive experimental results demonstrate that our proposed method achieves state-of-the-art (SOTA) performance across all metrics. The code is provided in the supplementary materials.</summary>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T08:09:40Z</published>
    <arxiv:primary_category term="cs.CV"/>
    <author>
      <name>Zizhi Chen</name>
    </author>
    <author>
      <name>Yizhen Gao</name>
    </author>
    <author>
      <name>Minghao Han</name>
    </author>
    <author>
      <name>Yizhou Liu</name>
    </author>
    <author>
      <name>Zhaoyu Chen</name>
    </author>
    <author>
      <name>Dingkang Yang</name>
    </author>
    <author>
      <name>Lihua Zhang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.13070v1</id>
    <title>M-GRPO: Stabilizing Self-Supervised Reinforcement Learning for Large Language Models with Momentum-Anchored Policy Optimization</title>
    <updated>2025-12-15T08:07:23Z</updated>
    <link href="https://arxiv.org/abs/2512.13070v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.13070v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Self-supervised reinforcement learning (RL) presents a promising approach for enhancing the reasoning capabilities of Large Language Models (LLMs) without reliance on expensive human-annotated data. However, we find that existing methods suffer from a critical failure mode under long-horizon training: a "policy collapse" where performance precipitously degrades. We diagnose this instability and demonstrate that simply scaling the number of rollouts -- a common strategy to improve performance -- only delays, but does not prevent, this collapse. To counteract this instability, we first introduce M-GRPO (Momentum-Anchored Group Relative Policy Optimization), a framework that leverages a slowly evolving momentum model to provide a stable training target. In addition, we identify that this process is often accompanied by a rapid collapse in policy entropy, resulting in a prematurely confident and suboptimal policy. To specifically address this issue, we propose a second contribution: an adaptive filtering method based on the interquartile range (IQR) that dynamically prunes low-entropy trajectories, preserving essential policy diversity. Our extensive experiments on multiple reasoning benchmarks demonstrate that M-GRPO stabilizes the training process while the IQR filter prevents premature convergence. The combination of these two innovations leads to superior training stability and state-of-the-art performance.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-15T08:07:23Z</published>
    <arxiv:comment>7 pages, 5 figures,Accepted NeurIPS 2025 Workshop on Efficient Reasoning</arxiv:comment>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Bizhe Bai</name>
    </author>
    <author>
      <name>Hongming Wu</name>
    </author>
    <author>
      <name>Peng Ye</name>
    </author>
    <author>
      <name>Tao Chen</name>
    </author>
  </entry>
</feed>
