# ai/embeddings/sentence_bert_handler.py
import numpy as np
from typing import List
import logging

logger = logging.getLogger(__name__)

class SentenceBERTHandler:
    """Handler pour g√©n√©rer des embeddings avec Sentence-BERT"""
    
    def __init__(self, model_name="all-MiniLM-L6-v2", device="cpu"):
        try:
            from sentence_transformers import SentenceTransformer
            logger.info(f"üì¶ Chargement du mod√®le {model_name}...")
            self.model = SentenceTransformer(model_name)
            self.model.to(device)
            logger.info("‚úÖ Mod√®le charg√© avec succ√®s")
        except ImportError as e:
            logger.warning(f"‚ö†Ô∏è SentenceTransformers non disponible: {e}")
            logger.info("üìù Utilisation de embeddings factices pour le d√©veloppement")
            self.model = None
    
    def encode(self, texts: List[str]) -> np.ndarray:
        """G√©n√®re des embeddings pour une liste de textes"""
        if self.model is None:
            # Retourne des embeddings factices pour le d√©veloppement
            n = len(texts)
            return np.random.randn(n, 384).astype(np.float32)
        
        try:
            embeddings = self.model.encode(texts, convert_to_numpy=True)
            return embeddings
        except Exception as e:
            logger.error(f"‚ùå Erreur g√©n√©ration embeddings: {e}")
            # Fallback vers embeddings factices
            n = len(texts)
            return np.random.randn(n, 384).astype(np.float32)
    
    def encode_single(self, text: str) -> np.ndarray:
        """G√©n√®re un embedding pour un seul texte"""
        return self.encode([text])[0]

# Version simplifi√©e sans d√©pendances lourdes
class DummyEmbeddingModel:
    """Mod√®le factice pour le d√©veloppement quand sentence-transformers n'est pas disponible"""
    
    def __init__(self, dim=384):
        self.dim = dim
    
    def encode(self, texts):
        n = len(texts)
        # G√©n√®re des embeddings al√©atoires mais coh√©rents (bas√©s sur le hash du texte)
        embeddings = np.zeros((n, self.dim), dtype=np.float32)
        for i, text in enumerate(texts):
            # Hash simple pour avoir des embeddings d√©terministes
            import hashlib
            hash_val = int(hashlib.md5(text.encode()).hexdigest(), 16) % 10000
            np.random.seed(hash_val)
            embeddings[i] = np.random.randn(self.dim)
        return embeddings

# Alternative : utiliser directement
def get_embedding_model(use_dummy=False):
    """Retourne un mod√®le d'embedding (r√©el ou factice)"""
    if use_dummy:
        logger.info("üé≠ Utilisation du mod√®le factice d'embeddings")
        return DummyEmbeddingModel()
    else:
        try:
            return SentenceBERTHandler()
        except:
            logger.warning("‚ö†Ô∏è Retour au mod√®le factice")
            return DummyEmbeddingModel()
