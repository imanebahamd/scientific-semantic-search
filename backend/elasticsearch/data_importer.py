#!/usr/bin/env python3
"""
Importateur optimis√© pour 5000+ documents sans embeddings
Version rapide pour d√©veloppement
"""

import json
import requests
import time
import hashlib
import sys
from pathlib import Path
from tqdm import tqdm
import logging

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DataImporter:
    """Importateur optimis√© pour datasets de d√©veloppement (5000+ docs)"""
    
    def __init__(self):
        self.es_url = "http://localhost:9200"
        self.index_name = "arxiv_papers"
        self.batch_size = 500
        self.max_docs = 5000
        
        # V√©rifier la connexion Elasticsearch
        self.check_elasticsearch()
    
    def check_elasticsearch(self):
        """V√©rifie la connexion √† Elasticsearch"""
        try:
            response = requests.get(self.es_url, timeout=10)
            if response.status_code == 200:
                logger.info(f"‚úÖ Connect√© √† Elasticsearch: {self.es_url}")
                return True
            else:
                logger.error(f"‚ùå Elasticsearch non disponible: {response.status_code}")
                return False
        except Exception as e:
            logger.error(f"‚ùå Erreur connexion Elasticsearch: {e}")
            return False
    
    def find_data_files(self):
        """Trouve tous les fichiers de donn√©es disponibles"""
        logger.info("üîç Recherche des fichiers de donn√©es...")
        
        # Chemins possibles
        possible_paths = [
            Path("../../data"),  # Par rapport √† backend/elasticsearch
            Path("../../../data"),  # Par rapport √† la racine du projet
            Path("data")  # Local
        ]
        
        data_dir = None
        for path in possible_paths:
            if path.exists() and path.is_dir():
                data_dir = path
                break
        
        if not data_dir:
            logger.error("‚ùå Aucun dossier data trouv√©")
            return []
        
        # Fichiers JSON √† rechercher
        json_patterns = ["*.json", "cleaned/*.json"]
        data_files = []
        
        for pattern in json_patterns:
            for filepath in data_dir.glob(pattern):
                if filepath.is_file() and filepath.stat().st_size > 0:
                    data_files.append(filepath)
        
        # Trier par taille (les plus gros d'abord)
        data_files.sort(key=lambda x: x.stat().st_size, reverse=True)
        
        logger.info(f"üìÅ {len(data_files)} fichiers trouv√©s:")
        for f in data_files[:5]:  # Afficher seulement 5 premiers
            size_mb = f.stat().st_size / (1024*1024)
            logger.info(f"   ‚Ä¢ {f.name} ({size_mb:.1f} MB)")
        
        if len(data_files) > 5:
            logger.info(f"   ... et {len(data_files) - 5} autres")
        
        return data_files
    
    def load_and_merge_data(self, data_files):
        """Charge et fusionne les donn√©es de tous les fichiers"""
        logger.info("üìñ Chargement et fusion des donn√©es...")
        
        all_documents = []
        seen_hashes = set()
        
        for filepath in data_files:
            try:
                logger.info(f"  Lecture de {filepath.name}...")
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                if not isinstance(data, list):
                    logger.warning(f"    ‚ö† Format invalide dans {filepath.name} (attendu liste)")
                    continue
                
                logger.info(f"    ‚úÖ {len(data)} documents trouv√©s")
                
                # Traiter chaque document
                for doc in data:
                    # Standardiser le document
                    standardized = self.standardize_document(doc, filepath.name)
                    
                    if standardized:
                        # V√©rifier les doublons par hash
                        doc_hash = hashlib.md5(json.dumps(standardized, sort_keys=True).encode()).hexdigest()
                        
                        if doc_hash not in seen_hashes:
                            seen_hashes.add(doc_hash)
                            all_documents.append(standardized)
                
            except json.JSONDecodeError as e:
                logger.error(f"    ‚ùå Erreur JSON dans {filepath.name}: {e}")
            except Exception as e:
                logger.error(f"    ‚ùå Erreur lecture {filepath.name}: {e}")
        
        # Limiter au nombre maximum
        if len(all_documents) > self.max_docs:
            logger.info(f"üìä Limitation √† {self.max_docs} documents (sur {len(all_documents)})")
            all_documents = all_documents[:self.max_docs]
        
        logger.info(f"üìä Total documents uniques: {len(all_documents)}")
        return all_documents
    
    def standardize_document(self, doc, source_file):
        """Standardise un document"""
        # Extraire l'ID
        doc_id = doc.get('arxiv_id') or doc.get('id') or doc.get('_id', '')
        if not doc_id:
            return None
        
        # Standardiser les champs
        standardized = {
            "id": doc_id,
            "arxiv_id": doc_id,
            "title": doc.get('title', doc.get('Title', '')).strip(),
            "abstract": doc.get('abstract', doc.get('Abstract', doc.get('summary', ''))).strip(),
            "authors": self.clean_list_field(doc.get('authors', [])),
            "categories": self.clean_list_field(doc.get('categories', [])),
            "primary_category": doc.get('primary_category', doc.get('main_category', '')),
            "date": self.extract_date(doc),
            "year": self.extract_year(doc),
            "source": "arXiv",
            "import_source": source_file,
            "import_timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        
        # V√©rifier que le document a un titre et un abstract
        if not standardized['title'] or not standardized['abstract']:
            return None
        
        # Nettoyer l'abstract (limiter la longueur)
        if len(standardized['abstract']) > 5000:
            standardized['abstract'] = standardized['abstract'][:5000] + "..."
        
        return standardized
    
    def clean_list_field(self, field):
        """Nettoie un champ qui peut √™tre une liste ou une string"""
        if isinstance(field, list):
            return [str(item).strip() for item in field if item]
        elif isinstance(field, str):
            # Essayer de parser comme JSON si c'est une string JSON
            try:
                parsed = json.loads(field)
                if isinstance(parsed, list):
                    return [str(item).strip() for item in parsed if item]
            except:
                # Sinon, split par virgule
                return [item.strip() for item in field.split(',') if item.strip()]
        return []
    
    def extract_date(self, doc):
        """Extrait la date du document"""
        date_fields = ['published', 'date', 'publication_date', 'created']
        
        for field in date_fields:
            if field in doc and doc[field]:
                date_str = str(doc[field])
                # Essayer d'extraire YYYY-MM-DD
                if len(date_str) >= 10:
                    return date_str[:10]
        
        # Date par d√©faut
        return "2025-01-01"
    
    def extract_year(self, doc):
        """Extrait l'ann√©e du document"""
        # D'abord essayer les champs explicites
        if 'year' in doc and doc['year']:
            try:
                return int(doc['year'])
            except:
                pass
        
        # Essayer d'extraire de la date
        date_str = self.extract_date(doc)
        if len(date_str) >= 4:
            try:
                return int(date_str[:4])
            except:
                pass
        
        # Par d√©faut
        return 2025
    
    def create_index(self):
        """Cr√©e l'index Elasticsearch"""
        logger.info(f"üìÅ Cr√©ation de l'index '{self.index_name}'...")
        
        # V√©rifier si l'index existe d√©j√†
        try:
            response = requests.get(f"{self.es_url}/{self.index_name}")
            if response.status_code == 200:
                logger.info("üóëÔ∏è Index existe d√©j√†, suppression...")
                requests.delete(f"{self.es_url}/{self.index_name}")
                time.sleep(2)
        except:
            pass
        
        # Mapping optimis√© pour la recherche
        mapping = {
            "settings": {
                "number_of_shards": 1,
                "number_of_replicas": 0,
                "refresh_interval": "30s",
                "analysis": {
                    "analyzer": {
                        "default": {
                            "type": "standard",
                            "stopwords": "_english_"
                        }
                    }
                }
            },
            "mappings": {
                "properties": {
                    "id": {"type": "keyword"},
                    "arxiv_id": {"type": "keyword"},
                    "title": {
                        "type": "text",
                        "analyzer": "default",
                        "fields": {
                            "keyword": {"type": "keyword", "ignore_above": 256}
                        }
                    },
                    "abstract": {
                        "type": "text",
                        "analyzer": "default"
                    },
                    "authors": {"type": "keyword"},
                    "categories": {"type": "keyword"},
                    "primary_category": {"type": "keyword"},
                    "date": {"type": "date", "format": "yyyy-MM-dd"},
                    "year": {"type": "integer"},
                    "source": {"type": "keyword"},
                    "import_source": {"type": "keyword"},
                    "import_timestamp": {"type": "date", "format": "yyyy-MM-dd HH:mm:ss"}
                }
            }
        }
        
        try:
            response = requests.put(
                f"{self.es_url}/{self.index_name}",
                json=mapping,
                timeout=30
            )
            
            if response.status_code == 200:
                logger.info("‚úÖ Index cr√©√© avec succ√®s")
                return True
            else:
                logger.error(f"‚ùå Erreur cr√©ation index: {response.text}")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Exception cr√©ation index: {e}")
            return False
    
    def import_documents(self, documents):
        """Importe les documents dans Elasticsearch"""
        logger.info(f"üöÄ Importation de {len(documents)} documents...")
        
        total_imported = 0
        start_time = time.time()
        
        # Importer par lots
        for i in tqdm(range(0, len(documents), self.batch_size), 
                     desc="Importation", unit="batch"):
            batch = documents[i:i + self.batch_size]
            
            # Pr√©parer le format bulk
            bulk_data = []
            for doc in batch:
                # Action d'indexation
                bulk_data.append(json.dumps({
                    "index": {
                        "_index": self.index_name,
                        "_id": doc["id"]
                    }
                }))
                # Document
                bulk_data.append(json.dumps(doc))
            
            # Convertir en NDJSON
            ndjson = '\n'.join(bulk_data) + '\n'
            
            # Envoyer avec retry
            success = self.send_batch_with_retry(ndjson)
            if success:
                total_imported += len(batch)
            
            # Log p√©riodique
            if (i + self.batch_size) % 1000 == 0:
                elapsed = time.time() - start_time
                rate = total_imported / elapsed
                logger.info(f"  ‚ö° {total_imported:,} docs import√©s @ {rate:.1f} docs/sec")
            
            # Petite pause
            time.sleep(0.1)
        
        elapsed = time.time() - start_time
        rate = total_imported / elapsed if elapsed > 0 else 0
        
        logger.info(f"‚úÖ Importation termin√©e!")
        logger.info(f"   ‚Ä¢ Documents import√©s: {total_imported:,}")
        logger.info(f"   ‚Ä¢ Temps total: {elapsed:.1f} secondes")
        logger.info(f"   ‚Ä¢ Vitesse: {rate:.1f} documents/sec")
        
        return total_imported
    
    def send_batch_with_retry(self, ndjson_data, max_retries=3):
        """Envoie un batch avec m√©canisme de retry"""
        for attempt in range(max_retries):
            try:
                response = requests.post(
                    f"{self.es_url}/_bulk",
                    data=ndjson_data,
                    headers={'Content-Type': 'application/x-ndjson'},
                    timeout=60
                )
                
                if response.status_code == 200:
                    result = response.json()
                    if not result.get('errors'):
                        return True
                    else:
                        # Log des erreurs mais continuer
                        errors = [item.get('index', {}).get('error') 
                                 for item in result.get('items', []) 
                                 if 'error' in item.get('index', {})]
                        if errors:
                            logger.warning(f"‚ö† Erreurs dans le batch: {errors[:2]}...")
                        return len(errors) < len(result.get('items', [])) / 2  # Moiti√© d'erreurs max
                else:
                    logger.warning(f"‚ö† Tentative {attempt + 1}/{max_retries} √©chou√©e: {response.status_code}")
                    
            except Exception as e:
                logger.warning(f"‚ö† Exception tentative {attempt + 1}/{max_retries}: {e}")
            
            # Attendre avant de r√©essayer
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt  # Exponential backoff
                logger.info(f"  ‚è≥ Attente de {wait_time}s avant nouvelle tentative...")
                time.sleep(wait_time)
        
        return False
    
    def verify_import(self):
        """V√©rifie l'importation"""
        logger.info("üîç V√©rification de l'importation...")
        
        # Attendre que l'indexation soit termin√©e
        time.sleep(3)
        
        try:
            # Rafra√Æchir l'index
            requests.post(f"{self.es_url}/{self.index_name}/_refresh", timeout=10)
            
            # Compter les documents
            response = requests.get(f"{self.es_url}/{self.index_name}/_count", timeout=10)
            if response.status_code == 200:
                count = response.json().get('count', 0)
                logger.info(f"üìä Documents dans l'index: {count:,}")
                
                # Test recherche
                test_query = {
                    "query": {
                        "match": {
                            "title": "machine learning"
                        }
                    },
                    "size": 1
                }
                
                response = requests.post(
                    f"{self.es_url}/{self.index_name}/_search",
                    json=test_query,
                    timeout=10
                )
                
                if response.status_code == 200:
                    data = response.json()
                    total_hits = data['hits']['total']['value']
                    took = data['took']
                    
                    logger.info(f"üîç Test recherche 'machine learning':")
                    logger.info(f"   ‚Ä¢ R√©sultats: {total_hits:,}")
                    logger.info(f"   ‚Ä¢ Temps: {took}ms")
                    
                    if data['hits']['hits']:
                        doc = data['hits']['hits'][0]['_source']
                        logger.info(f"üìÑ Exemple: {doc.get('title', '')[:60]}...")
                
                return True
                
            else:
                logger.error(f"‚ùå Impossible de v√©rifier: {response.status_code}")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Erreur v√©rification: {e}")
            return False
    
    def optimize_index(self):
        """Optimise l'index apr√®s importation"""
        logger.info("üîß Optimisation de l'index...")
        
        try:
            # Fusionner les segments
            response = requests.post(
                f"{self.es_url}/{self.index_name}/_forcemerge?max_num_segments=1",
                timeout=120  # Long timeout pour le merge
            )
            
            if response.status_code == 200:
                logger.info("‚úÖ Fusion des segments termin√©e")
            else:
                logger.warning(f"‚ö† Fusion √©chou√©e: {response.status_code}")
            
            # Statistiques
            response = requests.get(f"{self.es_url}/{self.index_name}/_stats/store", timeout=10)
            if response.status_code == 200:
                stats = response.json()
                size_bytes = stats['indices'][self.index_name]['total']['store']['size_in_bytes']
                size_mb = size_bytes / (1024*1024)
                logger.info(f"üíæ Taille index: {size_mb:.1f} MB")
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erreur optimisation: {e}")
            return False
    
    def run(self):
        """Ex√©cute l'importation compl√®te"""
        logger.info("=" * 60)
        logger.info("üöÄ IMPORTATEUR OPTIMIS√â POUR D√âVELOPPEMENT")
        logger.info("=" * 60)
        
        # 1. V√©rifier Elasticsearch
        if not self.check_elasticsearch():
            return False
        
        # 2. Trouver les fichiers de donn√©es
        data_files = self.find_data_files()
        if not data_files:
            logger.error("‚ùå Aucun fichier de donn√©es trouv√©")
            return False
        
        # 3. Charger et fusionner les donn√©es
        documents = self.load_and_merge_data(data_files)
        if not documents:
            logger.error("‚ùå Aucun document √† importer")
            return False
        
        # 4. Cr√©er l'index
        if not self.create_index():
            return False
        
        time.sleep(2)
        
        # 5. Importer les documents
        imported_count = self.import_documents(documents)
        
        # 6. V√©rifier
        self.verify_import()
        
        # 7. Optimiser
        self.optimize_index()
        
        # R√©sum√©
        logger.info("=" * 60)
        logger.info("‚úÖ IMPORTATION TERMIN√âE AVEC SUCC√àS!")
        logger.info(f"üìÅ Index: {self.index_name}")
        logger.info(f"üìä Documents: {imported_count:,}")
        logger.info(f"üåê Test API: http://localhost:8000/search?query=machine+learning")
        logger.info("=" * 60)
        
        return True

def main():
    """Point d'entr√©e principal"""
    try:
        # V√©rifier si tqdm est install√©
        try:
            import tqdm
        except ImportError:
            print("‚ùå Le module 'tqdm' n'est pas install√©.")
            print("   Installer avec: pip install tqdm")
            return
        
        importer = DataImporter()
        success = importer.run()
        
        if success:
            sys.exit(0)
        else:
            sys.exit(1)
            
    except KeyboardInterrupt:
        print("\n\n‚èπ Importation interrompue par l'utilisateur.")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå Erreur critique: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
